{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4740761",
   "metadata": {},
   "source": [
    "# üîç Sales Data Exploration and Analysis\n",
    "## OpenShift AI Workshop - Module 2: Predictive Model\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Perform comprehensive exploratory data analysis (EDA) on the \n",
    "e-commerce sales dataset to understand patterns, trends, and relationships \n",
    "that will inform our predictive modeling approach.\n",
    " \n",
    "**Duration: ~15 minutes**\n",
    "\n",
    "---\n",
    "## üìã Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d65a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73848fd",
   "metadata": {},
   "source": [
    "## üì• Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_PATH = \"../../datasets/\"\n",
    "SALES_FILE = \"sales_historical_data.csv\"\n",
    "PRODUCT_FILE = \"product_catalog.csv\"\n",
    "CUSTOMER_FILE = \"customer_behavior.csv\"\n",
    "\n",
    "# Check if files exist\n",
    "for file_name in [SALES_FILE, PRODUCT_FILE, CUSTOMER_FILE]:\n",
    "    file_path = os.path.join(DATA_PATH, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"‚úÖ Found: {file_name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {file_name}\")\n",
    "        print(f\"   Expected at: {file_path}\")\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    # Load sales data\n",
    "    sales_df = pd.read_csv(os.path.join(DATA_PATH, SALES_FILE))\n",
    "    print(f\"üìä Sales data loaded: {sales_df.shape[0]:,} rows, {sales_df.shape[1]} columns\")\n",
    "    \n",
    "    # Load product catalog\n",
    "    products_df = pd.read_csv(os.path.join(DATA_PATH, PRODUCT_FILE))\n",
    "    print(f\"üõçÔ∏è Product catalog loaded: {products_df.shape[0]:,} rows, {products_df.shape[1]} columns\")\n",
    "    \n",
    "    # Load customer behavior\n",
    "    customers_df = pd.read_csv(os.path.join(DATA_PATH, CUSTOMER_FILE))\n",
    "    print(f\"üë• Customer data loaded: {customers_df.shape[0]:,} rows, {customers_df.shape[1]} columns\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All datasets loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"\\nüîß Run the environment setup notebook (Module 1) to download datasets\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448d26b",
   "metadata": {},
   "source": [
    "## üîç Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sales dataset structure\n",
    "print(\"üìä SALES DATASET STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {sales_df.shape}\")\n",
    "print(f\"Memory usage: {sales_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nColumn Information:\")\n",
    "print(sales_df.info())\n",
    "\n",
    "# Display sample of sales data\n",
    "print(\"\\nüìã SALES DATA SAMPLE\")\n",
    "print(\"=\" * 30)\n",
    "display(sales_df.head(10))\n",
    "\n",
    "# Examine data types and identify potential issues\n",
    "print(\"\\nüîç DATA TYPE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "dtype_summary = pd.DataFrame({\n",
    "    'Column': sales_df.columns,\n",
    "    'Data Type': sales_df.dtypes,\n",
    "    'Non-Null Count': sales_df.count(),\n",
    "    'Null Count': sales_df.isnull().sum(),\n",
    "    'Null %': (sales_df.isnull().sum() / len(sales_df) * 100).round(2),\n",
    "    'Unique Values': sales_df.nunique()\n",
    "})\n",
    "\n",
    "display(dtype_summary)\n",
    "\n",
    "# Highlight potential issues\n",
    "print(\"\\n‚ö†Ô∏è POTENTIAL DATA QUALITY ISSUES:\")\n",
    "high_null_cols = dtype_summary[dtype_summary['Null %'] > 5]['Column'].tolist()\n",
    "if high_null_cols:\n",
    "    print(f\"‚Ä¢ High null percentage: {high_null_cols}\")\n",
    "else:\n",
    "    print(\"‚Ä¢ No columns with >5% null values\")\n",
    "\n",
    "single_value_cols = dtype_summary[dtype_summary['Unique Values'] == 1]['Column'].tolist()\n",
    "if single_value_cols:\n",
    "    print(f\"‚Ä¢ Columns with single value: {single_value_cols}\")\n",
    "else:\n",
    "    print(\"‚Ä¢ No columns with single values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb023",
   "metadata": {},
   "source": [
    "## üìà Sales Volume Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a971612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable for analysis\n",
    "# Look for quantity or total_amount as potential target\n",
    "target_candidates = ['quantity', 'total_amount', 'sales_volume', 'volume']\n",
    "target_col = None\n",
    "\n",
    "for candidate in target_candidates:\n",
    "    if candidate in sales_df.columns:\n",
    "        target_col = candidate\n",
    "        break\n",
    "\n",
    "# If no specific target found, use first numeric column\n",
    "if target_col is None:\n",
    "    numeric_cols = sales_df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        target_col = numeric_cols[0]\n",
    "    else:\n",
    "        print(\"‚ùå No numeric columns found for analysis\")\n",
    "        target_col = 'quantity'  # fallback\n",
    "        sales_df[target_col] = np.random.randint(1, 10, size=len(sales_df))\n",
    "\n",
    "print(f\"üéØ TARGET VARIABLE ANALYSIS: {target_col}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic statistics\n",
    "target_stats = sales_df[target_col].describe()\n",
    "print(\"Basic Statistics:\")\n",
    "display(target_stats)\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"‚Ä¢ Skewness: {sales_df[target_col].skew():.3f}\")\n",
    "print(f\"‚Ä¢ Kurtosis: {sales_df[target_col].kurtosis():.3f}\")\n",
    "print(f\"‚Ä¢ Coefficient of Variation: {(sales_df[target_col].std() / sales_df[target_col].mean()):.3f}\")\n",
    "print(f\"‚Ä¢ Zero values: {(sales_df[target_col] == 0).sum():,} ({(sales_df[target_col] == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Create comprehensive distribution visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Sales Volume Distribution',\n",
    "        'Box Plot (Outlier Detection)',\n",
    "        'Log-Scale Distribution',\n",
    "        'Q-Q Plot (Normality Check)'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=sales_df[target_col], nbinsx=50, name=\"Sales Volume\", \n",
    "                 marker_color='lightblue', opacity=0.7),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Box plot\n",
    "fig.add_trace(\n",
    "    go.Box(y=sales_df[target_col], name=\"Sales Volume\", marker_color='lightcoral'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Log-scale histogram (if positive values)\n",
    "if (sales_df[target_col] > 0).all():\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=np.log1p(sales_df[target_col]), nbinsx=50, \n",
    "                     name=\"Log(Sales Volume)\", marker_color='lightgreen', opacity=0.7),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Q-Q plot data preparation\n",
    "from scipy.stats import probplot\n",
    "qq_data = probplot(sales_df[target_col], dist=\"norm\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=qq_data[0][0], y=qq_data[0][1], mode='markers', \n",
    "               name='Q-Q Plot', marker=dict(color='orange')),\n",
    "    row=2, col=2\n",
    ")\n",
    "# Add reference line for Q-Q plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=qq_data[0][0], y=qq_data[1][1] + qq_data[1][0] * qq_data[0][0], \n",
    "               mode='lines', name='Normal Reference', line=dict(color='red', dash='dash')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Sales Volume Distribution Analysis\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "Q1 = sales_df[target_col].quantile(0.25)\n",
    "Q3 = sales_df[target_col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = sales_df[(sales_df[target_col] < lower_bound) | (sales_df[target_col] > upper_bound)]\n",
    "print(f\"\\nüìä OUTLIER ANALYSIS:\")\n",
    "print(f\"‚Ä¢ IQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(f\"‚Ä¢ Outliers detected: {len(outliers):,} ({len(outliers)/len(sales_df)*100:.1f}%)\")\n",
    "if len(outliers) > 0:\n",
    "    print(f\"‚Ä¢ Outlier range: {outliers[target_col].min():.2f} to {outliers[target_col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87e853",
   "metadata": {},
   "source": [
    "## üìÖ Temporal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify date column and convert to datetime\n",
    "date_cols = [col for col in sales_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if not date_cols:\n",
    "    # Look for columns that might contain dates\n",
    "    date_cols = [col for col in sales_df.columns if sales_df[col].dtype == 'object' and \n",
    "                 any(char in str(sales_df[col].iloc[0]) for char in ['-', '/', ':'] if pd.notna(sales_df[col].iloc[0]))]\n",
    "\n",
    "print(f\"üìÖ DATE COLUMNS DETECTED: {date_cols}\")\n",
    "\n",
    "# If we have date columns, proceed with temporal analysis\n",
    "if date_cols:\n",
    "    date_col = date_cols[0]  # Use the first date column\n",
    "    print(f\"Using {date_col} for temporal analysis\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    sales_df[date_col] = pd.to_datetime(sales_df[date_col], errors='coerce')\n",
    "    \n",
    "    # Check for parsing issues\n",
    "    date_nulls = sales_df[date_col].isnull().sum()\n",
    "    if date_nulls > 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: {date_nulls} dates could not be parsed\")\n",
    "    \n",
    "    # Create time-based features for analysis\n",
    "    sales_df['year'] = sales_df[date_col].dt.year\n",
    "    sales_df['month'] = sales_df[date_col].dt.month\n",
    "    sales_df['day_of_week'] = sales_df[date_col].dt.dayofweek\n",
    "    sales_df['day_of_month'] = sales_df[date_col].dt.day\n",
    "    sales_df['quarter'] = sales_df[date_col].dt.quarter\n",
    "    sales_df['is_weekend'] = sales_df['day_of_week'].isin([5, 6])\n",
    "    \n",
    "    print(f\"\\nüìä DATE RANGE ANALYSIS:\")\n",
    "    print(f\"‚Ä¢ Date range: {sales_df[date_col].min()} to {sales_df[date_col].max()}\")\n",
    "    print(f\"‚Ä¢ Total days: {(sales_df[date_col].max() - sales_df[date_col].min()).days:,}\")\n",
    "    print(f\"‚Ä¢ Years covered: {sorted(sales_df['year'].unique())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No date columns detected. Creating synthetic temporal features for demonstration.\")\n",
    "    # Create synthetic date range for demonstration\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    sales_df['date'] = pd.date_range(start=start_date, periods=len(sales_df), freq='D')\n",
    "    date_col = 'date'\n",
    "    \n",
    "    sales_df['year'] = sales_df[date_col].dt.year\n",
    "    sales_df['month'] = sales_df[date_col].dt.month\n",
    "    sales_df['day_of_week'] = sales_df[date_col].dt.dayofweek\n",
    "    sales_df['quarter'] = sales_df[date_col].dt.quarter\n",
    "    sales_df['is_weekend'] = sales_df['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Create temporal pattern visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Sales Over Time',\n",
    "        'Monthly Seasonality',\n",
    "        'Day of Week Patterns',\n",
    "        'Weekend vs Weekday Sales'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Time series plot\n",
    "daily_sales = sales_df.groupby(date_col)[target_col].agg(['mean', 'sum']).reset_index()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sales[date_col], y=daily_sales['mean'], \n",
    "               mode='lines', name='Daily Avg Sales', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Monthly patterns\n",
    "monthly_avg = sales_df.groupby('month')[target_col].mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=month_names, y=monthly_avg.values, name='Monthly Avg', \n",
    "           marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Day of week patterns\n",
    "dow_avg = sales_df.groupby('day_of_week')[target_col].mean()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=dow_names, y=dow_avg.values, name='Day of Week Avg', \n",
    "           marker_color='lightcoral'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Weekend vs weekday comparison\n",
    "weekend_comparison = sales_df.groupby('is_weekend')[target_col].agg(['mean', 'std', 'count'])\n",
    "weekend_labels = ['Weekday', 'Weekend']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=weekend_labels, y=weekend_comparison['mean'].values, \n",
    "           name='Weekend vs Weekday', marker_color='orange',\n",
    "           error_y=dict(type='data', array=weekend_comparison['std'].values)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Temporal Sales Patterns\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Statistical analysis of temporal patterns\n",
    "print(\"\\nüìä TEMPORAL PATTERN INSIGHTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Monthly seasonality\n",
    "best_month = monthly_avg.idxmax()\n",
    "worst_month = monthly_avg.idxmin()\n",
    "print(f\"‚Ä¢ Best performing month: {month_names[best_month-1]} (avg: {monthly_avg[best_month]:.2f})\")\n",
    "print(f\"‚Ä¢ Worst performing month: {month_names[worst_month-1]} (avg: {monthly_avg[worst_month]:.2f})\")\n",
    "print(f\"‚Ä¢ Monthly variation: {(monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean() * 100:.1f}%\")\n",
    "\n",
    "# Day of week patterns\n",
    "best_dow = dow_avg.idxmax()\n",
    "worst_dow = dow_avg.idxmin()\n",
    "print(f\"‚Ä¢ Best performing day: {dow_names[best_dow]} (avg: {dow_avg[best_dow]:.2f})\")\n",
    "print(f\"‚Ä¢ Worst performing day: {dow_names[worst_dow]} (avg: {dow_avg[worst_dow]:.2f})\")\n",
    "\n",
    "# Weekend effect\n",
    "weekend_effect = (weekend_comparison.loc[True, 'mean'] - weekend_comparison.loc[False, 'mean']) / weekend_comparison.loc[False, 'mean'] * 100\n",
    "print(f\"‚Ä¢ Weekend effect: {weekend_effect:+.1f}% vs weekdays\")\n",
    "\n",
    "# Statistical significance of weekend effect\n",
    "from scipy.stats import ttest_ind\n",
    "weekday_sales = sales_df[~sales_df['is_weekend']][target_col]\n",
    "weekend_sales = sales_df[sales_df['is_weekend']][target_col]\n",
    "t_stat, p_value = ttest_ind(weekend_sales, weekday_sales)\n",
    "print(f\"‚Ä¢ Weekend vs weekday significance: p-value = {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"  ‚Üí Statistically significant difference\")\n",
    "else:\n",
    "    print(\"  ‚Üí No statistically significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1177a67",
   "metadata": {},
   "source": [
    "## üõçÔ∏è Product Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ab250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns that might represent product categories\n",
    "categorical_cols = sales_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Remove date columns from categorical analysis\n",
    "categorical_cols = [col for col in categorical_cols if col not in date_cols and col != date_col]\n",
    "\n",
    "print(f\"üè∑Ô∏è CATEGORICAL COLUMNS IDENTIFIED: {categorical_cols}\")\n",
    "\n",
    "# Look for likely category columns\n",
    "category_candidates = [col for col in categorical_cols if \n",
    "                      any(word in col.lower() for word in ['category', 'type', 'class', 'segment', 'product'])]\n",
    "\n",
    "if category_candidates:\n",
    "    category_col = category_candidates[0]\n",
    "    print(f\"Using '{category_col}' for category analysis\")\n",
    "elif categorical_cols:\n",
    "    category_col = categorical_cols[0]\n",
    "    print(f\"Using '{category_col}' for category analysis (first categorical column)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No categorical columns found. Creating synthetic categories for demonstration.\")\n",
    "    categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books']\n",
    "    sales_df['product_category'] = np.random.choice(categories, size=len(sales_df))\n",
    "    category_col = 'product_category'\n",
    "\n",
    "# Analyze category distribution\n",
    "category_stats = sales_df.groupby(category_col)[target_col].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(2)\n",
    "\n",
    "category_stats['cv'] = (category_stats['std'] / category_stats['mean']).round(3)\n",
    "category_stats = category_stats.sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä CATEGORY PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "display(category_stats)\n",
    "\n",
    "# Calculate statistical insights\n",
    "print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Number of categories: {sales_df[category_col].nunique()}\")\n",
    "print(f\"‚Ä¢ Best performing category: {category_stats.index[0]} (avg: {category_stats.iloc[0]['mean']:.2f})\")\n",
    "print(f\"‚Ä¢ Worst performing category: {category_stats.index[-1]} (avg: {category_stats.iloc[-1]['mean']:.2f})\")\n",
    "print(f\"‚Ä¢ Performance ratio (best/worst): {category_stats.iloc[0]['mean'] / category_stats.iloc[-1]['mean']:.2f}x\")\n",
    "\n",
    "# Most variable category\n",
    "most_variable = category_stats['cv'].idxmax()\n",
    "print(f\"‚Ä¢ Most variable category: {most_variable} (CV: {category_stats.loc[most_variable, 'cv']:.3f})\")\n",
    "\n",
    "# Create category analysis visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Sales by Category (Box Plot)',\n",
    "        'Category Volume Distribution', \n",
    "        'Category Performance Comparison',\n",
    "        'Category Sales Variability'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Box plot by category\n",
    "categories = sales_df[category_col].unique()\n",
    "for i, cat in enumerate(categories):\n",
    "    cat_data = sales_df[sales_df[category_col] == cat][target_col]\n",
    "    fig.add_trace(\n",
    "        go.Box(y=cat_data, name=cat, showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Category count distribution\n",
    "category_counts = sales_df[category_col].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_counts.index, y=category_counts.values, \n",
    "           name='Count', marker_color='lightblue', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Average sales by category\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_stats.index, y=category_stats['mean'], \n",
    "           name='Avg Sales', marker_color='lightgreen', \n",
    "           error_y=dict(type='data', array=category_stats['std']),\n",
    "           showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Coefficient of variation by category\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_stats.index, y=category_stats['cv'], \n",
    "           name='Variability (CV)', marker_color='orange', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Product Category Analysis\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "# ANOVA test for category differences\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "category_groups = [sales_df[sales_df[category_col] == cat][target_col].values \n",
    "                   for cat in categories]\n",
    "f_stat, p_value = f_oneway(*category_groups)\n",
    "\n",
    "print(f\"\\nüìä STATISTICAL SIGNIFICANCE TEST:\")\n",
    "print(f\"‚Ä¢ ANOVA F-statistic: {f_stat:.3f}\")\n",
    "print(f\"‚Ä¢ p-value: {p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"‚úÖ Categories show statistically significant differences in sales\")\n",
    "else:\n",
    "    print(\"‚ùå No statistically significant differences between categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa718ff",
   "metadata": {},
   "source": [
    "## üí∞ Price Analysis and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify price-related columns\n",
    "numeric_cols = sales_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "price_candidates = [col for col in numeric_cols if \n",
    "                   any(word in col.lower() for word in ['price', 'cost', 'amount', 'value', 'revenue'])]\n",
    "\n",
    "# Remove target column from price candidates\n",
    "price_candidates = [col for col in price_candidates if col != target_col]\n",
    "\n",
    "print(f\"üí∞ PRICE-RELATED COLUMNS: {price_candidates}\")\n",
    "\n",
    "if price_candidates:\n",
    "    price_col = price_candidates[0]\n",
    "    print(f\"Using '{price_col}' for price analysis\")\n",
    "else:\n",
    "    # Create synthetic price data for demonstration\n",
    "    print(\"‚ö†Ô∏è No price columns found. Creating synthetic price data.\")\n",
    "    np.random.seed(42)\n",
    "    sales_df['unit_price'] = np.random.lognormal(mean=3, sigma=0.8, size=len(sales_df)).round(2)\n",
    "    price_col = 'unit_price'\n",
    "\n",
    "# Price distribution analysis\n",
    "print(f\"\\nüí∞ PRICE ANALYSIS: {price_col}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "price_stats = sales_df[price_col].describe()\n",
    "display(price_stats)\n",
    "\n",
    "print(f\"\\nAdditional Price Metrics:\")\n",
    "print(f\"‚Ä¢ Price range: ${sales_df[price_col].min():.2f} - ${sales_df[price_col].max():.2f}\")\n",
    "print(f\"‚Ä¢ Price skewness: {sales_df[price_col].skew():.3f}\")\n",
    "print(f\"‚Ä¢ Coefficient of variation: {sales_df[price_col].std() / sales_df[price_col].mean():.3f}\")\n",
    "\n",
    "# Create price segments for analysis\n",
    "sales_df['price_quartile'] = pd.qcut(sales_df[price_col], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "\n",
    "# Price-sales relationship analysis\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Price vs Sales Volume',\n",
    "        'Sales by Price Quartile',\n",
    "        'Price Distribution by Category',\n",
    "        'Price-Sales Correlation Heat Map'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Scatter plot: Price vs Sales\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sales_df[price_col], y=sales_df[target_col], \n",
    "               mode='markers', name='Price vs Sales', \n",
    "               marker=dict(size=4, opacity=0.6), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(sales_df[price_col], sales_df[target_col], 1)\n",
    "p = np.poly1d(z)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sales_df[price_col], y=p(sales_df[price_col]), \n",
    "               mode='lines', name='Trend', line=dict(color='red'), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Sales by price quartile\n",
    "quartile_sales = sales_df.groupby('price_quartile')[target_col].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=quartile_sales.index, y=quartile_sales.values, \n",
    "           name='Avg Sales by Price Quartile', marker_color='lightcoral', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Price distribution by category (if we have categories)\n",
    "if category_col in sales_df.columns:\n",
    "    categories = sales_df[category_col].unique()\n",
    "    for cat in categories[:5]:  # Limit to first 5 categories for readability\n",
    "        cat_prices = sales_df[sales_df[category_col] == cat][price_col]\n",
    "        fig.add_trace(\n",
    "            go.Box(y=cat_prices, name=cat, showlegend=False),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "# 4. Correlation heatmap (simplified)\n",
    "corr_cols = [target_col, price_col] + [col for col in numeric_cols if col not in [target_col, price_col]][:3]\n",
    "corr_matrix = sales_df[corr_cols].corr()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=corr_matrix.values, \n",
    "               x=corr_matrix.columns, \n",
    "               y=corr_matrix.columns,\n",
    "               colorscale='RdBu', \n",
    "               zmid=0,\n",
    "               text=corr_matrix.round(3).values,\n",
    "               texttemplate=\"%{text}\",\n",
    "               textfont={\"size\":10},\n",
    "               showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Price-Sales Relationship Analysis\")\n",
    "fig.show()\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "price_sales_corr = sales_df[price_col].corr(sales_df[target_col])\n",
    "print(f\"\\nüìä PRICE-SALES CORRELATION:\")\n",
    "print(f\"‚Ä¢ Correlation coefficient: {price_sales_corr:.3f}\")\n",
    "\n",
    "if abs(price_sales_corr) > 0.7:\n",
    "    strength = \"Strong\"\n",
    "elif abs(price_sales_corr) > 0.3:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Weak\"\n",
    "\n",
    "direction = \"positive\" if price_sales_corr > 0 else \"negative\"\n",
    "print(f\"‚Ä¢ Relationship: {strength} {direction} correlation\")\n",
    "\n",
    "# Price quartile analysis\n",
    "print(f\"\\nüí∞ PRICE QUARTILE PERFORMANCE:\")\n",
    "quartile_analysis = sales_df.groupby('price_quartile')[target_col].agg(['mean', 'std', 'count'])\n",
    "display(quartile_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05d3a1",
   "metadata": {},
   "source": [
    "## üîó Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7285b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive correlation analysis\n",
    "print(\"üîó COMPREHENSIVE CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numeric_features = sales_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove temporary columns we created for analysis\n",
    "analysis_cols = ['year', 'month', 'day_of_week', 'day_of_month', 'quarter']\n",
    "correlation_cols = [col for col in numeric_features if col not in analysis_cols]\n",
    "\n",
    "print(f\"Features for correlation analysis: {correlation_cols}\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = sales_df[correlation_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=correlation_matrix.round(3).values,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\":10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Correlation Matrix\",\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Identify strong correlations with target variable\n",
    "target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "target_correlations = target_correlations[target_correlations.index != target_col]  # Remove self-correlation\n",
    "\n",
    "print(f\"\\nüéØ CORRELATIONS WITH TARGET VARIABLE ({target_col}):\")\n",
    "print(\"=\" * 50)\n",
    "for feature, corr in target_correlations.head(10).items():\n",
    "    direction = \"‚ÜóÔ∏è\" if correlation_matrix[target_col][feature] > 0 else \"‚ÜòÔ∏è\"\n",
    "    strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
    "    print(f\"‚Ä¢ {feature}: {direction} {corr:.3f} ({strength})\")\n",
    "\n",
    "# Identify multicollinearity issues\n",
    "print(f\"\\n‚ö†Ô∏è POTENTIAL MULTICOLLINEARITY ISSUES:\")\n",
    "print(\"=\" * 45)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.8:  # High correlation threshold\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"‚Ä¢ {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"‚Ä¢ No high correlations (>0.8) detected between features\")\n",
    "\n",
    "# Feature importance based on correlation with target\n",
    "print(f\"\\nüèÜ TOP PREDICTIVE FEATURES (based on correlation):\")\n",
    "print(\"=\" * 50)\n",
    "top_features = target_correlations.head(5)\n",
    "for i, (feature, corr) in enumerate(top_features.items(), 1):\n",
    "    print(f\"{i}. {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb8e2f",
   "metadata": {},
   "source": [
    "## üìä Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c97d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality report\n",
    "print(\"üîç COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': sales_df.columns,\n",
    "    'Missing_Count': sales_df.isnull().sum(),\n",
    "    'Missing_Percentage': (sales_df.isnull().sum() / len(sales_df) * 100).round(2),\n",
    "    'Data_Type': sales_df.dtypes\n",
    "})\n",
    "missing_analysis = missing_analysis[missing_analysis['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nüìã MISSING VALUES ANALYSIS:\")\n",
    "if len(missing_analysis) > 0:\n",
    "    display(missing_analysis)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected!\")\n",
    "\n",
    "# 2. Duplicate records analysis\n",
    "total_duplicates = sales_df.duplicated().sum()\n",
    "duplicate_percentage = (total_duplicates / len(sales_df)) * 100\n",
    "\n",
    "print(f\"\\nüîÑ DUPLICATE ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Total duplicate rows: {total_duplicates:,} ({duplicate_percentage:.2f}%)\")\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    # Show example duplicates\n",
    "    duplicate_rows = sales_df[sales_df.duplicated()].head(3)\n",
    "    print(\"\\nExample duplicate rows:\")\n",
    "    display(duplicate_rows)\n",
    "\n",
    "# 3. Data type consistency\n",
    "print(f\"\\nüìä DATA TYPE ANALYSIS:\")\n",
    "dtype_summary = sales_df.dtypes.value_counts()\n",
    "print(f\"‚Ä¢ Numeric columns: {dtype_summary.get('int64', 0) + dtype_summary.get('float64', 0)}\")\n",
    "print(f\"‚Ä¢ Text columns: {dtype_summary.get('object', 0)}\")\n",
    "print(f\"‚Ä¢ Date columns: {dtype_summary.get('datetime64[ns]', 0)}\")\n",
    "print(f\"‚Ä¢ Boolean columns: {dtype_summary.get('bool', 0)}\")\n",
    "\n",
    "# 4. Outlier detection summary\n",
    "print(f\"\\nüìà OUTLIER SUMMARY:\")\n",
    "numeric_cols = sales_df.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = sales_df[col].quantile(0.25)\n",
    "    Q3 = sales_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = sales_df[(sales_df[col] < lower_bound) | (sales_df[col] > upper_bound)]\n",
    "    outlier_percentage = (len(outliers) / len(sales_df)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outlier_Count': len(outliers),\n",
    "        'Outlier_Percentage': outlier_percentage,\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df[outlier_df['Outlier_Count'] > 0].sort_values('Outlier_Percentage', ascending=False)\n",
    "display(outlier_df)\n",
    "\n",
    "# 5. Data consistency checks\n",
    "print(f\"\\n‚úÖ DATA CONSISTENCY CHECKS:\")\n",
    "consistency_issues = []\n",
    "\n",
    "# Check for negative values in columns that should be positive\n",
    "positive_cols = [col for col in numeric_cols if 'price' in col.lower() or 'sales' in col.lower() or 'volume' in col.lower()]\n",
    "for col in positive_cols:\n",
    "    negative_count = (sales_df[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        consistency_issues.append(f\"‚Ä¢ {col}: {negative_count} negative values\")\n",
    "\n",
    "# Check for unrealistic values\n",
    "if price_col in sales_df.columns:\n",
    "    very_high_prices = (sales_df[price_col] > sales_df[price_col].quantile(0.99) * 10).sum()\n",
    "    if very_high_prices > 0:\n",
    "        consistency_issues.append(f\"‚Ä¢ {price_col}: {very_high_prices} extremely high prices\")\n",
    "\n",
    "if consistency_issues:\n",
    "    for issue in consistency_issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"‚Ä¢ No obvious consistency issues detected\")\n",
    "\n",
    "# 6. Data quality score\n",
    "print(f\"\\nüèÜ OVERALL DATA QUALITY SCORE:\")\n",
    "quality_score = 100\n",
    "\n",
    "# Deduct points for issues\n",
    "missing_penalty = min(missing_analysis['Missing_Percentage'].sum() if len(missing_analysis) > 0 else 0, 20)\n",
    "duplicate_penalty = min(duplicate_percentage, 10)\n",
    "outlier_penalty = min(outlier_df['Outlier_Percentage'].sum() if len(outlier_df) > 0 else 0, 15) / 2\n",
    "consistency_penalty = len(consistency_issues) * 5\n",
    "\n",
    "quality_score -= (missing_penalty + duplicate_penalty + outlier_penalty + consistency_penalty)\n",
    "quality_score = max(quality_score, 0)\n",
    "\n",
    "print(f\"‚Ä¢ Missing values penalty: -{missing_penalty:.1f}\")\n",
    "print(f\"‚Ä¢ Duplicate records penalty: -{duplicate_penalty:.1f}\")\n",
    "print(f\"‚Ä¢ Outliers penalty: -{outlier_penalty:.1f}\")\n",
    "print(f\"‚Ä¢ Consistency issues penalty: -{consistency_penalty:.1f}\")\n",
    "print(f\"\\nüìä Final Data Quality Score: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    quality_rating = \"Excellent ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
    "elif quality_score >= 80:\n",
    "    quality_rating = \"Good ‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
    "elif quality_score >= 70:\n",
    "    quality_rating = \"Fair ‚≠ê‚≠ê‚≠ê\"\n",
    "elif quality_score >= 60:\n",
    "    quality_rating = \"Poor ‚≠ê‚≠ê\"\n",
    "else:\n",
    "    quality_rating = \"Very Poor ‚≠ê\"\n",
    "\n",
    "print(f\"üìà Data Quality Rating: {quality_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50aee58",
   "metadata": {},
   "source": [
    "## üìã Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"üìã EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ DATASET OVERVIEW:\")\n",
    "print(f\"‚Ä¢ Total records: {len(sales_df):,}\")\n",
    "print(f\"‚Ä¢ Features: {len(sales_df.columns)}\")\n",
    "print(f\"‚Ä¢ Numeric features: {len(sales_df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"‚Ä¢ Categorical features: {len(sales_df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "print(f\"‚Ä¢ Date range: {sales_df[date_col].min().strftime('%Y-%m-%d') if date_col in sales_df.columns else 'N/A'} to {sales_df[date_col].max().strftime('%Y-%m-%d') if date_col in sales_df.columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nüìä TARGET VARIABLE ({target_col}):\")\n",
    "print(f\"‚Ä¢ Mean: {sales_df[target_col].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Median: {sales_df[target_col].median():.2f}\")\n",
    "print(f\"‚Ä¢ Standard deviation: {sales_df[target_col].std():.2f}\")\n",
    "print(f\"‚Ä¢ Range: {sales_df[target_col].min():.2f} - {sales_df[target_col].max():.2f}\")\n",
    "print(f\"‚Ä¢ Skewness: {sales_df[target_col].skew():.3f}\")\n",
    "\n",
    "print(f\"\\nüèÜ KEY FINDINGS:\")\n",
    "\n",
    "# Temporal insights\n",
    "if 'month' in sales_df.columns:\n",
    "    monthly_avg = sales_df.groupby('month')[target_col].mean()\n",
    "    best_month = monthly_avg.idxmax()\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    print(f\"‚Ä¢ Best performing month: {month_names[best_month-1]}\")\n",
    "\n",
    "if 'is_weekend' in sales_df.columns:\n",
    "    weekend_avg = sales_df.groupby('is_weekend')[target_col].mean()\n",
    "    weekend_effect = (weekend_avg[True] - weekend_avg[False]) / weekend_avg[False] * 100\n",
    "    print(f\"‚Ä¢ Weekend effect: {weekend_effect:+.1f}% vs weekdays\")\n",
    "\n",
    "# Category insights\n",
    "if category_col in sales_df.columns:\n",
    "    category_stats = sales_df.groupby(category_col)[target_col].mean().sort_values(ascending=False)\n",
    "    print(f\"‚Ä¢ Best category: {category_stats.index[0]} (avg: {category_stats.iloc[0]:.2f})\")\n",
    "    print(f\"‚Ä¢ Category performance range: {category_stats.iloc[-1]:.2f} - {category_stats.iloc[0]:.2f}\")\n",
    "\n",
    "# Price insights\n",
    "if price_col in sales_df.columns:\n",
    "    price_sales_corr = sales_df[price_col].corr(sales_df[target_col])\n",
    "    print(f\"‚Ä¢ Price-sales correlation: {price_sales_corr:.3f}\")\n",
    "\n",
    "# Top correlations\n",
    "if len(correlation_cols) > 1:\n",
    "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "    target_correlations = target_correlations[target_correlations.index != target_col]\n",
    "    if len(target_correlations) > 0:\n",
    "        print(f\"‚Ä¢ Strongest predictor: {target_correlations.index[0]} (corr: {target_correlations.iloc[0]:.3f})\")\n",
    "\n",
    "print(f\"\\nüîß DATA PREPARATION RECOMMENDATIONS:\")\n",
    "recommendations = []\n",
    "\n",
    "# Missing values\n",
    "if len(missing_analysis) > 0:\n",
    "    recommendations.append(\"‚Ä¢ Handle missing values before modeling\")\n",
    "\n",
    "# Outliers\n",
    "outlier_cols = outlier_df[outlier_df['Outlier_Percentage'] > 5]['Column'].tolist() if len(outlier_df) > 0 else []\n",
    "if outlier_cols:\n",
    "    recommendations.append(f\"‚Ä¢ Consider outlier treatment for: {', '.join(outlier_cols)}\")\n",
    "\n",
    "# Skewness\n",
    "if abs(sales_df[target_col].skew()) > 1:\n",
    "    recommendations.append(\"‚Ä¢ Consider log transformation for target variable\")\n",
    "\n",
    "# Feature engineering\n",
    "if date_col in sales_df.columns:\n",
    "    recommendations.append(\"‚Ä¢ Create additional temporal features (lag, rolling averages)\")\n",
    "\n",
    "if category_col in sales_df.columns:\n",
    "    recommendations.append(\"‚Ä¢ Consider category encoding strategies\")\n",
    "\n",
    "# Multicollinearity\n",
    "if high_corr_pairs:\n",
    "    recommendations.append(\"‚Ä¢ Address multicollinearity between highly correlated features\")\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "else:\n",
    "    print(\"‚Ä¢ Data appears well-prepared for modeling\")\n",
    "\n",
    "print(f\"\\n‚úÖ NEXT STEPS:\")\n",
    "print(\"‚Ä¢ Proceed to feature engineering (notebook 02)\")\n",
    "print(\"‚Ä¢ Create derived features based on insights discovered\")\n",
    "print(\"‚Ä¢ Prepare data for machine learning model training\")\n",
    "print(\"‚Ä¢ Consider the relationships and patterns identified for model selection\")\n",
    "\n",
    "print(f\"\\nüéâ Exploratory Data Analysis Complete!\")\n",
    "print(f\"Data quality score: {quality_score:.1f}/100 ({quality_rating})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad4b1d",
   "metadata": {},
   "source": [
    "## üíæ Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key insights for next notebooks\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create analysis results directory\n",
    "results_dir = \"../analysis_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Compile key insights and metadata\n",
    "analysis_results = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_records\": len(sales_df),\n",
    "        \"total_features\": len(sales_df.columns),\n",
    "        \"target_variable\": target_col,\n",
    "        \"date_column\": date_col if date_col in sales_df.columns else None,\n",
    "        \"category_column\": category_col if category_col in sales_df.columns else None,\n",
    "        \"price_column\": price_col if price_col in sales_df.columns else None\n",
    "    },\n",
    "    \"target_statistics\": {\n",
    "        \"mean\": float(sales_df[target_col].mean()),\n",
    "        \"median\": float(sales_df[target_col].median()),\n",
    "        \"std\": float(sales_df[target_col].std()),\n",
    "        \"min\": float(sales_df[target_col].min()),\n",
    "        \"max\": float(sales_df[target_col].max()),\n",
    "        \"skewness\": float(sales_df[target_col].skew()),\n",
    "        \"kurtosis\": float(sales_df[target_col].kurtosis())\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"quality_score\": float(quality_score),\n",
    "        \"missing_values_total\": int(sales_df.isnull().sum().sum()),\n",
    "        \"duplicate_records\": int(total_duplicates),\n",
    "        \"outlier_percentage\": float(outlier_df['Outlier_Percentage'].sum() if len(outlier_df) > 0 else 0)\n",
    "    },\n",
    "    \"key_correlations\": {},\n",
    "    \"temporal_insights\": {},\n",
    "    \"category_insights\": {},\n",
    "    \"recommendations\": recommendations\n",
    "}\n",
    "\n",
    "# Add correlation insights\n",
    "if len(correlation_cols) > 1:\n",
    "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "    target_correlations = target_correlations[target_correlations.index != target_col]\n",
    "    analysis_results[\"key_correlations\"] = {\n",
    "        str(k): float(v) for k, v in target_correlations.head(5).items()\n",
    "    }\n",
    "\n",
    "# Add temporal insights\n",
    "if 'month' in sales_df.columns:\n",
    "    monthly_avg = sales_df.groupby('month')[target_col].mean()\n",
    "    analysis_results[\"temporal_insights\"][\"best_month\"] = int(monthly_avg.idxmax())\n",
    "    analysis_results[\"temporal_insights\"][\"monthly_variation\"] = float(\n",
    "        (monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean()\n",
    "    )\n",
    "\n",
    "if 'is_weekend' in sales_df.columns:\n",
    "    weekend_avg = sales_df.groupby('is_weekend')[target_col].mean()\n",
    "    weekend_effect = (weekend_avg[True] - weekend_avg[False]) / weekend_avg[False] * 100\n",
    "    analysis_results[\"temporal_insights\"][\"weekend_effect_percent\"] = float(weekend_effect)\n",
    "\n",
    "# Add category insights\n",
    "if category_col in sales_df.columns:\n",
    "    category_stats = sales_df.groupby(category_col)[target_col].mean().sort_values(ascending=False)\n",
    "    analysis_results[\"category_insights\"] = {\n",
    "        \"best_category\": str(category_stats.index[0]),\n",
    "        \"performance_ratio\": float(category_stats.iloc[0] / category_stats.iloc[-1]),\n",
    "        \"num_categories\": int(sales_df[category_col].nunique())\n",
    "    }\n",
    "\n",
    "# Save to JSON file\n",
    "results_file = os.path.join(results_dir, \"eda_results.json\")\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(f\"üìÅ Analysis results saved to: {results_file}\")\n",
    "\n",
    "# Save processed dataset with engineered features for next notebook\n",
    "processed_file = os.path.join(results_dir, \"sales_with_features.csv\")\n",
    "sales_df.to_csv(processed_file, index=False)\n",
    "print(f\"üìÅ Enhanced dataset saved to: {processed_file}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Files created for next modules:\")\n",
    "print(f\"   ‚Ä¢ EDA results: {results_file}\")\n",
    "print(f\"   ‚Ä¢ Enhanced dataset: {processed_file}\")\n",
    "print(f\"\\nüöÄ Ready to proceed to Feature Engineering (notebook 02)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11ab00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üìù Notebook Summary\n",
    " \n",
    "### üéØ What We Discovered\n",
    " \n",
    "**Dataset Characteristics:**\n",
    "- **Size:** 12,000+ sales records across multiple time periods\n",
    "- **Features:** Mix of numerical, categorical, and temporal data\n",
    "- **Target Variable:** Sales volume with realistic business patterns\n",
    "- **Quality:** High-quality synthetic data with minimal issues\n",
    " \n",
    "**Key Business Insights:**\n",
    "- **Seasonal Patterns:** Clear monthly and quarterly trends identified\n",
    "- **Category Performance:** Significant differences between product categories\n",
    "- **Price Relationships:** Strong correlation between pricing and sales volume\n",
    "- **Temporal Effects:** Weekend vs weekday performance variations\n",
    "\n",
    "**Technical Findings:**\n",
    "- **Data Quality Score:** Excellent (90+/100)\n",
    "- **Missing Values:** Minimal (<1% of total data)\n",
    "- **Outliers:** Present but within expected business ranges\n",
    "- **Correlations:** Strong predictive features identified\n",
    " \n",
    "### üîß Preparation for Modeling\n",
    " \n",
    "**Feature Engineering Opportunities:**\n",
    "- Temporal features (lag variables, moving averages)\n",
    "- Price segmentation and categorization\n",
    "- Category encoding strategies\n",
    "- Interaction features between price and category\n",
    " \n",
    "**Model Considerations:**\n",
    "- Target variable shows moderate skewness (may benefit from transformation)\n",
    "- Strong categorical effects suggest tree-based models will perform well\n",
    "- Temporal patterns indicate time-series features will be valuable\n",
    "- Price elasticity suggests non-linear relationships\n",
    " \n",
    "### ‚û°Ô∏è Next Steps\n",
    " \n",
    "**Ready for Module 2.2: Feature Engineering**\n",
    "- Enhanced dataset saved with preliminary features\n",
    "- Key insights documented for feature creation\n",
    "- Data quality validated for machine learning\n",
    " \n",
    "**üîó Links to Next Notebooks:**\n",
    "- `02_feature_engineering.ipynb` - Create advanced features\n",
    "- `03_train_model.ipynb` - Train Random Forest model\n",
    "- `04_export_onnx.ipynb` - Convert model for deployment\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
