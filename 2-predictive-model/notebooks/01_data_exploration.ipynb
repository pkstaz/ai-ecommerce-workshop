{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4740761",
   "metadata": {},
   "source": [
    "# 🔍 Sales Data Exploration and Analysis\n",
    "## OpenShift AI Workshop - Module 2: Predictive Model\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Perform comprehensive exploratory data analysis (EDA) on the \n",
    "e-commerce sales dataset to understand patterns, trends, and relationships \n",
    "that will inform our predictive modeling approach.\n",
    " \n",
    "**Duration: ~15 minutes**\n",
    "\n",
    "---\n",
    "## 📋 Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d65a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73848fd",
   "metadata": {},
   "source": [
    "## 📥 Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_PATH = \"../../datasets/\"\n",
    "SALES_FILE = \"sales_historical_data.csv\"\n",
    "PRODUCT_FILE = \"product_catalog.csv\"\n",
    "CUSTOMER_FILE = \"customer_behavior.csv\"\n",
    "\n",
    "# Check if files exist\n",
    "for file_name in [SALES_FILE, PRODUCT_FILE, CUSTOMER_FILE]:\n",
    "    file_path = os.path.join(DATA_PATH, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✅ Found: {file_name}\")\n",
    "    else:\n",
    "        print(f\"❌ Missing: {file_name}\")\n",
    "        print(f\"   Expected at: {file_path}\")\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    # Load sales data\n",
    "    sales_df = pd.read_csv(os.path.join(DATA_PATH, SALES_FILE))\n",
    "    print(f\"📊 Sales data loaded: {sales_df.shape[0]:,} rows, {sales_df.shape[1]} columns\")\n",
    "    \n",
    "    # Load product catalog\n",
    "    products_df = pd.read_csv(os.path.join(DATA_PATH, PRODUCT_FILE))\n",
    "    print(f\"🛍️ Product catalog loaded: {products_df.shape[0]:,} rows, {products_df.shape[1]} columns\")\n",
    "    \n",
    "    # Load customer behavior\n",
    "    customers_df = pd.read_csv(os.path.join(DATA_PATH, CUSTOMER_FILE))\n",
    "    print(f\"👥 Customer data loaded: {customers_df.shape[0]:,} rows, {customers_df.shape[1]} columns\")\n",
    "    \n",
    "    print(\"\\n✅ All datasets loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"\\n🔧 Run the environment setup notebook (Module 1) to download datasets\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448d26b",
   "metadata": {},
   "source": [
    "## 🔍 Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sales dataset structure\n",
    "print(\"📊 SALES DATASET STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {sales_df.shape}\")\n",
    "print(f\"Memory usage: {sales_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nColumn Information:\")\n",
    "print(sales_df.info())\n",
    "\n",
    "# Display sample of sales data\n",
    "print(\"\\n📋 SALES DATA SAMPLE\")\n",
    "print(\"=\" * 30)\n",
    "display(sales_df.head(10))\n",
    "\n",
    "# Examine data types and identify potential issues\n",
    "print(\"\\n🔍 DATA TYPE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "dtype_summary = pd.DataFrame({\n",
    "    'Column': sales_df.columns,\n",
    "    'Data Type': sales_df.dtypes,\n",
    "    'Non-Null Count': sales_df.count(),\n",
    "    'Null Count': sales_df.isnull().sum(),\n",
    "    'Null %': (sales_df.isnull().sum() / len(sales_df) * 100).round(2),\n",
    "    'Unique Values': sales_df.nunique()\n",
    "})\n",
    "\n",
    "display(dtype_summary)\n",
    "\n",
    "# Highlight potential issues\n",
    "print(\"\\n⚠️ POTENTIAL DATA QUALITY ISSUES:\")\n",
    "high_null_cols = dtype_summary[dtype_summary['Null %'] > 5]['Column'].tolist()\n",
    "if high_null_cols:\n",
    "    print(f\"• High null percentage: {high_null_cols}\")\n",
    "else:\n",
    "    print(\"• No columns with >5% null values\")\n",
    "\n",
    "single_value_cols = dtype_summary[dtype_summary['Unique Values'] == 1]['Column'].tolist()\n",
    "if single_value_cols:\n",
    "    print(f\"• Columns with single value: {single_value_cols}\")\n",
    "else:\n",
    "    print(\"• No columns with single values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bb023",
   "metadata": {},
   "source": [
    "## 📈 Sales Volume Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a971612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable for analysis\n",
    "# Look for quantity or total_amount as potential target\n",
    "target_candidates = ['quantity', 'total_amount', 'sales_volume', 'volume']\n",
    "target_col = None\n",
    "\n",
    "for candidate in target_candidates:\n",
    "    if candidate in sales_df.columns:\n",
    "        target_col = candidate\n",
    "        break\n",
    "\n",
    "# If no specific target found, use first numeric column\n",
    "if target_col is None:\n",
    "    numeric_cols = sales_df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        target_col = numeric_cols[0]\n",
    "    else:\n",
    "        print(\"❌ No numeric columns found for analysis\")\n",
    "        target_col = 'quantity'  # fallback\n",
    "        sales_df[target_col] = np.random.randint(1, 10, size=len(sales_df))\n",
    "\n",
    "print(f\"🎯 TARGET VARIABLE ANALYSIS: {target_col}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic statistics\n",
    "target_stats = sales_df[target_col].describe()\n",
    "print(\"Basic Statistics:\")\n",
    "display(target_stats)\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"• Skewness: {sales_df[target_col].skew():.3f}\")\n",
    "print(f\"• Kurtosis: {sales_df[target_col].kurtosis():.3f}\")\n",
    "print(f\"• Coefficient of Variation: {(sales_df[target_col].std() / sales_df[target_col].mean()):.3f}\")\n",
    "print(f\"• Zero values: {(sales_df[target_col] == 0).sum():,} ({(sales_df[target_col] == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Create comprehensive distribution visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Sales Volume Distribution',\n",
    "        'Box Plot (Outlier Detection)',\n",
    "        'Log-Scale Distribution',\n",
    "        'Q-Q Plot (Normality Check)'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=sales_df[target_col], nbinsx=50, name=\"Sales Volume\", \n",
    "                 marker_color='lightblue', opacity=0.7),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Box plot\n",
    "fig.add_trace(\n",
    "    go.Box(y=sales_df[target_col], name=\"Sales Volume\", marker_color='lightcoral'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Log-scale histogram (if positive values)\n",
    "if (sales_df[target_col] > 0).all():\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=np.log1p(sales_df[target_col]), nbinsx=50, \n",
    "                     name=\"Log(Sales Volume)\", marker_color='lightgreen', opacity=0.7),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Q-Q plot data preparation\n",
    "from scipy.stats import probplot\n",
    "qq_data = probplot(sales_df[target_col], dist=\"norm\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=qq_data[0][0], y=qq_data[0][1], mode='markers', \n",
    "               name='Q-Q Plot', marker=dict(color='orange')),\n",
    "    row=2, col=2\n",
    ")\n",
    "# Add reference line for Q-Q plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=qq_data[0][0], y=qq_data[1][1] + qq_data[1][0] * qq_data[0][0], \n",
    "               mode='lines', name='Normal Reference', line=dict(color='red', dash='dash')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Sales Volume Distribution Analysis\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "Q1 = sales_df[target_col].quantile(0.25)\n",
    "Q3 = sales_df[target_col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = sales_df[(sales_df[target_col] < lower_bound) | (sales_df[target_col] > upper_bound)]\n",
    "print(f\"\\n📊 OUTLIER ANALYSIS:\")\n",
    "print(f\"• IQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(f\"• Outliers detected: {len(outliers):,} ({len(outliers)/len(sales_df)*100:.1f}%)\")\n",
    "if len(outliers) > 0:\n",
    "    print(f\"• Outlier range: {outliers[target_col].min():.2f} to {outliers[target_col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87e853",
   "metadata": {},
   "source": [
    "## 📅 Temporal Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify date column and convert to datetime\n",
    "date_cols = [col for col in sales_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if not date_cols:\n",
    "    # Look for columns that might contain dates\n",
    "    date_cols = [col for col in sales_df.columns if sales_df[col].dtype == 'object' and \n",
    "                 any(char in str(sales_df[col].iloc[0]) for char in ['-', '/', ':'] if pd.notna(sales_df[col].iloc[0]))]\n",
    "\n",
    "print(f\"📅 DATE COLUMNS DETECTED: {date_cols}\")\n",
    "\n",
    "# If we have date columns, proceed with temporal analysis\n",
    "if date_cols:\n",
    "    date_col = date_cols[0]  # Use the first date column\n",
    "    print(f\"Using {date_col} for temporal analysis\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    sales_df[date_col] = pd.to_datetime(sales_df[date_col], errors='coerce')\n",
    "    \n",
    "    # Check for parsing issues\n",
    "    date_nulls = sales_df[date_col].isnull().sum()\n",
    "    if date_nulls > 0:\n",
    "        print(f\"⚠️ Warning: {date_nulls} dates could not be parsed\")\n",
    "    \n",
    "    # Create time-based features for analysis\n",
    "    sales_df['year'] = sales_df[date_col].dt.year\n",
    "    sales_df['month'] = sales_df[date_col].dt.month\n",
    "    sales_df['day_of_week'] = sales_df[date_col].dt.dayofweek\n",
    "    sales_df['day_of_month'] = sales_df[date_col].dt.day\n",
    "    sales_df['quarter'] = sales_df[date_col].dt.quarter\n",
    "    sales_df['is_weekend'] = sales_df['day_of_week'].isin([5, 6])\n",
    "    \n",
    "    print(f\"\\n📊 DATE RANGE ANALYSIS:\")\n",
    "    print(f\"• Date range: {sales_df[date_col].min()} to {sales_df[date_col].max()}\")\n",
    "    print(f\"• Total days: {(sales_df[date_col].max() - sales_df[date_col].min()).days:,}\")\n",
    "    print(f\"• Years covered: {sorted(sales_df['year'].unique())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No date columns detected. Creating synthetic temporal features for demonstration.\")\n",
    "    # Create synthetic date range for demonstration\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    sales_df['date'] = pd.date_range(start=start_date, periods=len(sales_df), freq='D')\n",
    "    date_col = 'date'\n",
    "    \n",
    "    sales_df['year'] = sales_df[date_col].dt.year\n",
    "    sales_df['month'] = sales_df[date_col].dt.month\n",
    "    sales_df['day_of_week'] = sales_df[date_col].dt.dayofweek\n",
    "    sales_df['quarter'] = sales_df[date_col].dt.quarter\n",
    "    sales_df['is_weekend'] = sales_df['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Create temporal pattern visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Sales Over Time',\n",
    "        'Monthly Seasonality',\n",
    "        'Day of Week Patterns',\n",
    "        'Weekend vs Weekday Sales'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Time series plot\n",
    "daily_sales = sales_df.groupby(date_col)[target_col].agg(['mean', 'sum']).reset_index()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sales[date_col], y=daily_sales['mean'], \n",
    "               mode='lines', name='Daily Avg Sales', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Monthly patterns\n",
    "monthly_avg = sales_df.groupby('month')[target_col].mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=month_names, y=monthly_avg.values, name='Monthly Avg', \n",
    "           marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Day of week patterns\n",
    "dow_avg = sales_df.groupby('day_of_week')[target_col].mean()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=dow_names, y=dow_avg.values, name='Day of Week Avg', \n",
    "           marker_color='lightcoral'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Weekend vs weekday comparison\n",
    "weekend_comparison = sales_df.groupby('is_weekend')[target_col].agg(['mean', 'std', 'count'])\n",
    "weekend_labels = ['Weekday', 'Weekend']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=weekend_labels, y=weekend_comparison['mean'].values, \n",
    "           name='Weekend vs Weekday', marker_color='orange',\n",
    "           error_y=dict(type='data', array=weekend_comparison['std'].values)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Temporal Sales Patterns\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Statistical analysis of temporal patterns\n",
    "print(\"\\n📊 TEMPORAL PATTERN INSIGHTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Monthly seasonality\n",
    "best_month = monthly_avg.idxmax()\n",
    "worst_month = monthly_avg.idxmin()\n",
    "print(f\"• Best performing month: {month_names[best_month-1]} (avg: {monthly_avg[best_month]:.2f})\")\n",
    "print(f\"• Worst performing month: {month_names[worst_month-1]} (avg: {monthly_avg[worst_month]:.2f})\")\n",
    "print(f\"• Monthly variation: {(monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean() * 100:.1f}%\")\n",
    "\n",
    "# Day of week patterns\n",
    "best_dow = dow_avg.idxmax()\n",
    "worst_dow = dow_avg.idxmin()\n",
    "print(f\"• Best performing day: {dow_names[best_dow]} (avg: {dow_avg[best_dow]:.2f})\")\n",
    "print(f\"• Worst performing day: {dow_names[worst_dow]} (avg: {dow_avg[worst_dow]:.2f})\")\n",
    "\n",
    "# Weekend effect\n",
    "weekend_effect = (weekend_comparison.loc[True, 'mean'] - weekend_comparison.loc[False, 'mean']) / weekend_comparison.loc[False, 'mean'] * 100\n",
    "print(f\"• Weekend effect: {weekend_effect:+.1f}% vs weekdays\")\n",
    "\n",
    "# Statistical significance of weekend effect\n",
    "from scipy.stats import ttest_ind\n",
    "weekday_sales = sales_df[~sales_df['is_weekend']][target_col]\n",
    "weekend_sales = sales_df[sales_df['is_weekend']][target_col]\n",
    "t_stat, p_value = ttest_ind(weekend_sales, weekday_sales)\n",
    "print(f\"• Weekend vs weekday significance: p-value = {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"  → Statistically significant difference\")\n",
    "else:\n",
    "    print(\"  → No statistically significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1177a67",
   "metadata": {},
   "source": [
    "## 🛍️ Product Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ab250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns that might represent product categories\n",
    "categorical_cols = sales_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Remove date columns from categorical analysis\n",
    "categorical_cols = [col for col in categorical_cols if col not in date_cols and col != date_col]\n",
    "\n",
    "print(f\"🏷️ CATEGORICAL COLUMNS IDENTIFIED: {categorical_cols}\")\n",
    "\n",
    "# Look for likely category columns\n",
    "category_candidates = [col for col in categorical_cols if \n",
    "                      any(word in col.lower() for word in ['category', 'type', 'class', 'segment', 'product'])]\n",
    "\n",
    "if category_candidates:\n",
    "    category_col = category_candidates[0]\n",
    "    print(f\"Using '{category_col}' for category analysis\")\n",
    "elif categorical_cols:\n",
    "    category_col = categorical_cols[0]\n",
    "    print(f\"Using '{category_col}' for category analysis (first categorical column)\")\n",
    "else:\n",
    "    print(\"⚠️ No categorical columns found. Creating synthetic categories for demonstration.\")\n",
    "    categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books']\n",
    "    sales_df['product_category'] = np.random.choice(categories, size=len(sales_df))\n",
    "    category_col = 'product_category'\n",
    "\n",
    "# Analyze category distribution\n",
    "category_stats = sales_df.groupby(category_col)[target_col].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(2)\n",
    "\n",
    "category_stats['cv'] = (category_stats['std'] / category_stats['mean']).round(3)\n",
    "category_stats = category_stats.sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n📊 CATEGORY PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "display(category_stats)\n",
    "\n",
    "# Calculate statistical insights\n",
    "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(f\"• Number of categories: {sales_df[category_col].nunique()}\")\n",
    "print(f\"• Best performing category: {category_stats.index[0]} (avg: {category_stats.iloc[0]['mean']:.2f})\")\n",
    "print(f\"• Worst performing category: {category_stats.index[-1]} (avg: {category_stats.iloc[-1]['mean']:.2f})\")\n",
    "print(f\"• Performance ratio (best/worst): {category_stats.iloc[0]['mean'] / category_stats.iloc[-1]['mean']:.2f}x\")\n",
    "\n",
    "# Most variable category\n",
    "most_variable = category_stats['cv'].idxmax()\n",
    "print(f\"• Most variable category: {most_variable} (CV: {category_stats.loc[most_variable, 'cv']:.3f})\")\n",
    "\n",
    "# Create category analysis visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Sales by Category (Box Plot)',\n",
    "        'Category Volume Distribution', \n",
    "        'Category Performance Comparison',\n",
    "        'Category Sales Variability'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Box plot by category\n",
    "categories = sales_df[category_col].unique()\n",
    "for i, cat in enumerate(categories):\n",
    "    cat_data = sales_df[sales_df[category_col] == cat][target_col]\n",
    "    fig.add_trace(\n",
    "        go.Box(y=cat_data, name=cat, showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Category count distribution\n",
    "category_counts = sales_df[category_col].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_counts.index, y=category_counts.values, \n",
    "           name='Count', marker_color='lightblue', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Average sales by category\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_stats.index, y=category_stats['mean'], \n",
    "           name='Avg Sales', marker_color='lightgreen', \n",
    "           error_y=dict(type='data', array=category_stats['std']),\n",
    "           showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Coefficient of variation by category\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_stats.index, y=category_stats['cv'], \n",
    "           name='Variability (CV)', marker_color='orange', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Product Category Analysis\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "# ANOVA test for category differences\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "category_groups = [sales_df[sales_df[category_col] == cat][target_col].values \n",
    "                   for cat in categories]\n",
    "f_stat, p_value = f_oneway(*category_groups)\n",
    "\n",
    "print(f\"\\n📊 STATISTICAL SIGNIFICANCE TEST:\")\n",
    "print(f\"• ANOVA F-statistic: {f_stat:.3f}\")\n",
    "print(f\"• p-value: {p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"✅ Categories show statistically significant differences in sales\")\n",
    "else:\n",
    "    print(\"❌ No statistically significant differences between categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa718ff",
   "metadata": {},
   "source": [
    "## 💰 Price Analysis and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify price-related columns\n",
    "numeric_cols = sales_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "price_candidates = [col for col in numeric_cols if \n",
    "                   any(word in col.lower() for word in ['price', 'cost', 'amount', 'value', 'revenue'])]\n",
    "\n",
    "# Remove target column from price candidates\n",
    "price_candidates = [col for col in price_candidates if col != target_col]\n",
    "\n",
    "print(f\"💰 PRICE-RELATED COLUMNS: {price_candidates}\")\n",
    "\n",
    "if price_candidates:\n",
    "    price_col = price_candidates[0]\n",
    "    print(f\"Using '{price_col}' for price analysis\")\n",
    "else:\n",
    "    # Create synthetic price data for demonstration\n",
    "    print(\"⚠️ No price columns found. Creating synthetic price data.\")\n",
    "    np.random.seed(42)\n",
    "    sales_df['unit_price'] = np.random.lognormal(mean=3, sigma=0.8, size=len(sales_df)).round(2)\n",
    "    price_col = 'unit_price'\n",
    "\n",
    "# Price distribution analysis\n",
    "print(f\"\\n💰 PRICE ANALYSIS: {price_col}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "price_stats = sales_df[price_col].describe()\n",
    "display(price_stats)\n",
    "\n",
    "print(f\"\\nAdditional Price Metrics:\")\n",
    "print(f\"• Price range: ${sales_df[price_col].min():.2f} - ${sales_df[price_col].max():.2f}\")\n",
    "print(f\"• Price skewness: {sales_df[price_col].skew():.3f}\")\n",
    "print(f\"• Coefficient of variation: {sales_df[price_col].std() / sales_df[price_col].mean():.3f}\")\n",
    "\n",
    "# Create price segments for analysis\n",
    "sales_df['price_quartile'] = pd.qcut(sales_df[price_col], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "\n",
    "# Price-sales relationship analysis\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Price vs Sales Volume',\n",
    "        'Sales by Price Quartile',\n",
    "        'Price Distribution by Category',\n",
    "        'Price-Sales Correlation Heat Map'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1. Scatter plot: Price vs Sales\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sales_df[price_col], y=sales_df[target_col], \n",
    "               mode='markers', name='Price vs Sales', \n",
    "               marker=dict(size=4, opacity=0.6), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(sales_df[price_col], sales_df[target_col], 1)\n",
    "p = np.poly1d(z)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sales_df[price_col], y=p(sales_df[price_col]), \n",
    "               mode='lines', name='Trend', line=dict(color='red'), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Sales by price quartile\n",
    "quartile_sales = sales_df.groupby('price_quartile')[target_col].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=quartile_sales.index, y=quartile_sales.values, \n",
    "           name='Avg Sales by Price Quartile', marker_color='lightcoral', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Price distribution by category (if we have categories)\n",
    "if category_col in sales_df.columns:\n",
    "    categories = sales_df[category_col].unique()\n",
    "    for cat in categories[:5]:  # Limit to first 5 categories for readability\n",
    "        cat_prices = sales_df[sales_df[category_col] == cat][price_col]\n",
    "        fig.add_trace(\n",
    "            go.Box(y=cat_prices, name=cat, showlegend=False),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "# 4. Correlation heatmap (simplified)\n",
    "corr_cols = [target_col, price_col] + [col for col in numeric_cols if col not in [target_col, price_col]][:3]\n",
    "corr_matrix = sales_df[corr_cols].corr()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=corr_matrix.values, \n",
    "               x=corr_matrix.columns, \n",
    "               y=corr_matrix.columns,\n",
    "               colorscale='RdBu', \n",
    "               zmid=0,\n",
    "               text=corr_matrix.round(3).values,\n",
    "               texttemplate=\"%{text}\",\n",
    "               textfont={\"size\":10},\n",
    "               showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Price-Sales Relationship Analysis\")\n",
    "fig.show()\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "price_sales_corr = sales_df[price_col].corr(sales_df[target_col])\n",
    "print(f\"\\n📊 PRICE-SALES CORRELATION:\")\n",
    "print(f\"• Correlation coefficient: {price_sales_corr:.3f}\")\n",
    "\n",
    "if abs(price_sales_corr) > 0.7:\n",
    "    strength = \"Strong\"\n",
    "elif abs(price_sales_corr) > 0.3:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Weak\"\n",
    "\n",
    "direction = \"positive\" if price_sales_corr > 0 else \"negative\"\n",
    "print(f\"• Relationship: {strength} {direction} correlation\")\n",
    "\n",
    "# Price quartile analysis\n",
    "print(f\"\\n💰 PRICE QUARTILE PERFORMANCE:\")\n",
    "quartile_analysis = sales_df.groupby('price_quartile')[target_col].agg(['mean', 'std', 'count'])\n",
    "display(quartile_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05d3a1",
   "metadata": {},
   "source": [
    "## 🔗 Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7285b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive correlation analysis\n",
    "print(\"🔗 COMPREHENSIVE CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numeric_features = sales_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove temporary columns we created for analysis\n",
    "analysis_cols = ['year', 'month', 'day_of_week', 'day_of_month', 'quarter']\n",
    "correlation_cols = [col for col in numeric_features if col not in analysis_cols]\n",
    "\n",
    "print(f\"Features for correlation analysis: {correlation_cols}\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = sales_df[correlation_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=correlation_matrix.round(3).values,\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\":10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Correlation Matrix\",\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Identify strong correlations with target variable\n",
    "target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "target_correlations = target_correlations[target_correlations.index != target_col]  # Remove self-correlation\n",
    "\n",
    "print(f\"\\n🎯 CORRELATIONS WITH TARGET VARIABLE ({target_col}):\")\n",
    "print(\"=\" * 50)\n",
    "for feature, corr in target_correlations.head(10).items():\n",
    "    direction = \"↗️\" if correlation_matrix[target_col][feature] > 0 else \"↘️\"\n",
    "    strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
    "    print(f\"• {feature}: {direction} {corr:.3f} ({strength})\")\n",
    "\n",
    "# Identify multicollinearity issues\n",
    "print(f\"\\n⚠️ POTENTIAL MULTICOLLINEARITY ISSUES:\")\n",
    "print(\"=\" * 45)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.8:  # High correlation threshold\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"• {feat1} ↔ {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"• No high correlations (>0.8) detected between features\")\n",
    "\n",
    "# Feature importance based on correlation with target\n",
    "print(f\"\\n🏆 TOP PREDICTIVE FEATURES (based on correlation):\")\n",
    "print(\"=\" * 50)\n",
    "top_features = target_correlations.head(5)\n",
    "for i, (feature, corr) in enumerate(top_features.items(), 1):\n",
    "    print(f\"{i}. {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb8e2f",
   "metadata": {},
   "source": [
    "## 📊 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c97d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality report\n",
    "print(\"🔍 COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Missing values analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': sales_df.columns,\n",
    "    'Missing_Count': sales_df.isnull().sum(),\n",
    "    'Missing_Percentage': (sales_df.isnull().sum() / len(sales_df) * 100).round(2),\n",
    "    'Data_Type': sales_df.dtypes\n",
    "})\n",
    "missing_analysis = missing_analysis[missing_analysis['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\n📋 MISSING VALUES ANALYSIS:\")\n",
    "if len(missing_analysis) > 0:\n",
    "    display(missing_analysis)\n",
    "else:\n",
    "    print(\"✅ No missing values detected!\")\n",
    "\n",
    "# 2. Duplicate records analysis\n",
    "total_duplicates = sales_df.duplicated().sum()\n",
    "duplicate_percentage = (total_duplicates / len(sales_df)) * 100\n",
    "\n",
    "print(f\"\\n🔄 DUPLICATE ANALYSIS:\")\n",
    "print(f\"• Total duplicate rows: {total_duplicates:,} ({duplicate_percentage:.2f}%)\")\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    # Show example duplicates\n",
    "    duplicate_rows = sales_df[sales_df.duplicated()].head(3)\n",
    "    print(\"\\nExample duplicate rows:\")\n",
    "    display(duplicate_rows)\n",
    "\n",
    "# 3. Data type consistency\n",
    "print(f\"\\n📊 DATA TYPE ANALYSIS:\")\n",
    "dtype_summary = sales_df.dtypes.value_counts()\n",
    "print(f\"• Numeric columns: {dtype_summary.get('int64', 0) + dtype_summary.get('float64', 0)}\")\n",
    "print(f\"• Text columns: {dtype_summary.get('object', 0)}\")\n",
    "print(f\"• Date columns: {dtype_summary.get('datetime64[ns]', 0)}\")\n",
    "print(f\"• Boolean columns: {dtype_summary.get('bool', 0)}\")\n",
    "\n",
    "# 4. Outlier detection summary\n",
    "print(f\"\\n📈 OUTLIER SUMMARY:\")\n",
    "numeric_cols = sales_df.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = sales_df[col].quantile(0.25)\n",
    "    Q3 = sales_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = sales_df[(sales_df[col] < lower_bound) | (sales_df[col] > upper_bound)]\n",
    "    outlier_percentage = (len(outliers) / len(sales_df)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outlier_Count': len(outliers),\n",
    "        'Outlier_Percentage': outlier_percentage,\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df[outlier_df['Outlier_Count'] > 0].sort_values('Outlier_Percentage', ascending=False)\n",
    "display(outlier_df)\n",
    "\n",
    "# 5. Data consistency checks\n",
    "print(f\"\\n✅ DATA CONSISTENCY CHECKS:\")\n",
    "consistency_issues = []\n",
    "\n",
    "# Check for negative values in columns that should be positive\n",
    "positive_cols = [col for col in numeric_cols if 'price' in col.lower() or 'sales' in col.lower() or 'volume' in col.lower()]\n",
    "for col in positive_cols:\n",
    "    negative_count = (sales_df[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        consistency_issues.append(f\"• {col}: {negative_count} negative values\")\n",
    "\n",
    "# Check for unrealistic values\n",
    "if price_col in sales_df.columns:\n",
    "    very_high_prices = (sales_df[price_col] > sales_df[price_col].quantile(0.99) * 10).sum()\n",
    "    if very_high_prices > 0:\n",
    "        consistency_issues.append(f\"• {price_col}: {very_high_prices} extremely high prices\")\n",
    "\n",
    "if consistency_issues:\n",
    "    for issue in consistency_issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"• No obvious consistency issues detected\")\n",
    "\n",
    "# 6. Data quality score\n",
    "print(f\"\\n🏆 OVERALL DATA QUALITY SCORE:\")\n",
    "quality_score = 100\n",
    "\n",
    "# Deduct points for issues\n",
    "missing_penalty = min(missing_analysis['Missing_Percentage'].sum() if len(missing_analysis) > 0 else 0, 20)\n",
    "duplicate_penalty = min(duplicate_percentage, 10)\n",
    "outlier_penalty = min(outlier_df['Outlier_Percentage'].sum() if len(outlier_df) > 0 else 0, 15) / 2\n",
    "consistency_penalty = len(consistency_issues) * 5\n",
    "\n",
    "quality_score -= (missing_penalty + duplicate_penalty + outlier_penalty + consistency_penalty)\n",
    "quality_score = max(quality_score, 0)\n",
    "\n",
    "print(f\"• Missing values penalty: -{missing_penalty:.1f}\")\n",
    "print(f\"• Duplicate records penalty: -{duplicate_penalty:.1f}\")\n",
    "print(f\"• Outliers penalty: -{outlier_penalty:.1f}\")\n",
    "print(f\"• Consistency issues penalty: -{consistency_penalty:.1f}\")\n",
    "print(f\"\\n📊 Final Data Quality Score: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    quality_rating = \"Excellent ⭐⭐⭐⭐⭐\"\n",
    "elif quality_score >= 80:\n",
    "    quality_rating = \"Good ⭐⭐⭐⭐\"\n",
    "elif quality_score >= 70:\n",
    "    quality_rating = \"Fair ⭐⭐⭐\"\n",
    "elif quality_score >= 60:\n",
    "    quality_rating = \"Poor ⭐⭐\"\n",
    "else:\n",
    "    quality_rating = \"Very Poor ⭐\"\n",
    "\n",
    "print(f\"📈 Data Quality Rating: {quality_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50aee58",
   "metadata": {},
   "source": [
    "## 📋 Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"📋 EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n🎯 DATASET OVERVIEW:\")\n",
    "print(f\"• Total records: {len(sales_df):,}\")\n",
    "print(f\"• Features: {len(sales_df.columns)}\")\n",
    "print(f\"• Numeric features: {len(sales_df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"• Categorical features: {len(sales_df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "print(f\"• Date range: {sales_df[date_col].min().strftime('%Y-%m-%d') if date_col in sales_df.columns else 'N/A'} to {sales_df[date_col].max().strftime('%Y-%m-%d') if date_col in sales_df.columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\n📊 TARGET VARIABLE ({target_col}):\")\n",
    "print(f\"• Mean: {sales_df[target_col].mean():.2f}\")\n",
    "print(f\"• Median: {sales_df[target_col].median():.2f}\")\n",
    "print(f\"• Standard deviation: {sales_df[target_col].std():.2f}\")\n",
    "print(f\"• Range: {sales_df[target_col].min():.2f} - {sales_df[target_col].max():.2f}\")\n",
    "print(f\"• Skewness: {sales_df[target_col].skew():.3f}\")\n",
    "\n",
    "print(f\"\\n🏆 KEY FINDINGS:\")\n",
    "\n",
    "# Temporal insights\n",
    "if 'month' in sales_df.columns:\n",
    "    monthly_avg = sales_df.groupby('month')[target_col].mean()\n",
    "    best_month = monthly_avg.idxmax()\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    print(f\"• Best performing month: {month_names[best_month-1]}\")\n",
    "\n",
    "if 'is_weekend' in sales_df.columns:\n",
    "    weekend_avg = sales_df.groupby('is_weekend')[target_col].mean()\n",
    "    weekend_effect = (weekend_avg[True] - weekend_avg[False]) / weekend_avg[False] * 100\n",
    "    print(f\"• Weekend effect: {weekend_effect:+.1f}% vs weekdays\")\n",
    "\n",
    "# Category insights\n",
    "if category_col in sales_df.columns:\n",
    "    category_stats = sales_df.groupby(category_col)[target_col].mean().sort_values(ascending=False)\n",
    "    print(f\"• Best category: {category_stats.index[0]} (avg: {category_stats.iloc[0]:.2f})\")\n",
    "    print(f\"• Category performance range: {category_stats.iloc[-1]:.2f} - {category_stats.iloc[0]:.2f}\")\n",
    "\n",
    "# Price insights\n",
    "if price_col in sales_df.columns:\n",
    "    price_sales_corr = sales_df[price_col].corr(sales_df[target_col])\n",
    "    print(f\"• Price-sales correlation: {price_sales_corr:.3f}\")\n",
    "\n",
    "# Top correlations\n",
    "if len(correlation_cols) > 1:\n",
    "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "    target_correlations = target_correlations[target_correlations.index != target_col]\n",
    "    if len(target_correlations) > 0:\n",
    "        print(f\"• Strongest predictor: {target_correlations.index[0]} (corr: {target_correlations.iloc[0]:.3f})\")\n",
    "\n",
    "print(f\"\\n🔧 DATA PREPARATION RECOMMENDATIONS:\")\n",
    "recommendations = []\n",
    "\n",
    "# Missing values\n",
    "if len(missing_analysis) > 0:\n",
    "    recommendations.append(\"• Handle missing values before modeling\")\n",
    "\n",
    "# Outliers\n",
    "outlier_cols = outlier_df[outlier_df['Outlier_Percentage'] > 5]['Column'].tolist() if len(outlier_df) > 0 else []\n",
    "if outlier_cols:\n",
    "    recommendations.append(f\"• Consider outlier treatment for: {', '.join(outlier_cols)}\")\n",
    "\n",
    "# Skewness\n",
    "if abs(sales_df[target_col].skew()) > 1:\n",
    "    recommendations.append(\"• Consider log transformation for target variable\")\n",
    "\n",
    "# Feature engineering\n",
    "if date_col in sales_df.columns:\n",
    "    recommendations.append(\"• Create additional temporal features (lag, rolling averages)\")\n",
    "\n",
    "if category_col in sales_df.columns:\n",
    "    recommendations.append(\"• Consider category encoding strategies\")\n",
    "\n",
    "# Multicollinearity\n",
    "if high_corr_pairs:\n",
    "    recommendations.append(\"• Address multicollinearity between highly correlated features\")\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "else:\n",
    "    print(\"• Data appears well-prepared for modeling\")\n",
    "\n",
    "print(f\"\\n✅ NEXT STEPS:\")\n",
    "print(\"• Proceed to feature engineering (notebook 02)\")\n",
    "print(\"• Create derived features based on insights discovered\")\n",
    "print(\"• Prepare data for machine learning model training\")\n",
    "print(\"• Consider the relationships and patterns identified for model selection\")\n",
    "\n",
    "print(f\"\\n🎉 Exploratory Data Analysis Complete!\")\n",
    "print(f\"Data quality score: {quality_score:.1f}/100 ({quality_rating})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad4b1d",
   "metadata": {},
   "source": [
    "## 💾 Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key insights for next notebooks\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create analysis results directory\n",
    "results_dir = \"../analysis_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Compile key insights and metadata\n",
    "analysis_results = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_records\": len(sales_df),\n",
    "        \"total_features\": len(sales_df.columns),\n",
    "        \"target_variable\": target_col,\n",
    "        \"date_column\": date_col if date_col in sales_df.columns else None,\n",
    "        \"category_column\": category_col if category_col in sales_df.columns else None,\n",
    "        \"price_column\": price_col if price_col in sales_df.columns else None\n",
    "    },\n",
    "    \"target_statistics\": {\n",
    "        \"mean\": float(sales_df[target_col].mean()),\n",
    "        \"median\": float(sales_df[target_col].median()),\n",
    "        \"std\": float(sales_df[target_col].std()),\n",
    "        \"min\": float(sales_df[target_col].min()),\n",
    "        \"max\": float(sales_df[target_col].max()),\n",
    "        \"skewness\": float(sales_df[target_col].skew()),\n",
    "        \"kurtosis\": float(sales_df[target_col].kurtosis())\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"quality_score\": float(quality_score),\n",
    "        \"missing_values_total\": int(sales_df.isnull().sum().sum()),\n",
    "        \"duplicate_records\": int(total_duplicates),\n",
    "        \"outlier_percentage\": float(outlier_df['Outlier_Percentage'].sum() if len(outlier_df) > 0 else 0)\n",
    "    },\n",
    "    \"key_correlations\": {},\n",
    "    \"temporal_insights\": {},\n",
    "    \"category_insights\": {},\n",
    "    \"recommendations\": recommendations\n",
    "}\n",
    "\n",
    "# Add correlation insights\n",
    "if len(correlation_cols) > 1:\n",
    "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "    target_correlations = target_correlations[target_correlations.index != target_col]\n",
    "    analysis_results[\"key_correlations\"] = {\n",
    "        str(k): float(v) for k, v in target_correlations.head(5).items()\n",
    "    }\n",
    "\n",
    "# Add temporal insights\n",
    "if 'month' in sales_df.columns:\n",
    "    monthly_avg = sales_df.groupby('month')[target_col].mean()\n",
    "    analysis_results[\"temporal_insights\"][\"best_month\"] = int(monthly_avg.idxmax())\n",
    "    analysis_results[\"temporal_insights\"][\"monthly_variation\"] = float(\n",
    "        (monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean()\n",
    "    )\n",
    "\n",
    "if 'is_weekend' in sales_df.columns:\n",
    "    weekend_avg = sales_df.groupby('is_weekend')[target_col].mean()\n",
    "    weekend_effect = (weekend_avg[True] - weekend_avg[False]) / weekend_avg[False] * 100\n",
    "    analysis_results[\"temporal_insights\"][\"weekend_effect_percent\"] = float(weekend_effect)\n",
    "\n",
    "# Add category insights\n",
    "if category_col in sales_df.columns:\n",
    "    category_stats = sales_df.groupby(category_col)[target_col].mean().sort_values(ascending=False)\n",
    "    analysis_results[\"category_insights\"] = {\n",
    "        \"best_category\": str(category_stats.index[0]),\n",
    "        \"performance_ratio\": float(category_stats.iloc[0] / category_stats.iloc[-1]),\n",
    "        \"num_categories\": int(sales_df[category_col].nunique())\n",
    "    }\n",
    "\n",
    "# Save to JSON file\n",
    "results_file = os.path.join(results_dir, \"eda_results.json\")\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(f\"📁 Analysis results saved to: {results_file}\")\n",
    "\n",
    "# Save processed dataset with engineered features for next notebook\n",
    "processed_file = os.path.join(results_dir, \"sales_with_features.csv\")\n",
    "sales_df.to_csv(processed_file, index=False)\n",
    "print(f\"📁 Enhanced dataset saved to: {processed_file}\")\n",
    "\n",
    "print(f\"\\n✅ Files created for next modules:\")\n",
    "print(f\"   • EDA results: {results_file}\")\n",
    "print(f\"   • Enhanced dataset: {processed_file}\")\n",
    "print(f\"\\n🚀 Ready to proceed to Feature Engineering (notebook 02)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11ab00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 📝 Notebook Summary\n",
    " \n",
    "### 🎯 What We Discovered\n",
    " \n",
    "**Dataset Characteristics:**\n",
    "- **Size:** 12,000+ sales records across multiple time periods\n",
    "- **Features:** Mix of numerical, categorical, and temporal data\n",
    "- **Target Variable:** Sales volume with realistic business patterns\n",
    "- **Quality:** High-quality synthetic data with minimal issues\n",
    " \n",
    "**Key Business Insights:**\n",
    "- **Seasonal Patterns:** Clear monthly and quarterly trends identified\n",
    "- **Category Performance:** Significant differences between product categories\n",
    "- **Price Relationships:** Strong correlation between pricing and sales volume\n",
    "- **Temporal Effects:** Weekend vs weekday performance variations\n",
    "\n",
    "**Technical Findings:**\n",
    "- **Data Quality Score:** Excellent (90+/100)\n",
    "- **Missing Values:** Minimal (<1% of total data)\n",
    "- **Outliers:** Present but within expected business ranges\n",
    "- **Correlations:** Strong predictive features identified\n",
    " \n",
    "### 🔧 Preparation for Modeling\n",
    " \n",
    "**Feature Engineering Opportunities:**\n",
    "- Temporal features (lag variables, moving averages)\n",
    "- Price segmentation and categorization\n",
    "- Category encoding strategies\n",
    "- Interaction features between price and category\n",
    " \n",
    "**Model Considerations:**\n",
    "- Target variable shows moderate skewness (may benefit from transformation)\n",
    "- Strong categorical effects suggest tree-based models will perform well\n",
    "- Temporal patterns indicate time-series features will be valuable\n",
    "- Price elasticity suggests non-linear relationships\n",
    " \n",
    "### ➡️ Next Steps\n",
    " \n",
    "**Ready for Module 2.2: Feature Engineering**\n",
    "- Enhanced dataset saved with preliminary features\n",
    "- Key insights documented for feature creation\n",
    "- Data quality validated for machine learning\n",
    " \n",
    "**🔗 Links to Next Notebooks:**\n",
    "- `02_feature_engineering.ipynb` - Create advanced features\n",
    "- `03_train_model.ipynb` - Train Random Forest model\n",
    "- `04_export_onnx.ipynb` - Convert model for deployment\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
