{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Sales Forecasting - Part 2\n",
    "## Module 2: Predictive Model - Advanced Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Complete advanced feature engineering and preprocessing\n",
    "\n",
    "**This notebook covers:**\n",
    "- Lag features and moving averages\n",
    "- Interaction features and combinations\n",
    "- Data preprocessing and encoding\n",
    "- Feature validation and quality assessment\n",
    "\n",
    "**Prerequisites:** Complete `02_feature_engineering_part1.ipynb` first\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Setup and Load Intermediate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Part 2 feature engineering started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data from Part 1\n",
    "data_dir = Path(\"../../datasets\")\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "print(\"üìÇ Loading intermediate data from Part 1...\")\n",
    "\n",
    "# Load enriched data from Part 1\n",
    "enriched_data = pd.read_csv(processed_dir / \"enriched_data_part1.csv\")\n",
    "\n",
    "# Convert date column back to datetime\n",
    "enriched_data['date'] = pd.to_datetime(enriched_data['date'])\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {enriched_data.shape}\")\n",
    "print(f\"üìä Starting Part 2 with {len(enriched_data.columns)} features\")\n",
    "\n",
    "# Quick data check\n",
    "print(f\"\\nüìã Data Overview:\")\n",
    "print(f\"   Records: {len(enriched_data):,}\")\n",
    "print(f\"   Date range: {enriched_data['date'].min()} to {enriched_data['date'].max()}\")\n",
    "print(f\"   Missing values: {enriched_data.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Lag Features and Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df):\n",
    "    \"\"\"\n",
    "    Create lag features and moving averages for time series patterns\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating lag features and moving averages...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by date to ensure proper lag calculation\n",
    "    df = df.sort_values(['date', 'product_id']).reset_index(drop=True)\n",
    "    \n",
    "    # Create daily aggregations first\n",
    "    print(\"   üìÖ Creating daily aggregations...\")\n",
    "    \n",
    "    daily_sales = df.groupby(['date', 'product_id']).agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    daily_sales.columns = ['date', 'product_id', 'daily_quantity', 'daily_revenue', 'daily_transactions']\n",
    "    \n",
    "    # Create category-level daily aggregations\n",
    "    product_category_map = df[['product_id', 'category']].drop_duplicates()\n",
    "    daily_sales = daily_sales.merge(product_category_map, on='product_id', how='left')\n",
    "    \n",
    "    category_daily = df.groupby(['date', 'category']).agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    category_daily.columns = ['date', 'category', 'category_daily_quantity', 'category_daily_revenue']\n",
    "    \n",
    "    # Overall daily metrics\n",
    "    overall_daily = df.groupby('date').agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    overall_daily.columns = ['date', 'overall_daily_quantity', 'overall_daily_revenue', 'overall_daily_transactions']\n",
    "    \n",
    "    return df, daily_sales, category_daily, overall_daily\n",
    "\n",
    "# Create daily aggregations\n",
    "enriched_data, daily_sales, category_daily, overall_daily = create_lag_features(enriched_data)\n",
    "\n",
    "print(f\"‚úÖ Daily aggregations created:\")\n",
    "print(f\"   Product-level daily data: {daily_sales.shape}\")\n",
    "print(f\"   Category-level daily data: {category_daily.shape}\")\n",
    "print(f\"   Overall daily data: {overall_daily.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(daily_sales, category_daily, overall_daily):\n",
    "    \"\"\"\n",
    "    Add lag features for different time windows\n",
    "    \"\"\"\n",
    "    print(\"‚è∞ Creating lag features...\")\n",
    "    \n",
    "    lag_windows = [1, 3, 7, 14, 30]  # 1 day, 3 days, 1 week, 2 weeks, 1 month\n",
    "    \n",
    "    # Product-level lag features\n",
    "    for lag in lag_windows:\n",
    "        daily_sales[f'quantity_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_quantity'].shift(lag)\n",
    "        daily_sales[f'revenue_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_revenue'].shift(lag)\n",
    "        daily_sales[f'transactions_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_transactions'].shift(lag)\n",
    "    \n",
    "    # Category-level lag features\n",
    "    for lag in lag_windows:\n",
    "        category_daily[f'category_quantity_lag_{lag}d'] = category_daily.groupby('category')['category_daily_quantity'].shift(lag)\n",
    "        category_daily[f'category_revenue_lag_{lag}d'] = category_daily.groupby('category')['category_daily_revenue'].shift(lag)\n",
    "    \n",
    "    # Overall market lag features\n",
    "    for lag in lag_windows:\n",
    "        overall_daily[f'market_quantity_lag_{lag}d'] = overall_daily['overall_daily_quantity'].shift(lag)\n",
    "        overall_daily[f'market_revenue_lag_{lag}d'] = overall_daily['overall_daily_revenue'].shift(lag)\n",
    "        overall_daily[f'market_transactions_lag_{lag}d'] = overall_daily['overall_daily_transactions'].shift(lag)\n",
    "    \n",
    "    print(f\"‚úÖ Lag features created for {len(lag_windows)} time windows\")\n",
    "    \n",
    "    return daily_sales, category_daily, overall_daily\n",
    "\n",
    "# Add lag features\n",
    "daily_sales, category_daily, overall_daily = add_lag_features(daily_sales, category_daily, overall_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_moving_averages(daily_sales, category_daily, overall_daily):\n",
    "    \"\"\"\n",
    "    Add moving averages for different time windows\n",
    "    \"\"\"\n",
    "    print(\"üìà Creating moving averages...\")\n",
    "    \n",
    "    ma_windows = [3, 7, 14, 30]  # 3 days, 1 week, 2 weeks, 1 month\n",
    "    \n",
    "    # Product-level moving averages\n",
    "    for window in ma_windows:\n",
    "        daily_sales[f'quantity_ma_{window}d'] = daily_sales.groupby('product_id')['daily_quantity'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        daily_sales[f'revenue_ma_{window}d'] = daily_sales.groupby('product_id')['daily_revenue'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Category-level moving averages\n",
    "    for window in ma_windows:\n",
    "        category_daily[f'category_quantity_ma_{window}d'] = category_daily.groupby('category')['category_daily_quantity'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        category_daily[f'category_revenue_ma_{window}d'] = category_daily.groupby('category')['category_daily_revenue'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Market-level moving averages\n",
    "    for window in ma_windows:\n",
    "        overall_daily[f'market_quantity_ma_{window}d'] = overall_daily['overall_daily_quantity'].rolling(\n",
    "            window=window, min_periods=1\n",
    "        ).mean()\n",
    "        overall_daily[f'market_revenue_ma_{window}d'] = overall_daily['overall_daily_revenue'].rolling(\n",
    "            window=window, min_periods=1\n",
    "        ).mean()\n",
    "    \n",
    "    print(f\"‚úÖ Moving averages created for {len(ma_windows)} time windows\")\n",
    "    \n",
    "    return daily_sales, category_daily, overall_daily\n",
    "\n",
    "# Add moving averages\n",
    "daily_sales, category_daily, overall_daily = add_moving_averages(daily_sales, category_daily, overall_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_features(daily_sales):\n",
    "    \"\"\"\n",
    "    Add trend and growth features\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating trend features...\")\n",
    "    \n",
    "    # Short-term vs long-term trends\n",
    "    daily_sales['quantity_trend_7_30'] = daily_sales['quantity_ma_7d'] / (daily_sales['quantity_ma_30d'] + 0.01)\n",
    "    daily_sales['revenue_trend_7_30'] = daily_sales['revenue_ma_7d'] / (daily_sales['revenue_ma_30d'] + 0.01)\n",
    "    \n",
    "    # Growth rates\n",
    "    daily_sales['quantity_growth_7d'] = (\n",
    "        (daily_sales['quantity_ma_7d'] - daily_sales['quantity_lag_7d']) / \n",
    "        (daily_sales['quantity_lag_7d'] + 0.01) * 100\n",
    "    ).clip(-100, 500)  # Cap extreme values\n",
    "    \n",
    "    daily_sales['revenue_growth_7d'] = (\n",
    "        (daily_sales['revenue_ma_7d'] - daily_sales['revenue_lag_7d']) / \n",
    "        (daily_sales['revenue_lag_7d'] + 0.01) * 100\n",
    "    ).clip(-100, 500)\n",
    "    \n",
    "    print(\"‚úÖ Trend and growth features created\")\n",
    "    \n",
    "    return daily_sales\n",
    "\n",
    "# Add trend features\n",
    "daily_sales = add_trend_features(daily_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lag_features(df, daily_sales, category_daily, overall_daily):\n",
    "    \"\"\"\n",
    "    Merge lag features back to main dataset\n",
    "    \"\"\"\n",
    "    print(\"üîó Merging lag features to main dataset...\")\n",
    "    \n",
    "    df = df.merge(daily_sales, on=['date', 'product_id'], how='left')\n",
    "    df = df.merge(category_daily, on=['date', 'category'], how='left')\n",
    "    df = df.merge(overall_daily, on='date', how='left')\n",
    "    \n",
    "    # Fill missing lag values with 0 or forward fill for first few records\n",
    "    lag_columns = [col for col in df.columns if 'lag_' in col or '_ma_' in col or '_growth_' in col or '_trend_' in col]\n",
    "    \n",
    "    for col in lag_columns:\n",
    "        if 'growth' in col or 'trend' in col:\n",
    "            df[col] = df[col].fillna(0)  # Fill growth/trend with 0\n",
    "        else:\n",
    "            df[col] = df.groupby('product_id')[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"‚úÖ Lag features merged: {len(lag_columns)} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Merge all lag features\n",
    "enriched_data = merge_lag_features(enriched_data, daily_sales, category_daily, overall_daily)\n",
    "\n",
    "print(f\"\\nüìä Dataset after lag features: {enriched_data.shape}\")\n",
    "print(f\"üìà New lag features: {len([col for col in enriched_data.columns if 'lag_' in col or '_ma_' in col or '_growth_' in col or '_trend_' in col])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Lag Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag feature patterns\n",
    "print(\"üìä Lag Feature Analysis:\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Moving average comparison\n",
    "sample_product = enriched_data['product_id'].value_counts().index[0]\n",
    "product_data = enriched_data[enriched_data['product_id'] == sample_product].sort_values('date')\n",
    "\n",
    "if len(product_data) > 10:\n",
    "    axes[0, 0].plot(product_data['date'], product_data['daily_quantity'], label='Daily', alpha=0.5)\n",
    "    axes[0, 0].plot(product_data['date'], product_data['quantity_ma_7d'], label='7-day MA')\n",
    "    axes[0, 0].plot(product_data['date'], product_data['quantity_ma_30d'], label='30-day MA')\n",
    "    axes[0, 0].set_title(f'Sales Trends for {sample_product}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Growth distribution\n",
    "growth_data = enriched_data['quantity_growth_7d'].dropna()\n",
    "axes[0, 1].hist(growth_data, bins=30, alpha=0.7)\n",
    "axes[0, 1].set_title('7-Day Quantity Growth Distribution')\n",
    "axes[0, 1].set_xlabel('Growth Rate (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Lag correlation analysis\n",
    "lag_corr_data = enriched_data[['quantity', 'quantity_lag_1d', 'quantity_lag_7d', 'quantity_lag_30d']].corr()\n",
    "sns.heatmap(lag_corr_data, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Lag Feature Correlations')\n",
    "\n",
    "# Trend vs actual sales\n",
    "trend_data = enriched_data[enriched_data['quantity_trend_7_30'].notna()]\n",
    "if len(trend_data) > 100:\n",
    "    sample_trend = trend_data.sample(n=min(1000, len(trend_data)))\n",
    "    axes[1, 1].scatter(sample_trend['quantity_trend_7_30'], sample_trend['quantity'], alpha=0.5)\n",
    "    axes[1, 1].set_title('Trend Indicator vs Actual Sales')\n",
    "    axes[1, 1].set_xlabel('7-day/30-day Trend Ratio')\n",
    "    axes[1, 1].set_ylabel('Quantity Sold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights about lag features\n",
    "print(f\"\\nüìà Key Lag Feature Insights:\")\n",
    "lag_corr = enriched_data[['quantity', 'quantity_lag_1d']].corr().iloc[0, 1]\n",
    "avg_growth = enriched_data['quantity_growth_7d'].mean()\n",
    "growth_volatility = enriched_data['quantity_growth_7d'].std()\n",
    "\n",
    "print(f\"   1-day lag correlation: {lag_corr:.3f}\")\n",
    "print(f\"   Average 7-day growth: {avg_growth:.2f}%\")\n",
    "print(f\"   Growth volatility: {growth_volatility:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 4: Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features between different variable groups\n",
    "    \"\"\"\n",
    "    print(\"üîó Creating interaction features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price and temporal interactions\n",
    "    print(\"   üí∞üìÖ Creating price-temporal interactions...\")\n",
    "    \n",
    "    # Price interactions with time features\n",
    "    df['price_x_weekend'] = df['unit_price'] * df['is_weekend']\n",
    "    df['price_x_holiday'] = df['unit_price'] * df['is_holiday_season']\n",
    "    df['discount_x_weekend'] = df['discount_percentage'] * df['is_weekend']\n",
    "    df['discount_x_holiday'] = df['discount_percentage'] * df['is_holiday_season']\n",
    "    \n",
    "    # Price and season interactions\n",
    "    df['price_x_quarter'] = df['unit_price'] * df['quarter']\n",
    "    df['price_x_month_sin'] = df['unit_price'] * df['month_sin']\n",
    "    df['price_x_month_cos'] = df['unit_price'] * df['month_cos']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply first set of interactions\n",
    "enriched_data = create_interaction_features(enriched_data)\n",
    "print(f\"‚úÖ Price-temporal interactions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_product_interactions(df):\n",
    "    \"\"\"\n",
    "    Create customer and product interaction features\n",
    "    \"\"\"\n",
    "    print(\"üë•üè∑Ô∏è Creating customer-product interactions...\")\n",
    "    \n",
    "    # Customer segment and price interactions\n",
    "    df['segment_x_price'] = df['customer_segment_encoded'] * df['unit_price']\n",
    "    df['segment_x_discount'] = df['customer_segment_encoded'] * df['discount_percentage']\n",
    "    \n",
    "    # Customer lifecycle and product interactions\n",
    "    lifecycle_encoding = {\n",
    "        'New': 1, 'Active': 2, 'Regular': 3, 'Loyal': 4, 'At_Risk': 2, 'Dormant': 1\n",
    "    }\n",
    "    df['lifecycle_encoded'] = df['customer_lifecycle_stage'].map(lifecycle_encoding)\n",
    "    df['lifecycle_x_price'] = df['lifecycle_encoded'] * df['unit_price']\n",
    "    df['lifecycle_x_rating'] = df['lifecycle_encoded'] * df['rating']\n",
    "    \n",
    "    # Customer value and product quality interactions\n",
    "    df['clv_x_rating'] = df['customer_total_spent'] * df['rating']\n",
    "    df['clv_x_brand_performance'] = df['customer_total_spent'] * df['brand_avg_rating']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply customer-product interactions\n",
    "enriched_data = create_customer_product_interactions(enriched_data)\n",
    "print(f\"‚úÖ Customer-product interactions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_interactions(df):\n",
    "    \"\"\"\n",
    "    Create advanced interaction features\n",
    "    \"\"\"\n",
    "    print(\"üßÆ Creating advanced interaction features...\")\n",
    "    \n",
    "    # Regional and product interactions\n",
    "    df['region_performance_x_price'] = df['customer_vs_region_performance'] * df['unit_price']\n",
    "    df['region_avg_x_price_ratio'] = df['region_avg_order_value'] * df['price_vs_category_avg']\n",
    "    \n",
    "    # Channel and behavior interactions\n",
    "    df['channel_consistency_x_engagement'] = df['customer_channel_consistency'] * df['digital_engagement_score']\n",
    "    \n",
    "    # Device and price interactions\n",
    "    device_encoding = {'Desktop': 3, 'Mobile': 2, 'Tablet': 2, 'Unknown': 1}\n",
    "    df['device_encoded'] = df['preferred_device'].map(device_encoding).fillna(1)\n",
    "    df['device_x_price'] = df['device_encoded'] * df['unit_price']\n",
    "    df['device_x_engagement'] = df['device_encoded'] * df['digital_engagement_score']\n",
    "    \n",
    "    # Temporal and lag feature interactions\n",
    "    df['holiday_x_trend'] = df['is_holiday_season'] * df['quantity_trend_7_30']\n",
    "    df['weekend_x_growth'] = df['is_weekend'] * df['quantity_growth_7d']\n",
    "    df['quarter_x_lag_performance'] = df['quarter'] * df['quantity_lag_7d']\n",
    "    \n",
    "    # Moving average and seasonal interactions\n",
    "    df['ma7_x_holiday'] = df['quantity_ma_7d'] * df['is_holiday_season']\n",
    "    df['ma30_x_quarter'] = df['quantity_ma_30d'] * df['quarter']\n",
    "    \n",
    "    # Product performance and time interactions\n",
    "    df['popularity_x_holiday'] = df['product_popularity_score'] * df['is_holiday_season']\n",
    "    df['popularity_x_weekend'] = df['product_popularity_score'] * df['is_weekend']\n",
    "    \n",
    "    # Brand performance and time\n",
    "    df['brand_rating_x_holiday'] = df['brand_avg_rating'] * df['is_holiday_season']\n",
    "    df['market_share_x_quarter'] = df['brand_market_share'] * df['quarter']\n",
    "    \n",
    "    # Category and customer segment interactions\n",
    "    category_encoding = {\n",
    "        'Electronics': 5, 'Clothing': 4, 'Sports': 3, \n",
    "        'Home & Garden': 3, 'Beauty': 2, 'Books': 1\n",
    "    }\n",
    "    df['category_encoded'] = df['category'].map(category_encoding)\n",
    "    df['category_x_segment'] = df['category_encoded'] * df['customer_segment_encoded']\n",
    "    df['category_x_lifecycle'] = df['category_encoded'] * df['lifecycle_encoded']\n",
    "    \n",
    "    # Three-way interactions (most important combinations)\n",
    "    df['price_x_rating_x_segment'] = df['unit_price'] * df['rating'] * df['customer_segment_encoded']\n",
    "    df['discount_x_holiday_x_category'] = df['discount_percentage'] * df['is_holiday_season'] * df['category_encoded']\n",
    "    df['trend_x_popularity_x_quarter'] = df['quantity_trend_7_30'] * df['product_popularity_score'] * df['quarter']\n",
    "    \n",
    "    # Ratio-based interactions\n",
    "    df['customer_diversity_x_category_performance'] = df['customer_diversity_score'] * df['category_avg_amount']\n",
    "    df['rfm_x_product_performance'] = (df['rfm_score'] / 100) * df['product_performance_vs_category']\n",
    "    \n",
    "    # Complex behavioral interactions\n",
    "    df['engagement_x_loyalty_x_price'] = (\n",
    "        df['digital_engagement_score'] * \n",
    "        df['customer_channel_consistency'] * \n",
    "        df['price_vs_category_avg']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply advanced interactions\n",
    "enriched_data = create_advanced_interactions(enriched_data)\n",
    "\n",
    "interaction_features = [col for col in enriched_data.columns if '_x_' in col]\n",
    "print(f\"‚úÖ All interaction features created: {len(interaction_features)} features\")\n",
    "print(f\"üìä Dataset after interactions: {enriched_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 5: Interaction Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interaction feature importance\n",
    "print(\"üìä Interaction Feature Analysis:\")\n",
    "\n",
    "# Calculate correlation of interaction features with target\n",
    "interaction_features = [col for col in enriched_data.columns if '_x_' in col]\n",
    "target_correlations = {}\n",
    "\n",
    "for feature in interaction_features[:10]:  # Analyze top 10 interaction features\n",
    "    correlation = enriched_data[['quantity', feature]].corr().iloc[0, 1]\n",
    "    if not np.isnan(correlation):\n",
    "        target_correlations[feature] = abs(correlation)\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_correlations = sorted(target_correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nüìà Top 10 Interaction Features by Correlation with Sales Quantity:\")\n",
    "for i, (feature, corr) in enumerate(sorted_correlations[:10], 1):\n",
    "    print(f\"   {i:2d}. {feature}: {corr:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Interaction Insights:\")\n",
    "if len(sorted_correlations) > 0:\n",
    "    print(f\"   Strongest interaction: {sorted_correlations[0][0]} (correlation: {sorted_correlations[0][1]:.4f})\")\n",
    "print(f\"   Total interaction features: {len(interaction_features)}\")\n",
    "print(f\"   Features with multiplicative interactions: {len([col for col in enriched_data.columns if '_x_' in col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some key interactions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price x Holiday interaction\n",
    "try:\n",
    "    holiday_price_data = enriched_data.groupby(['is_holiday_season', pd.cut(enriched_data['unit_price'], bins=5)])['quantity'].mean().unstack()\n",
    "    if holiday_price_data.shape[0] > 1 and holiday_price_data.shape[1] > 1:\n",
    "        sns.heatmap(holiday_price_data, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Average Quantity: Price vs Holiday Season')\n",
    "        axes[0, 0].set_xlabel('Price Bins')\n",
    "        axes[0, 0].set_ylabel('Holiday Season (0=No, 1=Yes)')\nexcept:\n",
    "    axes[0, 0].text(0.5, 0.5, 'Price x Holiday\\nInteraction Data', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "\n",
    "# Customer Segment x Category interaction\n",
    "try:\n",
    "    segment_category_data = enriched_data.groupby(['customer_segment', 'category'])['total_amount'].mean().unstack()\n",
    "    if segment_category_data.shape[0] > 1 and segment_category_data.shape[1] > 1:\n",
    "        sns.heatmap(segment_category_data, annot=True, fmt='.0f', cmap='viridis', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Order Value: Segment vs Category')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].tick_params(axis='y', rotation=0)\nexcept:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Segment x Category\\nInteraction Data', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "\n",
    "# Weekend x Discount interaction\n",
    "try:\n",
    "    weekend_discount_effect = enriched_data.groupby(['is_weekend', 'is_discounted'])['quantity'].mean().unstack()\n",
    "    if weekend_discount_effect.shape[0] > 1 and weekend_discount_effect.shape[1] > 1:\n",
    "        weekend_discount_effect.plot(kind='bar', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Quantity Sold: Weekend vs Discount Status')\n",
    "        axes[1, 0].set_xlabel('Weekend (0=Weekday, 1=Weekend)')\n",
    "        axes[1, 0].set_ylabel('Average Quantity')\n",
    "        axes[1, 0].legend(['No Discount', 'Discounted'])\n",
    "        axes[1, 0].tick_params(axis='x', rotation=0)\nexcept:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Weekend x Discount\\nInteraction Data', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# Rating x Price quartile interaction\n",
    "try:\n",
    "    rating_price_data = enriched_data.groupby(['price_quartile', pd.cut(enriched_data['rating'], bins=4)])['quantity'].mean().unstack()\n",
    "    if rating_price_data.shape[0] > 1 and rating_price_data.shape[1] > 1:\n",
    "        sns.heatmap(rating_price_data, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Average Quantity: Price Quartile vs Rating')\n",
    "        axes[1, 1].set_xlabel('Rating Bins')\n",
    "        axes[1, 1].set_ylabel('Price Quartile')\nexcept:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Rating x Price\\nInteraction Data', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 6: Data Preprocessing and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for machine learning by handling categorical variables and scaling\n",
    "    \"\"\"\n",
    "    print(\"üßπ Preprocessing features for machine learning...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify feature types\n",
    "    print(\"   üîç Identifying feature types...\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_columns = [\n",
    "        'transaction_id', 'date', 'customer_id', 'product_id', \n",
    "        'customer_first_purchase', 'customer_last_purchase'\n",
    "    ]\n",
    "    \n",
    "    feature_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    # Separate target variable\n",
    "    target_column = 'quantity'  # Our prediction target\n",
    "    feature_columns = [col for col in feature_columns if col != target_column]\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_columns = []\n",
    "    numerical_columns = []\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            categorical_columns.append(col)\n",
    "        else:\n",
    "            numerical_columns.append(col)\n",
    "    \n",
    "    print(f\"   üìä Found {len(numerical_columns)} numerical and {len(categorical_columns)} categorical features\")\n",
    "    \n",
    "    return df, categorical_columns, numerical_columns, target_column\n",
    "\n",
    "# Identify feature types\n",
    "enriched_data, categorical_columns, numerical_columns, target_column = preprocess_features(enriched_data)\n",
    "\n",
    "print(f\"\\nüìã Feature Type Summary:\")\n",
    "print(f\"   Categorical features: {categorical_columns}\")\n",
    "print(f\"   Numerical features: {len(numerical_columns)} features\")\n",
    "print(f\"   Target variable: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset\n",
    "    \"\"\"\n",
    "    print(\"üîß Handling missing values...\")\n",
    "    \n",
    "    # Fill missing values for numerical columns\n",
    "    for col in numerical_columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if 'price' in col.lower() or 'amount' in col.lower():\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            elif 'count' in col.lower() or 'quantity' in col.lower():\n",
    "                df[col] = df[col].fillna(0)\n",
    "            elif 'ratio' in col.lower() or 'percentage' in col.lower():\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    # Fill missing values for categorical columns\n",
    "    for col in categorical_columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Missing values handled\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "enriched_data = handle_missing_values(enriched_data, categorical_columns, numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_variables(df, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Encode categorical variables\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è Encoding categorical variables...\")\n",
    "    \n",
    "    # One-hot encode low cardinality categorical variables\n",
    "    low_cardinality_threshold = 10\n",
    "    high_cardinality_categorical = []\n",
    "    low_cardinality_categorical = []\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count <= low_cardinality_threshold:\n",
    "            low_cardinality_categorical.append(col)\n",
    "        else:\n",
    "            high_cardinality_categorical.append(col)\n",
    "    \n",
    "    print(f\"     Low cardinality ({len(low_cardinality_categorical)}): {low_cardinality_categorical}\")\n",
    "    print(f\"     High cardinality ({len(high_cardinality_categorical)}): {high_cardinality_categorical}\")\n",
    "    \n",
    "    # Label encode high cardinality categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in high_cardinality_categorical:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        numerical_columns.append(f'{col}_encoded')\n",
    "    \n",
    "    # One-hot encode low cardinality categorical variables\n",
    "    df_encoded = pd.get_dummies(\n",
    "        df[low_cardinality_categorical], \n",
    "        prefix=low_cardinality_categorical,\n",
    "        drop_first=True,\n",
    "        dummy_na=False\n",
    "    )\n",
    "    \n",
    "    return df, df_encoded, numerical_columns, label_encoders\n",
    "\n",
    "# Encode categorical variables\n",
    "enriched_data, df_encoded, numerical_columns, label_encoders = encode_categorical_variables(\n",
    "    enriched_data, categorical_columns, numerical_columns\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Categorical encoding completed:\")\n",
    "print(f\"   Label encoded features: {len(label_encoders)}\")\n",
    "print(f\"   One-hot encoded features: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_feature_matrix(df, df_encoded, numerical_columns, target_column):\n",
    "    \"\"\"\n",
    "    Create final feature matrix and target variable\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating final feature matrix...\")\n",
    "    \n",
    "    # Combine all features\n",
    "    final_features = numerical_columns + list(df_encoded.columns)\n",
    "    \n",
    "    # Create final feature matrix\n",
    "    X = pd.concat([\n",
    "        df[numerical_columns],\n",
    "        df_encoded\n",
    "    ], axis=1)\n",
    "    \n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Feature summary\n",
    "    feature_summary = {\n",
    "        'feature_names': final_features,\n",
    "        'numerical_features': numerical_columns,\n",
    "        'categorical_features': list(df_encoded.columns),\n",
    "        'label_encoders': label_encoders,\n",
    "        'target_column': target_column,\n",
    "        'total_features': len(final_features)\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Feature matrix created:\")\n",
    "    print(f\"   Total features: {len(final_features)}\")\n",
    "    print(f\"   Numerical features: {len(numerical_columns)}\")\n",
    "    print(f\"   One-hot encoded features: {len(df_encoded.columns)}\")\n",
    "    print(f\"   Target variable: {target_column}\")\n",
    "    print(f\"   Dataset shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, feature_summary\n",
    "\n",
    "# Create final feature matrix\n",
    "X, y, feature_summary = create_final_feature_matrix(\n",
    "    enriched_data, df_encoded, numerical_columns, target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 7: Feature Validation and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_feature_engineering(X, y):\n",
    "    \"\"\"\n",
    "    Validate the quality of engineered features\n",
    "    \"\"\"\n",
    "    print(\"üéØ Validating feature engineering quality...\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Feature completeness\n",
    "    print(\"   ‚úÖ Checking feature completeness...\")\n",
    "    missing_percentage = (X.isnull().sum().sum() / (X.shape[0] * X.shape[1])) * 100\n",
    "    validation_results['missing_percentage'] = missing_percentage\n",
    "    print(f\"     Missing values: {missing_percentage:.3f}%\")\n",
    "    \n",
    "    # 2. Feature variance\n",
    "    print(\"   üìä Checking feature variance...\")\n",
    "    low_variance_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].var() < 0.01:  # Very low variance threshold\n",
    "            low_variance_features.append(col)\n",
    "    \n",
    "    validation_results['low_variance_features'] = len(low_variance_features)\n",
    "    print(f\"     Low variance features: {len(low_variance_features)}\")\n",
    "    \n",
    "    # 3. Target variable distribution\n",
    "    print(\"   üéØ Analyzing target distribution...\")\n",
    "    target_skewness = y.skew()\n",
    "    target_kurtosis = y.kurtosis()\n",
    "    validation_results['target_skewness'] = target_skewness\n",
    "    validation_results['target_kurtosis'] = target_kurtosis\n",
    "    print(f\"     Target skewness: {target_skewness:.3f}\")\n",
    "    print(f\"     Target kurtosis: {target_kurtosis:.3f}\")\n",
    "    \n",
    "    # 4. Feature-target correlations\n",
    "    print(\"   üîó Analyzing feature-target relationships...\")\n",
    "    correlations = [abs(X[col].corr(y)) for col in X.columns]\n",
    "    correlations = [c for c in correlations if not np.isnan(c)]\n",
    "    \n",
    "    strong_correlations = sum(1 for c in correlations if c > 0.1)\n",
    "    validation_results['strong_correlations'] = strong_correlations\n",
    "    validation_results['avg_correlation'] = np.mean(correlations)\n",
    "    print(f\"     Features with |correlation| > 0.1: {strong_correlations}\")\n",
    "    print(f\"     Average absolute correlation: {np.mean(correlations):.4f}\")\n",
    "    \n",
    "    # 5. Feature engineering success metrics\n",
    "    print(\"   üìà Feature engineering success metrics...\")\n",
    "    \n",
    "    success_metrics = {\n",
    "        'total_features_created': X.shape[1],\n",
    "        'feature_density': X.shape[1] / X.shape[0],\n",
    "        'missing_data_handled': missing_percentage < 1.0,\n",
    "        'sufficient_variance': len(low_variance_features) < X.shape[1] * 0.1,\n",
    "        'good_target_correlation': strong_correlations > 10,\n",
    "        'reasonable_dimensionality': X.shape[1] < X.shape[0] * 0.1\n",
    "    }\n",
    "    \n",
    "    validation_results.update(success_metrics)\n",
    "    \n",
    "    # Overall score\n",
    "    passed_checks = sum([\n",
    "        success_metrics['missing_data_handled'],\n",
    "        success_metrics['sufficient_variance'],\n",
    "        success_metrics['good_target_correlation'],\n",
    "        success_metrics['reasonable_dimensionality']\n",
    "    ])\n",
    "    \n",
    "    overall_score = (passed_checks / 4) * 100\n",
    "    validation_results['overall_score'] = overall_score\n",
    "    \n",
    "    print(f\"\\nüèÜ Feature Engineering Quality Score: {overall_score:.0f}%\")\n",
    "    \n",
    "    if overall_score >= 75:\n",
    "        print(\"‚úÖ Feature engineering quality: EXCELLENT - Ready for model training!\")\n",
    "    elif overall_score >= 50:\n",
    "        print(\"‚ö†Ô∏è  Feature engineering quality: GOOD - Some improvements possible\")\n",
    "    else:\n",
    "        print(\"‚ùå Feature engineering quality: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Perform validation\n",
    "validation_results = validate_feature_engineering(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Save Final Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving processed data\n",
    "print(f\"üíæ Saving final processed features...\")\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save feature matrix and target\n",
    "X.to_csv(processed_dir / \"feature_matrix.csv\", index=False)\n",
    "y.to_csv(processed_dir / \"target_variable.csv\", index=False)\n",
    "\n",
    "# Save feature summary\n",
    "with open(processed_dir / \"feature_summary.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_summary, f)\n",
    "\n",
    "# Save validation results\n",
    "with open(processed_dir / \"validation_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(validation_results, f)\n",
    "\n",
    "# Save a sample of the enriched dataset for inspection\n",
    "enriched_data.head(1000).to_csv(processed_dir / \"enriched_sample.csv\", index=False)\n",
    "\n",
    "# Create feature importance data for later analysis\n",
    "feature_stats = pd.DataFrame({\n",
    "    'feature_name': X.columns,\n",
    "    'mean': X.mean(),\n",
    "    'std': X.std(),\n",
    "    'min': X.min(),\n",
    "    'max': X.max(),\n",
    "    'correlation_with_target': [X[col].corr(y) for col in X.columns]\n",
    "}).round(4)\n",
    "\n",
    "feature_stats.to_csv(processed_dir / \"feature_statistics.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Final processed data saved:\")\n",
    "print(f\"   üìÅ Feature matrix: {processed_dir / 'feature_matrix.csv'}\")\n",
    "print(f\"   üìÅ Target variable: {processed_dir / 'target_variable.csv'}\")\n",
    "print(f\"   üìÅ Feature summary: {processed_dir / 'feature_summary.pkl'}\")\n",
    "print(f\"   üìÅ Validation results: {processed_dir / 'validation_results.pkl'}\")\n",
    "print(f\"   üìÅ Feature statistics: {processed_dir / 'feature_statistics.csv'}\")\n",
    "print(f\"   üìÅ Enriched sample: {processed_dir / 'enriched_sample.csv'}\")"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Final Feature Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature engineering summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Original records: {len(enriched_data):,}\")\n",
    "print(f\"   Final feature matrix: {X.shape}\")\n",
    "print(f\"   Target variable: {feature_summary['target_column']}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Feature Categories:\")\n",
    "temporal_features = len([f for f in X.columns if any(x in f.lower() for x in ['day', 'week', 'month', 'year', 'quarter', 'holiday', 'weekend', 'sin', 'cos'])])\n",
    "product_features = len([f for f in X.columns if any(x in f.lower() for x in ['price', 'brand', 'category', 'rating', 'review', 'product'])])\n",
    "customer_features = len([f for f in X.columns if any(x in f.lower() for x in ['customer', 'segment', 'lifecycle', 'rfm', 'clv'])])\n",
    "lag_features = len([f for f in X.columns if 'lag_' in f or '_ma_' in f])\n",
    "interaction_features = len([f for f in X.columns if '_x_' in f])\n",
    "\n",
    "print(f\"   Temporal features: {temporal_features}\")\n",
    "print(f\"   Product features: {product_features}\")\n",
    "print(f\"   Customer features: {customer_features}\")\n",
    "print(f\"   Lag features: {lag_features}\")\n",
    "print(f\"   Interaction features: {interaction_features}\")\n",
    "\n",
    "print(f\"\\nüìà Target Variable Statistics:\")\n",
    "print(f\"   Mean quantity: {y.mean():.2f}\")\n",
    "print(f\"   Median quantity: {y.median():.2f}\")\n",
    "print(f\"   Standard deviation: {y.std():.2f}\")\n",
    "print(f\"   Min quantity: {y.min()}\")\n",
    "print(f\"   Max quantity: {y.max()}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_counts = X.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing values found:\")\n",
    "    for col, count in missing_counts[missing_counts > 0].items():\n",
    "        print(f\"   {col}: {count} missing values\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values in final feature matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization of key features\n",
    "print(\"\\nüìä Final Feature Analysis:\")\n",
    "\n",
    "# Create final summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Target distribution\n",
    "axes[0, 0].hist(y, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Target Variable Distribution (Quantity)')\n",
    "axes[0, 0].set_xlabel('Quantity Sold')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Feature correlations with target (top 15)\n",
    "feature_correlations = pd.Series([X[col].corr(y) for col in X.columns], index=X.columns)\n",
    "feature_correlations = feature_correlations.dropna().abs().sort_values(ascending=False)\n",
    "top_15_corr = feature_correlations.head(15)\n",
    "\n",
    "top_15_corr.plot(kind='barh', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Top 15 Features by Correlation with Target')\n",
    "axes[0, 1].set_xlabel('Absolute Correlation')\n",
    "\n",
    "# Feature variance distribution\n",
    "feature_variances = X.var().sort_values(ascending=False)\n",
    "axes[1, 0].hist(np.log1p(feature_variances), bins=30, alpha=0.7)\n",
    "axes[1, 0].set_title('Feature Variance Distribution (Log Scale)')\n",
    "axes[1, 0].set_xlabel('Log(1 + Variance)')\n",
    "axes[1, 0].set_ylabel('Number of Features')\n",
    "\n",
    "# Feature type distribution\n",
    "feature_types = {\n",
    "    'Temporal': temporal_features,\n",
    "    'Product': product_features,\n",
    "    'Customer': customer_features,\n",
    "    'Lag': lag_features,\n",
    "    'Interaction': interaction_features,\n",
    "    'Other': X.shape[1] - (temporal_features + product_features + customer_features + lag_features + interaction_features)\n",
    "}\n",
    "\n",
    "axes[1, 1].pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Feature Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top features by correlation\n",
    "print(f\"\\nüìà Top 10 Most Predictive Features:\")\n",
    "for i, (feature, corr) in enumerate(top_15_corr.head(10).items(), 1):\n",
    "    print(f\"   {i:2d}. {feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ COMPLETE FEATURE ENGINEERING FINISHED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìã Final Summary:\")\n",
    "print(f\"   ‚úÖ Features engineered: {X.shape[1]:,}\")\n",
    "print(f\"   ‚úÖ Records processed: {X.shape[0]:,}\")\n",
    "print(f\"   ‚úÖ Quality score: {validation_results['overall_score']:.0f}%\")\n",
    "print(f\"   ‚úÖ Ready for model training: {'Yes' if validation_results['overall_score'] >= 75 else 'Needs review'}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   üìÇ Proceed to: 03_train_model.ipynb\")\n",
    "print(f\"   üìÑ Module guide: 02-predictive-model.md\")\n",
    "print(f\"   üéØ Objective: Train Random Forest model for sales forecasting\")\n",
    "\n",
    "print(f\"\\nüí° Complete Feature Engineering Results:\")\n",
    "print(f\"   üïí Temporal: {feature_types['Temporal']} features - Seasonal patterns, cyclical encoding\")\n",
    "print(f\"   üè∑Ô∏è Product: {feature_types['Product']} features - Price analysis, brand performance\")\n",
    "print(f\"   üë• Customer: {feature_types['Customer']} features - RFM segmentation, behavioral metrics\")\n",
    "print(f\"   üìä Lag: {feature_types['Lag']} features - Historical trends, moving averages\")\n",
    "print(f\"   üîó Interaction: {feature_types['Interaction']} features - Cross-feature relationships\")\n",
    "\n",
    "print(f\"\\nüìà Model-Ready Dataset:\")\n",
    "print(f\"   üìä Feature matrix shape: {X.shape}\")\n",
    "print(f\"   üéØ Target variable: {feature_summary['target_column']}\")\n",
    "print(f\"   üíæ All files saved to: {processed_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "This notebook has successfully completed advanced feature engineering:\n",
    "\n",
    "‚úÖ **Lag Features & Time Series** - Historical patterns, moving averages, growth indicators  \n",
    "‚úÖ **Interaction Features** - Cross-variable relationships and multiplicative combinations  \n",
    "‚úÖ **Data Preprocessing** - Categorical encoding, missing value handling, feature scaling  \n",
    "‚úÖ **Quality Validation** - Comprehensive assessment with quality score analysis  \n",
    "‚úÖ **Model Preparation** - Complete engineered features ready for Random Forest training  \n",
    "\n",
    "**Combined with Part 1:** The complete feature engineering pipeline creates a rich, optimized dataset for highly accurate sales forecasting with sophisticated temporal, behavioral, and interaction patterns.\n",
    "\n",
    "**Dataset Ready for Training:**\n",
    "- **Feature Matrix:** Comprehensive set of engineered features\n",
    "- **Target Variable:** Sales quantity for prediction\n",
    "- **Quality Score:** Validated and optimized for machine learning\n",
    "- **File Organization:** All components saved and ready for model training\n",
    "\n",
    "---"
   ]
  }
]
}