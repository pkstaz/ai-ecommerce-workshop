{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Sales Forecasting - Part 2\n",
    "## Module 2: Predictive Model - Advanced Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Complete advanced feature engineering and preprocessing\n",
    "\n",
    "**This notebook covers:**\n",
    "- Lag features and moving averages\n",
    "- Interaction features and combinations\n",
    "- Data preprocessing and encoding\n",
    "- Feature validation and quality assessment\n",
    "\n",
    "**Prerequisites:** Complete `02_feature_engineering_part1.ipynb` first\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Step 1: Setup and Load Intermediate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ“… Part 2 feature engineering started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data from Part 1\n",
    "data_dir = Path(\"../../datasets\")\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "print(\"ğŸ“‚ Loading intermediate data from Part 1...\")\n",
    "\n",
    "# Load enriched data from Part 1\n",
    "enriched_data = pd.read_csv(processed_dir / \"enriched_data_part1.csv\")\n",
    "\n",
    "# Convert date column back to datetime\n",
    "enriched_data['date'] = pd.to_datetime(enriched_data['date'])\n",
    "\n",
    "print(f\"âœ… Data loaded: {enriched_data.shape}\")\n",
    "print(f\"ğŸ“Š Starting Part 2 with {len(enriched_data.columns)} features\")\n",
    "\n",
    "# Quick data check\n",
    "print(f\"\\nğŸ“‹ Data Overview:\")\n",
    "print(f\"   Records: {len(enriched_data):,}\")\n",
    "print(f\"   Date range: {enriched_data['date'].min()} to {enriched_data['date'].max()}\")\n",
    "print(f\"   Missing values: {enriched_data.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 2: Lag Features and Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df):\n",
    "    \"\"\"\n",
    "    Create lag features and moving averages for time series patterns\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Creating lag features and moving averages...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by date to ensure proper lag calculation\n",
    "    df = df.sort_values(['date', 'product_id']).reset_index(drop=True)\n",
    "    \n",
    "    # Create daily aggregations first\n",
    "    print(\"   ğŸ“… Creating daily aggregations...\")\n",
    "    \n",
    "    daily_sales = df.groupby(['date', 'product_id']).agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    daily_sales.columns = ['date', 'product_id', 'daily_quantity', 'daily_revenue', 'daily_transactions']\n",
    "    \n",
    "    # Create category-level daily aggregations\n",
    "    product_category_map = df[['product_id', 'category']].drop_duplicates()\n",
    "    daily_sales = daily_sales.merge(product_category_map, on='product_id', how='left')\n",
    "    \n",
    "    category_daily = df.groupby(['date', 'category']).agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    category_daily.columns = ['date', 'category', 'category_daily_quantity', 'category_daily_revenue']\n",
    "    \n",
    "    # Overall daily metrics\n",
    "    overall_daily = df.groupby('date').agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    overall_daily.columns = ['date', 'overall_daily_quantity', 'overall_daily_revenue', 'overall_daily_transactions']\n",
    "    \n",
    "    return df, daily_sales, category_daily, overall_daily\n",
    "\n",
    "# Create daily aggregations\n",
    "enriched_data, daily_sales, category_daily, overall_daily = create_lag_features(enriched_data)\n",
    "\n",
    "print(f\"âœ… Daily aggregations created:\")\n",
    "print(f\"   Product-level daily data: {daily_sales.shape}\")\n",
    "print(f\"   Category-level daily data: {category_daily.shape}\")\n",
    "print(f\"   Overall daily data: {overall_daily.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(daily_sales, category_daily, overall_daily):\n",
    "    \"\"\"\n",
    "    Add lag features for different time windows\n",
    "    \"\"\"\n",
    "    print(\"â° Creating lag features...\")\n",
    "    \n",
    "    lag_windows = [1, 3, 7, 14, 30]  # 1 day, 3 days, 1 week, 2 weeks, 1 month\n",
    "    \n",
    "    # Product-level lag features\n",
    "    for lag in lag_windows:\n",
    "        daily_sales[f'quantity_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_quantity'].shift(lag)\n",
    "        daily_sales[f'revenue_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_revenue'].shift(lag)\n",
    "        daily_sales[f'transactions_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_transactions'].shift(lag)\n",
    "    \n",
    "    # Category-level lag features\n",
    "    for lag in lag_windows:\n",
    "        category_daily[f'category_quantity_lag_{lag}d'] = category_daily.groupby('category')['category_daily_quantity'].shift(lag)\n",
    "        category_daily[f'category_revenue_lag_{lag}d'] = category_daily.groupby('category')['category_daily_revenue'].shift(lag)\n",
    "    \n",
    "    # Overall market lag features\n",
    "    for lag in lag_windows:\n",
    "        overall_daily[f'market_quantity_lag_{lag}d'] = overall_daily['overall_daily_quantity'].shift(lag)\n",
    "        overall_daily[f'market_revenue_lag_{lag}d'] = overall_daily['overall_daily_revenue'].shift(lag)\n",
    "        overall_daily[f'market_transactions_lag_{lag}d'] = overall_daily['overall_daily_transactions'].shift(lag)\n",
    "    \n",
    "    print(f\"âœ… Lag features created for {len(lag_windows)} time windows\")\n",
    "    \n",
    "    return daily_sales, category_daily, overall_daily\n",
    "\n",
    "# Add lag features\n",
    "daily_sales, category_daily, overall_daily = add_lag_features(daily_sales, category_daily, overall_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_moving_averages(daily_sales, category_daily, overall_daily):\n",
    "    \"\"\"\n",
    "    Add moving averages for different time windows\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ˆ Creating moving averages...\")\n",
    "    \n",
    "    ma_windows = [3, 7, 14, 30]  # 3 days, 1 week, 2 weeks, 1 month\n",
    "    \n",
    "    # Product-level moving averages\n",
    "    for window in ma_windows:\n",
    "        daily_sales[f'quantity_ma_{window}d'] = daily_sales.groupby('product_id')['daily_quantity'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        daily_sales[f'revenue_ma_{window}d'] = daily_sales.groupby('product_id')['daily_revenue'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Category-level moving averages\n",
    "    for window in ma_windows:\n",
    "        category_daily[f'category_quantity_ma_{window}d'] = category_daily.groupby('category')['category_daily_quantity'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        category_daily[f'category_revenue_ma_{window}d'] = category_daily.groupby('category')['category_daily_revenue'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Market-level moving averages\n",
    "    for window in ma_windows:\n",
    "        overall_daily[f'market_quantity_ma_{window}d'] = overall_daily['overall_daily_quantity'].rolling(\n",
    "            window=window, min_periods=1\n",
    "        ).mean()\n",
    "        overall_daily[f'market_revenue_ma_{window}d'] = overall_daily['overall_daily_revenue'].rolling(\n",
    "            window=window, min_periods=1\n",
    "        ).mean()\n",
    "    \n",
    "    print(f\"âœ… Moving averages created for {len(ma_windows)} time windows\")\n",
    "    \n",
    "    return daily_sales, category_daily, overall_daily\n",
    "\n",
    "# Add moving averages\n",
    "daily_sales, category_daily, overall_daily = add_moving_averages(daily_sales, category_daily, overall_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_features(daily_sales):\n",
    "    \"\"\"\n",
    "    Add trend and growth features\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Creating trend features...\")\n",
    "    \n",
    "    # Short-term vs long-term trends\n",
    "    daily_sales['quantity_trend_7_30'] = daily_sales['quantity_ma_7d'] / (daily_sales['quantity_ma_30d'] + 0.01)\n",
    "    daily_sales['revenue_trend_7_30'] = daily_sales['revenue_ma_7d'] / (daily_sales['revenue_ma_30d'] + 0.01)\n",
    "    \n",
    "    # Growth rates\n",
    "    daily_sales['quantity_growth_7d'] = (\n",
    "        (daily_sales['quantity_ma_7d'] - daily_sales['quantity_lag_7d']) / \n",
    "        (daily_sales['quantity_lag_7d'] + 0.01) * 100\n",
    "    ).clip(-100, 500)  # Cap extreme values\n",
    "    \n",
    "    daily_sales['revenue_growth_7d'] = (\n",
    "        (daily_sales['revenue_ma_7d'] - daily_sales['revenue_lag_7d']) / \n",
    "        (daily_sales['revenue_lag_7d'] + 0.01) * 100\n",
    "    ).clip(-100, 500)\n",
    "    \n",
    "    print(\"âœ… Trend and growth features created\")\n",
    "    \n",
    "    return daily_sales\n",
    "\n",
    "# Add trend features\n",
    "daily_sales = add_trend_features(daily_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lag_features(df, daily_sales, category_daily, overall_daily):\n",
    "    \"\"\"\n",
    "    Merge lag features back to main dataset\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”— Merging lag features to main dataset...\")\n",
    "    \n",
    "    df = df.merge(daily_sales, on=['date', 'product_id'], how='left')\n",
    "    df = df.merge(category_daily, on=['date', 'category'], how='left')\n",
    "    df = df.merge(overall_daily, on='date', how='left')\n",
    "    \n",
    "    # Fill missing lag values with 0 or forward fill for first few records\n",
    "    lag_columns = [col for col in df.columns if 'lag_' in col or '_ma_' in col or '_growth_' in col or '_trend_' in col]\n",
    "    \n",
    "    for col in lag_columns:\n",
    "        if 'growth' in col or 'trend' in col:\n",
    "            df[col] = df[col].fillna(0)  # Fill growth/trend with 0\n",
    "        else:\n",
    "            df[col] = df.groupby('product_id')[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"âœ… Lag features merged: {len(lag_columns)} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Merge all lag features\n",
    "enriched_data = merge_lag_features(enriched_data, daily_sales, category_daily, overall_daily)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset after lag features: {enriched_data.shape}\")\n",
    "print(f\"ğŸ“ˆ New lag features: {len([col for col in enriched_data.columns if 'lag_' in col or '_ma_' in col or '_growth_' in col or '_trend_' in col])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 3: Lag Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag feature patterns\n",
    "print(\"ğŸ“Š Lag Feature Analysis:\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Moving average comparison\n",
    "sample_product = enriched_data['product_id'].value_counts().index[0]\n",
    "product_data = enriched_data[enriched_data['product_id'] == sample_product].sort_values('date')\n",
    "\n",
    "if len(product_data) > 10:\n",
    "    axes[0, 0].plot(product_data['date'], product_data['daily_quantity'], label='Daily', alpha=0.5)\n",
    "    axes[0, 0].plot(product_data['date'], product_data['quantity_ma_7d'], label='7-day MA')\n",
    "    axes[0, 0].plot(product_data['date'], product_data['quantity_ma_30d'], label='30-day MA')\n",
    "    axes[0, 0].set_title(f'Sales Trends for {sample_product}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Growth distribution\n",
    "growth_data = enriched_data['quantity_growth_7d'].dropna()\n",
    "axes[0, 1].hist(growth_data, bins=30, alpha=0.7)\n",
    "axes[0, 1].set_title('7-Day Quantity Growth Distribution')\n",
    "axes[0, 1].set_xlabel('Growth Rate (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Lag correlation analysis\n",
    "lag_corr_data = enriched_data[['quantity', 'quantity_lag_1d', 'quantity_lag_7d', 'quantity_lag_30d']].corr()\n",
    "sns.heatmap(lag_corr_data, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Lag Feature Correlations')\n",
    "\n",
    "# Trend vs actual sales\n",
    "trend_data = enriched_data[enriched_data['quantity_trend_7_30'].notna()]\n",
    "if len(trend_data) > 100:\n",
    "    sample_trend = trend_data.sample(n=min(1000, len(trend_data)))\n",
    "    axes[1, 1].scatter(sample_trend['quantity_trend_7_30'], sample_trend['quantity'], alpha=0.5)\n",
    "    axes[1, 1].set_title('Trend Indicator vs Actual Sales')\n",
    "    axes[1, 1].set_xlabel('7-day/30-day Trend Ratio')\n",
    "    axes[1, 1].set_ylabel('Quantity Sold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights about lag features\n",
    "print(f\"\\nğŸ“ˆ Key Lag Feature Insights:\")\n",
    "lag_corr = enriched_data[['quantity', 'quantity_lag_1d']].corr().iloc[0, 1]\n",
    "avg_growth = enriched_data['quantity_growth_7d'].mean()\n",
    "growth_volatility = enriched_data['quantity_growth_7d'].std()\n",
    "\n",
    "print(f\"   1-day lag correlation: {lag_corr:.3f}\")\n",
    "print(f\"   Average 7-day growth: {avg_growth:.2f}%\")\n",
    "print(f\"   Growth volatility: {growth_volatility:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Step 4: Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features between different variable groups\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”— Creating interaction features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price and temporal interactions\n",
    "    print(\"   ğŸ’°ğŸ“… Creating price-temporal interactions...\")\n",
    "    \n",
    "    # Price interactions with time features\n",
    "    df['price_x_weekend'] = df['unit_price'] * df['is_weekend']\n",
    "    df['price_x_holiday'] = df['unit_price'] * df['is_holiday_season']\n",
    "    df['discount_x_weekend'] = df['discount_percentage'] * df['is_weekend']\n",
    "    df['discount_x_holiday'] = df['discount_percentage'] * df['is_holiday_season']\n",
    "    \n",
    "    # Price and season interactions\n",
    "    df['price_x_quarter'] = df['unit_price'] * df['quarter']\n",
    "    df['price_x_month_sin'] = df['unit_price'] * df['month_sin']\n",
    "    df['price_x_month_cos'] = df['unit_price'] * df['month_cos']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply first set of interactions\n",
    "enriched_data = create_interaction_features(enriched_data)\n",
    "print(f\"âœ… Price-temporal interactions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_product_interactions(df):\n",
    "    \"\"\"\n",
    "    Create customer and product interaction features\n",
    "    \"\"\"\n",
    "    print(\"ğŸ‘¥ğŸ·ï¸ Creating customer-product interactions...\")\n",
    "    \n",
    "    # Customer segment and price interactions\n",
    "    df['segment_x_price'] = df['customer_segment_encoded'] * df['unit_price']\n",
    "    df['segment_x_discount'] = df['customer_segment_encoded'] * df['discount_percentage']\n",
    "    \n",
    "    # Customer lifecycle and product interactions\n",
    "    lifecycle_encoding = {\n",
    "        'New': 1, 'Active': 2, 'Regular': 3, 'Loyal': 4, 'At_Risk': 2, 'Dormant': 1\n",
    "    }\n",
    "    df['lifecycle_encoded'] = df['customer_lifecycle_stage'].map(lifecycle_encoding)\n",
    "    df['lifecycle_x_price'] = df['lifecycle_encoded'] * df['unit_price']\n",
    "    df['lifecycle_x_rating'] = df['lifecycle_encoded'] * df['rating']\n",
    "    \n",
    "    # Customer value and product quality interactions\n",
    "    df['clv_x_rating'] = df['customer_total_spent'] * df['rating']\n",
    "    df['clv_x_brand_performance'] = df['customer_total_spent'] * df['brand_avg_rating']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply customer-product interactions\n",
    "enriched_data = create_customer_product_interactions(enriched_data)\n",
    "print(f\"âœ… Customer-product interactions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_interactions(df):\n",
    "    \"\"\"\n",
    "    Create advanced interaction features\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§® Creating advanced interaction features...\")\n",
    "    \n",
    "    # Regional and product interactions\n",
    "    df['region_performance_x_price'] = df['customer_vs_region_performance'] * df['unit_price']\n",
    "    df['region_avg_x_price_ratio'] = df['region_avg_order_value'] * df['price_vs_category_avg']\n",
    "    \n",
    "    # Channel and behavior interactions\n",
    "    df['channel_consistency_x_engagement'] = df['customer_channel_consistency'] * df['digital_engagement_score']\n",
    "    \n",
    "    # Device and price interactions\n",
    "    device_encoding = {'Desktop': 3, 'Mobile': 2, 'Tablet': 2, 'Unknown': 1}\n",
    "    df['device_encoded'] = df['preferred_device'].map(device_encoding).fillna(1)\n",
    "    df['device_x_price'] = df['device_encoded'] * df['unit_price']\n",
    "    df['device_x_engagement'] = df['device_encoded'] * df['digital_engagement_score']\n",
    "    \n",
    "    # Temporal and lag feature interactions\n",
    "    df['holiday_x_trend'] = df['is_holiday_season'] * df['quantity_trend_7_30']\n",
    "    df['weekend_x_growth'] = df['is_weekend'] * df['quantity_growth_7d']\n",
    "    df['quarter_x_lag_performance'] = df['quarter'] * df['quantity_lag_7d']\n",
    "    \n",
    "    # Moving average and seasonal interactions\n",
    "    df['ma7_x_holiday'] = df['quantity_ma_7d'] * df['is_holiday_season']\n",
    "    df['ma30_x_quarter'] = df['quantity_ma_30d'] * df['quarter']\n",
    "    \n",
    "    # Product performance and time interactions\n",
    "    df['popularity_x_holiday'] = df['product_popularity_score'] * df['is_holiday_season']\n",
    "    df['popularity_x_weekend'] = df['product_popularity_score'] * df['is_weekend']\n",
    "    \n",
    "    # Brand performance and time\n",
    "    df['brand_rating_x_holiday'] = df['brand_avg_rating'] * df['is_holiday_season']\n",
    "    df['market_share_x_quarter'] = df['brand_market_share'] * df['quarter']\n",
    "    \n",
    "    # Category and customer segment interactions\n",
    "    category_encoding = {\n",
    "        'Electronics': 5, 'Clothing': 4, 'Sports': 3, \n",
    "        'Home & Garden': 3, 'Beauty': 2, 'Books': 1\n",
    "    }\n",
    "    df['category_encoded'] = df['category'].map(category_encoding)\n",
    "    df['category_x_segment'] = df['category_encoded'] * df['customer_segment_encoded']\n",
    "    df['category_x_lifecycle'] = df['category_encoded'] * df['lifecycle_encoded']\n",
    "    \n",
    "    # Three-way interactions (most important combinations)\n",
    "    df['price_x_rating_x_segment'] = df['unit_price'] * df['rating'] * df['customer_segment_encoded']\n",
    "    df['discount_x_holiday_x_category'] = df['discount_percentage'] * df['is_holiday_season'] * df['category_encoded']\n",
    "    df['trend_x_popularity_x_quarter'] = df['quantity_trend_7_30'] * df['product_popularity_score'] * df['quarter']\n",
    "    \n",
    "    # Ratio-based interactions\n",
    "    df['customer_diversity_x_category_performance'] = df['customer_diversity_score'] * df['category_avg_amount']\n",
    "    df['rfm_x_product_performance'] = (df['rfm_score'] / 100) * df['product_performance_vs_category']\n",
    "    \n",
    "    # Complex behavioral interactions\n",
    "    df['engagement_x_loyalty_x_price'] = (\n",
    "        df['digital_engagement_score'] * \n",
    "        df['customer_channel_consistency'] * \n",
    "        df['price_vs_category_avg']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply advanced interactions\n",
    "enriched_data = create_advanced_interactions(enriched_data)\n",
    "\n",
    "interaction_features = [col for col in enriched_data.columns if '_x_' in col]\n",
    "print(f\"âœ… All interaction features created: {len(interaction_features)} features\")\n",
    "print(f\"ğŸ“Š Dataset after interactions: {enriched_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: Interaction Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interaction feature importance\n",
    "print(\"ğŸ“Š Interaction Feature Analysis:\")\n",
    "\n",
    "# Calculate correlation of interaction features with target\n",
    "interaction_features = [col for col in enriched_data.columns if '_x_' in col]\n",
    "target_correlations = {}\n",
    "\n",
    "for feature in interaction_features[:10]:  # Analyze top 10 interaction features\n",
    "    correlation = enriched_data[['quantity', feature]].corr().iloc[0, 1]\n",
    "    if not np.isnan(correlation):\n",
    "        target_correlations[feature] = abs(correlation)\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_correlations = sorted(target_correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Top 10 Interaction Features by Correlation with Sales Quantity:\")\n",
    "for i, (feature, corr) in enumerate(sorted_correlations[:10], 1):\n",
    "    print(f\"   {i:2d}. {feature}: {corr:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Interaction Insights:\")\n",
    "if len(sorted_correlations) > 0:\n",
    "    print(f\"   Strongest interaction: {sorted_correlations[0][0]} (correlation: {sorted_correlations[0][1]:.4f})\")\n",
    "print(f\"   Total interaction features: {len(interaction_features)}\")\n",
    "print(f\"   Features with multiplicative interactions: {len([col for col in enriched_data.columns if '_x_' in col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some key interactions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price x Holiday interaction\n",
    "try:\n",
    "    holiday_price_data = enriched_data.groupby(['is_holiday_season', pd.cut(enriched_data['unit_price'], bins=5)])['quantity'].mean().unstack()\n",
    "    if holiday_price_data.shape[0] > 1 and holiday_price_data.shape[1] > 1:\n",
    "        sns.heatmap(holiday_price_data, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Average Quantity: Price vs Holiday Season')\n",
    "        axes[0, 0].set_xlabel('Price Bins')\n",
    "        axes[0, 0].set_ylabel('Holiday Season (0=No, 1=Yes)')\nexcept:\n",
    "    axes[0, 0].text(0.5, 0.5, 'Price x Holiday\\nInteraction Data', ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "\n",
    "# Customer Segment x Category interaction\n",
    "try:\n",
    "    segment_category_data = enriched_data.groupby(['customer_segment', 'category'])['total_amount'].mean().unstack()\n",
    "    if segment_category_data.shape[0] > 1 and segment_category_data.shape[1] > 1:\n",
    "        sns.heatmap(segment_category_data, annot=True, fmt='.0f', cmap='viridis', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Order Value: Segment vs Category')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].tick_params(axis='y', rotation=0)\nexcept:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Segment x Category\\nInteraction Data', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "\n",
    "# Weekend x Discount interaction\n",
    "try:\n",
    "    weekend_discount_effect = enriched_data.groupby(['is_weekend', 'is_discounted'])['quantity'].mean().unstack()\n",
    "    if weekend_discount_effect.shape[0] > 1 and weekend_discount_effect.shape[1] > 1:\n",
    "        weekend_discount_effect.plot(kind='bar', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Quantity Sold: Weekend vs Discount Status')\n",
    "        axes[1, 0].set_xlabel('Weekend (0=Weekday, 1=Weekend)')\n",
    "        axes[1, 0].set_ylabel('Average Quantity')\n",
    "        axes[1, 0].legend(['No Discount', 'Discounted'])\n",
    "        axes[1, 0].tick_params(axis='x', rotation=0)\nexcept:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Weekend x Discount\\nInteraction Data', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# Rating x Price quartile interaction\n",
    "try:\n",
    "    rating_price_data = enriched_data.groupby(['price_quartile', pd.cut(enriched_data['rating'], bins=4)])['quantity'].mean().unstack()\n",
    "    if rating_price_data.shape[0] > 1 and rating_price_data.shape[1] > 1:\n",
    "        sns.heatmap(rating_price_data, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Average Quantity: Price Quartile vs Rating')\n",
    "        axes[1, 1].set_xlabel('Rating Bins')\n",
    "        axes[1, 1].set_ylabel('Price Quartile')\nexcept:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Rating x Price\\nInteraction Data', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Step 6: Data Preprocessing and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for machine learning by handling categorical variables and scaling\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ Preprocessing features for machine learning...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify feature types\n",
    "    print(\"   ğŸ” Identifying feature types...\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_columns = [\n",
    "        'transaction_id', 'date', 'customer_id', 'product_id', \n",
    "        'customer_first_purchase', 'customer_last_purchase'\n",
    "    ]\n",
    "    \n",
    "    feature_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    # Separate target variable\n",
    "    target_column = 'quantity'  # Our prediction target\n",
    "    feature_columns = [col for col in feature_columns if col != target_column]\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_columns = []\n",
    "    numerical_columns = []\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            categorical_columns.append(col)\n",
    "        else:\n",
    "            numerical_columns.append(col)\n",
    "    \n",
    "    print(f\"   ğŸ“Š Found {len(numerical_columns)} numerical and {len(categorical_columns)} categorical features\")\n",
    "    \n",
    "    return df, categorical_columns, numerical_columns, target_column\n",
    "\n",
    "# Identify feature types\n",
    "enriched_data, categorical_columns, numerical_columns, target_column = preprocess_features(enriched_data)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Feature Type Summary:\")\n",
    "print(f\"   Categorical features: {categorical_columns}\")\n",
    "print(f\"   Numerical features: {len(numerical_columns)} features\")\n",
    "print(f\"   Target variable: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ Handling missing values...\")\n",
    "    \n",
    "    # Fill missing values for numerical columns\n",
    "    for col in numerical_columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if 'price' in col.lower() or 'amount' in col.lower():\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            elif 'count' in col.lower() or 'quantity' in col.lower():\n",
    "                df[col] = df[col].fillna(0)\n",
    "            elif 'ratio' in col.lower() or 'percentage' in col.lower():\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    # Fill missing values for categorical columns\n",
    "    for col in categorical_columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    print(f\"âœ… Missing values handled\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "enriched_data = handle_missing_values(enriched_data, categorical_columns, numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_variables(df, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Encode categorical variables\n",
    "    \"\"\"\n",
    "    print(\"ğŸ·ï¸ Encoding categorical variables...\")\n",
    "    \n",
    "    # One-hot encode low cardinality categorical variables\n",
    "    low_cardinality_threshold = 10\n",
    "    high_cardinality_categorical = []\n",
    "    low_cardinality_categorical = []\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count <= low_cardinality_threshold:\n",
    "            low_cardinality_categorical.append(col)\n",
    "        else:\n",
    "            high_cardinality_categorical.append(col)\n",
    "    \n",
    "    print(f\"     Low cardinality ({len(low_cardinality_categorical)}): {low_cardinality_categorical}\")\n",
    "    print(f\"     High cardinality ({len(high_cardinality_categorical)}): {high_cardinality_categorical}\")\n",
    "    \n",
    "    # Label encode high cardinality categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in high_cardinality_categorical:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        numerical_columns.append(f'{col}_encoded')\n",
    "    \n",
    "    # One-hot encode low cardinality categorical variables\n",
    "    df_encoded = pd.get_dummies(\n",
    "        df[low_cardinality_categorical], \n",
    "        prefix=low_cardinality_categorical,\n",
    "        drop_first=True,\n",
    "        dummy_na=False\n",
    "    )\n",
    "    \n",
    "    return df, df_encoded, numerical_columns, label_encoders\n",
    "\n",
    "# Encode categorical variables\n",
    "enriched_data, df_encoded, numerical_columns, label_encoders = encode_categorical_variables(\n",
    "    enriched_data, categorical_columns, numerical_columns\n",
    ")\n",
    "\n",
    "print(f\"âœ… Categorical encoding completed:\")\n",
    "print(f\"   Label encoded features: {len(label_encoders)}\")\n",
    "print(f\"   One-hot encoded features: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_feature_matrix(df, df_encoded, numerical_columns, target_column):\n",
    "    \"\"\"\n",
    "    Create final feature matrix and target variable\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Creating final feature matrix...\")\n",
    "    \n",
    "    # Combine all features\n",
    "    final_features = numerical_columns + list(df_encoded.columns)\n",
    "    \n",
    "    # Create final feature matrix\n",
    "    X = pd.concat([\n",
    "        df[numerical_columns],\n",
    "        df_encoded\n",
    "    ], axis=1)\n",
    "    \n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Feature summary\n",
    "    feature_summary = {\n",
    "        'feature_names': final_features,\n",
    "        'numerical_features': numerical_columns,\n",
    "        'categorical_features': list(df_encoded.columns),\n",
    "        'label_encoders': label_encoders,\n",
    "        'target_column': target_column,\n",
    "        'total_features': len(final_features)\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Feature matrix created:\")\n",
    "    print(f\"   Total features: {len(final_features)}\")\n",
    "    print(f\"   Numerical features: {len(numerical_columns)}\")\n",
    "    print(f\"   One-hot encoded features: {len(df_encoded.columns)}\")\n",
    "    print(f\"   Target variable: {target_column}\")\n",
    "    print(f\"   Dataset shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, feature_summary\n",
    "\n",
    "# Create final feature matrix\n",
    "X, y, feature_summary = create_final_feature_matrix(\n",
    "    enriched_data, df_encoded, numerical_columns, target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 7: Feature Validation and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_feature_engineering(X, y):\n",
    "    \"\"\"\n",
    "    Validate the quality of engineered features\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ Validating feature engineering quality...\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Feature completeness\n",
    "    print(\"   âœ… Checking feature completeness...\")\n",
    "    missing_percentage = (X.isnull().sum().sum() / (X.shape[0] * X.shape[1])) * 100\n",
    "    validation_results['missing_percentage'] = missing_percentage\n",
    "    print(f\"     Missing values: {missing_percentage:.3f}%\")\n",
    "    \n",
    "    # 2. Feature variance\n",
    "    print(\"   ğŸ“Š Checking feature variance...\")\n",
    "    low_variance_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].var() < 0.01:  # Very low variance threshold\n",
    "            low_variance_features.append(col)\n",
    "    \n",
    "    validation_results['low_variance_features'] = len(low_variance_features)\n",
    "    print(f\"     Low variance features: {len(low_variance_features)}\")\n",
    "    \n",
    "    # 3. Target variable distribution\n",
    "    print(\"   ğŸ¯ Analyzing target distribution...\")\n",
    "    target_skewness = y.skew()\n",
    "    target_kurtosis = y.kurtosis()\n",
    "    validation_results['target_skewness'] = target_skewness\n",
    "    validation_results['target_kurtosis'] = target_kurtosis\n",
    "    print(f\"     Target skewness: {target_skewness:.3f}\")\n",
    "    print(f\"     Target kurtosis: {target_kurtosis:.3f}\")\n",
    "    \n",
    "    # 4. Feature-target correlations\n",
    "    print(\"   ğŸ”— Analyzing feature-target relationships...\")\n",
    "    correlations = [abs(X[col].corr(y)) for col in X.columns]\n",
    "    correlations = [c for c in correlations if not np.isnan(c)]\n",
    "    \n",
    "    strong_correlations = sum(1 for c in correlations if c > 0.1)\n",
    "    validation_results['strong_correlations'] = strong_correlations\n",
    "    validation_results['avg_correlation'] = np.mean(correlations)\n",
    "    print(f\"     Features with |correlation| > 0.1: {strong_correlations}\")\n",
    "    print(f\"     Average absolute correlation: {np.mean(correlations):.4f}\")\n",
    "    \n",
    "    # 5. Feature engineering success metrics\n",
    "    print(\"   ğŸ“ˆ Feature engineering success metrics...\")\n",
    "    \n",
    "    success_metrics = {\n",
    "        'total_features_created': X.shape[1],\n",
    "        'feature_density': X.shape[1] / X.shape[0],\n",
    "        'missing_data_handled': missing_percentage < 1.0,\n",
    "        'sufficient_variance': len(low_variance_features) < X.shape[1] * 0.1,\n",
    "        'good_target_correlation': strong_correlations > 10,\n",
    "        'reasonable_dimensionality': X.shape[1] < X.shape[0] * 0.1\n",
    "    }\n",
    "    \n",
    "    validation_results.update(success_metrics)\n",
    "    \n",
    "    # Overall score\n",
    "    passed_checks = sum([\n",
    "        success_metrics['missing_data_handled'],\n",
    "        success_metrics['sufficient_variance'],\n",
    "        success_metrics['good_target_correlation'],\n",
    "        success_metrics['reasonable_dimensionality']\n",
    "    ])\n",
    "    \n",
    "    overall_score = (passed_checks / 4) * 100\n",
    "    validation_results['overall_score'] = overall_score\n",
    "    \n",
    "    print(f\"\\nğŸ† Feature Engineering Quality Score: {overall_score:.0f}%\")\n",
    "    \n",
    "    if overall_score >= 75:\n",
    "        print(\"âœ… Feature engineering quality: EXCELLENT - Ready for model training!\")\n",
    "    elif overall_score >= 50:\n",
    "        print(\"âš ï¸  Feature engineering quality: GOOD - Some improvements possible\")\n",
    "    else:\n",
    "        print(\"âŒ Feature engineering quality: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Perform validation\n",
    "validation_results = validate_feature_engineering(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 8: Save Final Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving processed data\n",
    "print(f\"ğŸ’¾ Saving final processed features...\")\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save feature matrix and target\n",
    "X.to_csv(processed_dir / \"feature_matrix.csv\", index=False)\n",
    "y.to_csv(processed_dir / \"target_variable.csv\", index=False)\n",
    "\n",
    "# Save feature summary\n",
    "with open(processed_dir / \"feature_summary.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_summary, f)\n",
    "\n",
    "# Save validation results\n",
    "with open(processed_dir / \"validation_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(validation_results, f)\n",
    "\n",
    "# Save a sample of the enriched dataset for inspection\n",
    "enriched_data.head(1000).to_csv(processed_dir / \"enriched_sample.csv\", index=False)\n",
    "\n",
    "# Create feature importance data for later analysis\n",
    "feature_stats = pd.DataFrame({\n",
    "    'feature_name': X.columns,\n",
    "    'mean': X.mean(),\n",
    "    'std': X.std(),\n",
    "    'min': X.min(),\n",
    "    'max': X.max(),\n",
    "    'correlation_with_target': [X[col].corr(y) for col in X.columns]\n",
    "}).round(4)\n",
    "\n",
    "feature_stats.to_csv(processed_dir / \"feature_statistics.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Final processed data saved:\")\n",
    "print(f\"   ğŸ“ Feature matrix: {processed_dir / 'feature_matrix.csv'}\")\n",
    "print(f\"   ğŸ“ Target variable: {processed_dir / 'target_variable.csv'}\")\n",
    "print(f\"   ğŸ“ Feature summary: {processed_dir / 'feature_summary.pkl'}\")\n",
    "print(f\"   ğŸ“ Validation results: {processed_dir / 'validation_results.pkl'}\")\n",
    "print(f\"   ğŸ“ Feature statistics: {processed_dir / 'feature_statistics.csv'}\")\n",
    "print(f\"   ğŸ“ Enriched sample: {processed_dir / 'enriched_sample.csv'}\")"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 9: Final Feature Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature engineering summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Overview:\")\n",
    "print(f\"   Original records: {len(enriched_data):,}\")\n",
    "print(f\"   Final feature matrix: {X.shape}\")\n",
    "print(f\"   Target variable: {feature_summary['target_column']}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Feature Categories:\")\n",
    "temporal_features = len([f for f in X.columns if any(x in f.lower() for x in ['day', 'week', 'month', 'year', 'quarter', 'holiday', 'weekend', 'sin', 'cos'])])\n",
    "product_features = len([f for f in X.columns if any(x in f.lower() for x in ['price', 'brand', 'category', 'rating', 'review', 'product'])])\n",
    "customer_features = len([f for f in X.columns if any(x in f.lower() for x in ['customer', 'segment', 'lifecycle', 'rfm', 'clv'])])\n",
    "lag_features = len([f for f in X.columns if 'lag_' in f or '_ma_' in f])\n",
    "interaction_features = len([f for f in X.columns if '_x_' in f])\n",
    "\n",
    "print(f\"   Temporal features: {temporal_features}\")\n",
    "print(f\"   Product features: {product_features}\")\n",
    "print(f\"   Customer features: {customer_features}\")\n",
    "print(f\"   Lag features: {lag_features}\")\n",
    "print(f\"   Interaction features: {interaction_features}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Target Variable Statistics:\")\n",
    "print(f\"   Mean quantity: {y.mean():.2f}\")\n",
    "print(f\"   Median quantity: {y.median():.2f}\")\n",
    "print(f\"   Standard deviation: {y.std():.2f}\")\n",
    "print(f\"   Min quantity: {y.min()}\")\n",
    "print(f\"   Max quantity: {y.max()}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_counts = X.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"\\nâš ï¸  Missing values found:\")\n",
    "    for col, count in missing_counts[missing_counts > 0].items():\n",
    "        print(f\"   {col}: {count} missing values\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No missing values in final feature matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization of key features\n",
    "print(\"\\nğŸ“Š Final Feature Analysis:\")\n",
    "\n",
    "# Create final summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Target distribution\n",
    "axes[0, 0].hist(y, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Target Variable Distribution (Quantity)')\n",
    "axes[0, 0].set_xlabel('Quantity Sold')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Feature correlations with target (top 15)\n",
    "feature_correlations = pd.Series([X[col].corr(y) for col in X.columns], index=X.columns)\n",
    "feature_correlations = feature_correlations.dropna().abs().sort_values(ascending=False)\n",
    "top_15_corr = feature_correlations.head(15)\n",
    "\n",
    "top_15_corr.plot(kind='barh', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Top 15 Features by Correlation with Target')\n",
    "axes[0, 1].set_xlabel('Absolute Correlation')\n",
    "\n",
    "# Feature variance distribution\n",
    "feature_variances = X.var().sort_values(ascending=False)\n",
    "axes[1, 0].hist(np.log1p(feature_variances), bins=30, alpha=0.7)\n",
    "axes[1, 0].set_title('Feature Variance Distribution (Log Scale)')\n",
    "axes[1, 0].set_xlabel('Log(1 + Variance)')\n",
    "axes[1, 0].set_ylabel('Number of Features')\n",
    "\n",
    "# Feature type distribution\n",
    "feature_types = {\n",
    "    'Temporal': temporal_features,\n",
    "    'Product': product_features,\n",
    "    'Customer': customer_features,\n",
    "    'Lag': lag_features,\n",
    "    'Interaction': interaction_features,\n",
    "    'Other': X.shape[1] - (temporal_features + product_features + customer_features + lag_features + interaction_features)\n",
    "}\n",
    "\n",
    "axes[1, 1].pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Feature Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top features by correlation\n",
    "print(f\"\\nğŸ“ˆ Top 10 Most Predictive Features:\")\n",
    "for i, (feature, corr) in enumerate(top_15_corr.head(10).items(), 1):\n",
    "    print(f\"   {i:2d}. {feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ COMPLETE FEATURE ENGINEERING FINISHED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Final Summary:\")\n",
    "print(f\"   âœ… Features engineered: {X.shape[1]:,}\")\n",
    "print(f\"   âœ… Records processed: {X.shape[0]:,}\")\n",
    "print(f\"   âœ… Quality score: {validation_results['overall_score']:.0f}%\")\n",
    "print(f\"   âœ… Ready for model training: {'Yes' if validation_results['overall_score'] >= 75 else 'Needs review'}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Next Steps:\")\n",
    "print(f\"   ğŸ“‚ Proceed to: 03_train_model.ipynb\")\n",
    "print(f\"   ğŸ“„ Module guide: 02-predictive-model.md\")\n",
    "print(f\"   ğŸ¯ Objective: Train Random Forest model for sales forecasting\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Complete Feature Engineering Results:\")\n",
    "print(f\"   ğŸ•’ Temporal: {feature_types['Temporal']} features - Seasonal patterns, cyclical encoding\")\n",
    "print(f\"   ğŸ·ï¸ Product: {feature_types['Product']} features - Price analysis, brand performance\")\n",
    "print(f\"   ğŸ‘¥ Customer: {feature_types['Customer']} features - RFM segmentation, behavioral metrics\")\n",
    "print(f\"   ğŸ“Š Lag: {feature_types['Lag']} features - Historical trends, moving averages\")\n",
    "print(f\"   ğŸ”— Interaction: {feature_types['Interaction']} features - Cross-feature relationships\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Model-Ready Dataset:\")\n",
    "print(f\"   ğŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"   ğŸ¯ Target variable: {feature_summary['target_column']}\")\n",
    "print(f\"   ğŸ’¾ All files saved to: {processed_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "This notebook has successfully completed advanced feature engineering:\n",
    "\n",
    "âœ… **Lag Features & Time Series** - Historical patterns, moving averages, growth indicators  \n",
    "âœ… **Interaction Features** - Cross-variable relationships and multiplicative combinations  \n",
    "âœ… **Data Preprocessing** - Categorical encoding, missing value handling, feature scaling  \n",
    "âœ… **Quality Validation** - Comprehensive assessment with quality score analysis  \n",
    "âœ… **Model Preparation** - Complete engineered features ready for Random Forest training  \n",
    "\n",
    "**Combined with Part 1:** The complete feature engineering pipeline creates a rich, optimized dataset for highly accurate sales forecasting with sophisticated temporal, behavioral, and interaction patterns.\n",
    "\n",
    "**Dataset Ready for Training:**\n",
    "- **Feature Matrix:** Comprehensive set of engineered features\n",
    "- **Target Variable:** Sales quantity for prediction\n",
    "- **Quality Score:** Validated and optimized for machine learning\n",
    "- **File Organization:** All components saved and ready for model training\n",
    "\n",
    "---"
   ]
  }
]
}