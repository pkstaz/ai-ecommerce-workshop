{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Part 2 - Simplified\n",
    "## Advanced Features and Preprocessing\n",
    "\n",
    "**Prerequisites:** Complete Part 1 first\n",
    "\n",
    "**This notebook covers:**\n",
    "- Lag features and moving averages\n",
    "- Interaction features\n",
    "- Data preprocessing\n",
    "- Final validation and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n",
    "print(f\"ðŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Part 1\n",
    "data_dir = Path(\"../../datasets\")\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "enriched_data = pd.read_csv(processed_dir / \"enriched_data_part1.csv\")\n",
    "enriched_data['date'] = pd.to_datetime(enriched_data['date'])\n",
    "\n",
    "print(f\"âœ… Data loaded: {enriched_data.shape}\")\n",
    "print(f\"ðŸ“Š Starting with {len(enriched_data.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Lag Features and Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features_simple(df):\n",
    "    \"\"\"Create essential lag features and moving averages\"\"\"\n",
    "    print(\"ðŸ“Š Creating lag features...\")\n",
    "    \n",
    "    df = df.sort_values(['date', 'product_id']).copy()\n",
    "    \n",
    "    # Daily aggregations\n",
    "    daily_sales = df.groupby(['date', 'product_id']).agg({\n",
    "        'quantity': 'sum',\n",
    "        'total_amount': 'sum'\n",
    "    }).reset_index()\n",
    "    daily_sales.columns = ['date', 'product_id', 'daily_quantity', 'daily_revenue']\n",
    "    \n",
    "    # Add product category mapping\n",
    "    product_cat = df[['product_id', 'category']].drop_duplicates()\n",
    "    daily_sales = daily_sales.merge(product_cat, on='product_id')\n",
    "    \n",
    "    # Key lag features (simplified)\n",
    "    for lag in [1, 7, 30]:\n",
    "        daily_sales[f'qty_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_quantity'].shift(lag)\n",
    "        daily_sales[f'rev_lag_{lag}d'] = daily_sales.groupby('product_id')['daily_revenue'].shift(lag)\n",
    "    \n",
    "    # Moving averages (simplified)\n",
    "    for window in [7, 30]:\n",
    "        daily_sales[f'qty_ma_{window}d'] = daily_sales.groupby('product_id')['daily_quantity'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        daily_sales[f'rev_ma_{window}d'] = daily_sales.groupby('product_id')['daily_revenue'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Growth rate\n",
    "    daily_sales['qty_growth_7d'] = (\n",
    "        (daily_sales['qty_ma_7d'] - daily_sales['qty_lag_7d']) / \n",
    "        (daily_sales['qty_lag_7d'] + 0.01) * 100\n",
    "    ).clip(-100, 500)\n",
    "    \n",
    "    # Merge back\n",
    "    df = df.merge(daily_sales, on=['date', 'product_id'], how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    lag_cols = [col for col in df.columns if 'lag_' in col or '_ma_' in col or 'growth' in col]\n",
    "    for col in lag_cols:\n",
    "        if 'growth' in col:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            df[col] = df.groupby('product_id')[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"âœ… Created {len(lag_cols)} lag features\")\n",
    "    return df\n",
    "\n",
    "# Apply lag features\n",
    "enriched_data = create_lag_features_simple(enriched_data)\n",
    "print(f\"ðŸ“Š Dataset shape after lag features: {enriched_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features_simple(df):\n",
    "    \"\"\"Create key interaction features\"\"\"\n",
    "    print(\"ðŸ”— Creating interaction features...\")\n",
    "    \n",
    "    # Price and time interactions\n",
    "    df['price_x_weekend'] = df['unit_price'] * df['is_weekend']\n",
    "    df['price_x_holiday'] = df['unit_price'] * df['is_holiday_season']\n",
    "    df['discount_x_holiday'] = df['discount_percentage'] * df['is_holiday_season']\n",
    "    \n",
    "    # Customer and product interactions\n",
    "    df['segment_x_price'] = df['customer_segment_encoded'] * df['unit_price']\n",
    "    df['lifecycle_x_rating'] = df.get('lifecycle_encoded', 1) * df['rating']\n",
    "    \n",
    "    # Lag and seasonal interactions\n",
    "    df['holiday_x_growth'] = df['is_holiday_season'] * df['qty_growth_7d']\n",
    "    df['ma7_x_holiday'] = df['qty_ma_7d'] * df['is_holiday_season']\n",
    "    \n",
    "    # Advanced interactions\n",
    "    df['price_x_rating_x_segment'] = (\n",
    "        df['unit_price'] * df['rating'] * df['customer_segment_encoded']\n",
    "    )\n",
    "    \n",
    "    interaction_count = len([col for col in df.columns if '_x_' in col])\n",
    "    print(f\"âœ… Created {interaction_count} interaction features\")\n",
    "    return df\n",
    "\n",
    "# Apply interaction features\n",
    "enriched_data = create_interaction_features_simple(enriched_data)\n",
    "print(f\"ðŸ“Š Dataset shape after interactions: {enriched_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_ml(df):\n",
    "    \"\"\"Prepare data for machine learning\"\"\"\n",
    "    print(\"ðŸ§¹ Preprocessing for ML...\")\n",
    "    \n",
    "    # Remove non-feature columns\n",
    "    exclude_cols = ['transaction_id', 'date', 'customer_id', 'product_id', \n",
    "                   'customer_first_purchase', 'customer_last_purchase']\n",
    "    \n",
    "    # Separate target\n",
    "    target = 'quantity'\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols + [target]]\n",
    "    \n",
    "    # Identify categorical vs numerical\n",
    "    categorical_cols = [col for col in feature_cols if df[col].dtype == 'object']\n",
    "    numerical_cols = [col for col in feature_cols if col not in categorical_cols]\n",
    "    \n",
    "    print(f\"   ðŸ“Š {len(numerical_cols)} numerical, {len(categorical_cols)} categorical features\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if 'price' in col.lower() or 'amount' in col.lower():\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # High cardinality: label encoding\n",
    "    high_cardinality = [col for col in categorical_cols if df[col].nunique() > 10]\n",
    "    for col in high_cardinality:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        numerical_cols.append(f'{col}_encoded')\n",
    "    \n",
    "    # Low cardinality: one-hot encoding\n",
    "    low_cardinality = [col for col in categorical_cols if df[col].nunique() <= 10]\n",
    "    df_encoded = pd.get_dummies(df[low_cardinality], prefix=low_cardinality, drop_first=True)\n",
    "    \n",
    "    # Create final feature matrix\n",
    "    X = pd.concat([df[numerical_cols], df_encoded], axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    feature_summary = {\n",
    "        'feature_names': list(X.columns),\n",
    "        'numerical_features': numerical_cols,\n",
    "        'categorical_features': list(df_encoded.columns),\n",
    "        'label_encoders': label_encoders,\n",
    "        'target_column': target,\n",
    "        'total_features': len(X.columns)\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Preprocessing complete: {X.shape}\")\n",
    "    return X, y, feature_summary\n",
    "\n",
    "# Apply preprocessing\n",
    "X, y, feature_summary = preprocess_for_ml(enriched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_features(X, y):\n",
    "    \"\"\"Quick validation of feature quality\"\"\"\n",
    "    print(\"ðŸŽ¯ Validating features...\")\n",
    "    \n",
    "    # Check completeness\n",
    "    missing_pct = (X.isnull().sum().sum() / (X.shape[0] * X.shape[1])) * 100\n",
    "    print(f\"   Missing values: {missing_pct:.3f}%\")\n",
    "    \n",
    "    # Check variance\n",
    "    low_variance = sum(1 for col in X.columns if X[col].var() < 0.01)\n",
    "    print(f\"   Low variance features: {low_variance}\")\n",
    "    \n",
    "    # Check correlations\n",
    "    correlations = [abs(X[col].corr(y)) for col in X.columns]\n",
    "    correlations = [c for c in correlations if not np.isnan(c)]\n",
    "    strong_corr = sum(1 for c in correlations if c > 0.1)\n",
    "    \n",
    "    print(f\"   Strong correlations (>0.1): {strong_corr}\")\n",
    "    print(f\"   Average correlation: {np.mean(correlations):.4f}\")\n",
    "    \n",
    "    # Overall score\n",
    "    checks = [\n",
    "        missing_pct < 1.0,\n",
    "        low_variance < X.shape[1] * 0.1,\n",
    "        strong_corr > 10,\n",
    "        X.shape[1] < X.shape[0] * 0.1\n",
    "    ]\n",
    "    \n",
    "    score = (sum(checks) / len(checks)) * 100\n",
    "    print(f\"\\nðŸ† Quality Score: {score:.0f}%\")\n",
    "    \n",
    "    return {\n",
    "        'missing_percentage': missing_pct,\n",
    "        'low_variance_features': low_variance,\n",
    "        'strong_correlations': strong_corr,\n",
    "        'avg_correlation': np.mean(correlations),\n",
    "        'overall_score': score\n",
    "    }\n",
    "\n",
    "validation_results = validate_features(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"ðŸ’¾ Saving results...\")\n",
    "\n",
    "# Save feature matrix and target\n",
    "X.to_csv(processed_dir / \"feature_matrix.csv\", index=False)\n",
    "y.to_csv(processed_dir / \"target_variable.csv\", index=False)\n",
    "\n",
    "# Save metadata\n",
    "with open(processed_dir / \"feature_summary.pkl\", 'wb') as f:\n",
    "    pickle.dump(feature_summary, f)\n",
    "\n",
    "with open(processed_dir / \"validation_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(validation_results, f)\n",
    "\n",
    "# Feature statistics\n",
    "feature_stats = pd.DataFrame({\n",
    "    'feature_name': X.columns,\n",
    "    'mean': X.mean(),\n",
    "    'std': X.std(),\n",
    "    'correlation_with_target': [X[col].corr(y) for col in X.columns]\n",
    "}).round(4)\n",
    "\n",
    "feature_stats.to_csv(processed_dir / \"feature_statistics.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Files saved to: {processed_dir}\")\n",
    "print(f\"   ðŸ“ feature_matrix.csv: {X.shape}\")\n",
    "print(f\"   ðŸ“ target_variable.csv: {len(y)} records\")\n",
    "print(f\"   ðŸ“ feature_summary.pkl: metadata\")\n",
    "print(f\"   ðŸ“ feature_statistics.csv: correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and visualization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Feature breakdown\n",
    "temporal_features = len([f for f in X.columns if any(x in f.lower() for x in ['day', 'week', 'month', 'year', 'quarter', 'holiday', 'weekend', 'sin', 'cos'])])\n",
    "product_features = len([f for f in X.columns if any(x in f.lower() for x in ['price', 'brand', 'category', 'rating', 'review', 'product'])])\n",
    "customer_features = len([f for f in X.columns if any(x in f.lower() for x in ['customer', 'segment', 'lifecycle', 'rfm'])])\n",
    "lag_features = len([f for f in X.columns if 'lag_' in f or '_ma_' in f])\n",
    "interaction_features = len([f for f in X.columns if '_x_' in f])\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature Summary:\")\n",
    "print(f\"   ðŸ•’ Temporal: {temporal_features}\")\n",
    "print(f\"   ðŸ·ï¸ Product: {product_features}\")\n",
    "print(f\"   ðŸ‘¥ Customer: {customer_features}\")\n",
    "print(f\"   ðŸ“Š Lag: {lag_features}\")\n",
    "print(f\"   ðŸ”— Interaction: {interaction_features}\")\n",
    "print(f\"   ðŸ“‹ Total: {X.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Dataset Ready:\")\n",
    "print(f\"   Records: {X.shape[0]:,}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Quality Score: {validation_results['overall_score']:.0f}%\")\n",
    "print(f\"   Target: {feature_summary['target_column']}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"   ðŸ“‚ Continue to: 03_train_model.ipynb\")\n",
    "print(f\"   ðŸŽ¯ Train Random Forest for sales forecasting\")\n",
    "\n",
    "# Quick visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Target distribution\n",
    "axes[0].hist(y, bins=30, alpha=0.7)\n",
    "axes[0].set_title('Target Distribution')\n",
    "axes[0].set_xlabel('Quantity')\n",
    "axes[0].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Feature types\n",
    "feature_types = {\n",
    "    'Temporal': temporal_features,\n",
    "    'Product': product_features,\n",
    "    'Customer': customer_features,\n",
    "    'Lag': lag_features,\n",
    "    'Interaction': interaction_features\n",
    "}\n",
    "\n",
    "axes[1].pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%')\n",
    "axes[1].set_title('Feature Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}