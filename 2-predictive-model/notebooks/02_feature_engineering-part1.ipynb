{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Sales Forecasting - Part 1\n",
    "## Module 2: Predictive Model - Basic Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Create fundamental features from raw sales data\n",
    "\n",
    "**This notebook covers:**\n",
    "- Data integration and enrichment\n",
    "- Temporal feature engineering\n",
    "- Product and category features\n",
    "- Customer behavioral features\n",
    "\n",
    "**Next notebook:** `02_feature_engineering_part2.ipynb` - Advanced features and preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Feature engineering started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = Path(\"../../datasets\")\n",
    "processed_dir = data_dir / \"processed\"\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "print(\"üìä Loading datasets...\")\n",
    "\n",
    "# Load sales data\n",
    "sales_df = pd.read_csv(data_dir / \"sales_historical_data.csv\")\n",
    "print(f\"‚úÖ Sales data loaded: {len(sales_df):,} records\")\n",
    "\n",
    "# Load product catalog\n",
    "products_df = pd.read_csv(data_dir / \"product_catalog.csv\")\n",
    "print(f\"‚úÖ Product catalog loaded: {len(products_df):,} records\")\n",
    "\n",
    "# Load customer behavior data\n",
    "behavior_df = pd.read_csv(data_dir / \"customer_behavior.csv\")\n",
    "print(f\"‚úÖ Customer behavior loaded: {len(behavior_df):,} records\")\n",
    "\n",
    "# Display basic info about sales data\n",
    "print(f\"\\nüìà Sales Data Overview:\")\n",
    "print(f\"   Date range: {sales_df['date'].min()} to {sales_df['date'].max()}\")\n",
    "print(f\"   Categories: {sales_df['category'].nunique()} unique\")\n",
    "print(f\"   Products: {sales_df['product_id'].nunique()} unique\")\n",
    "print(f\"   Customers: {sales_df['customer_id'].nunique()} unique\")\n",
    "print(f\"   Total revenue: ${sales_df['total_amount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 3: Data Integration and Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_datasets(sales_df, products_df, behavior_df):\n",
    "    \"\"\"\n",
    "    Integrate multiple datasets to create enriched features\n",
    "    \"\"\"\n",
    "    print(\"üîó Integrating datasets...\")\n",
    "    \n",
    "    # Start with sales data as base\n",
    "    enriched_df = sales_df.copy()\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    enriched_df['date'] = pd.to_datetime(enriched_df['date'])\n",
    "    \n",
    "    # Merge with product information\n",
    "    print(\"   üì¶ Adding product features...\")\n",
    "    product_features = products_df[[\n",
    "        'product_id', 'brand', 'price', 'rating', 'review_count', \n",
    "        'weight_kg', 'is_active'\n",
    "    ]].copy()\n",
    "    \n",
    "    enriched_df = enriched_df.merge(\n",
    "        product_features, \n",
    "        on='product_id', \n",
    "        how='left',\n",
    "        suffixes=('', '_catalog')\n",
    "    )\n",
    "    \n",
    "    # Calculate customer interaction metrics from behavior data\n",
    "    print(\"   üë• Adding customer behavior features...\")\n",
    "    \n",
    "    # Convert behavior timestamp\n",
    "    behavior_df['timestamp'] = pd.to_datetime(behavior_df['timestamp'])\n",
    "    behavior_df['date'] = behavior_df['timestamp'].dt.date\n",
    "    \n",
    "    # Customer activity aggregations\n",
    "    customer_metrics = behavior_df.groupby('customer_id').agg({\n",
    "        'session_duration_minutes': ['mean', 'sum'],\n",
    "        'pages_viewed': ['mean', 'sum'],\n",
    "        'interaction_type': 'count',\n",
    "        'device_type': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown',\n",
    "        'customer_segment': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    customer_metrics.columns = [\n",
    "        'avg_session_duration', 'total_session_duration',\n",
    "        'avg_pages_viewed', 'total_pages_viewed',\n",
    "        'total_interactions', 'preferred_device', 'customer_segment'\n",
    "    ]\n",
    "    \n",
    "    customer_metrics = customer_metrics.reset_index()\n",
    "    \n",
    "    # Merge customer metrics\n",
    "    enriched_df = enriched_df.merge(\n",
    "        customer_metrics,\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing values for customers not in behavior data\n",
    "    behavior_fill_values = {\n",
    "        'avg_session_duration': enriched_df['avg_session_duration'].median(),\n",
    "        'total_session_duration': 0,\n",
    "        'avg_pages_viewed': enriched_df['avg_pages_viewed'].median(),\n",
    "        'total_pages_viewed': 0,\n",
    "        'total_interactions': 0,\n",
    "        'preferred_device': 'Unknown',\n",
    "        'customer_segment': 'Unknown'\n",
    "    }\n",
    "    \n",
    "    enriched_df = enriched_df.fillna(behavior_fill_values)\n",
    "    \n",
    "    print(f\"‚úÖ Datasets integrated: {len(enriched_df):,} records with {len(enriched_df.columns)} features\")\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "# Perform data integration\n",
    "enriched_data = integrate_datasets(sales_df, products_df, behavior_df)\n",
    "\n",
    "# Display sample of enriched data\n",
    "print(\"\\nüìã Sample of enriched dataset:\")\n",
    "print(enriched_data.head())\n",
    "print(f\"\\nüìä New features added: {len(enriched_data.columns) - len(sales_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÖ Step 4: Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create sophisticated temporal features for capturing seasonality and trends\n",
    "    \"\"\"\n",
    "    print(\"üìÖ Creating temporal features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    \n",
    "    # Cyclical encoding for temporal features\n",
    "    print(\"   üîÑ Creating cyclical temporal features...\")\n",
    "    \n",
    "    # Month cyclical (12 months)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Day of week cyclical (7 days)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Day of year cyclical (365 days)\n",
    "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "    # Week of year cyclical (52 weeks)\n",
    "    df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "    df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "    \n",
    "    # Business calendar features\n",
    "    print(\"   üìÜ Adding business calendar features...\")\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Month-end indicator (last 3 days of month)\n",
    "    df['is_month_end'] = (df['date'].dt.days_in_month - df['day'] <= 2).astype(int)\n",
    "    \n",
    "    # Quarter features\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['is_q4'] = (df['quarter'] == 4).astype(int)  # Holiday season\n",
    "    \n",
    "    # Holiday seasons (approximate)\n",
    "    df['is_holiday_season'] = (\n",
    "        ((df['month'] == 11) & (df['day'] >= 20)) |  # Late November\n",
    "        (df['month'] == 12) |  # December\n",
    "        ((df['month'] == 1) & (df['day'] <= 7))     # Early January\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Summer season\n",
    "    df['is_summer'] = (df['month'].isin([6, 7, 8])).astype(int)\n",
    "    \n",
    "    # Back-to-school season\n",
    "    df['is_back_to_school'] = (\n",
    "        ((df['month'] == 8) & (df['day'] >= 15)) |\n",
    "        ((df['month'] == 9) & (df['day'] <= 15))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Days since epoch (for trend analysis)\n",
    "    epoch = pd.to_datetime('2020-01-01')\n",
    "    df['days_since_epoch'] = (df['date'] - epoch).dt.days\n",
    "    \n",
    "    print(f\"‚úÖ Temporal features created: {len([col for col in df.columns if any(x in col.lower() for x in ['sin', 'cos', 'is_', 'day', 'week', 'month', 'year', 'quarter'])])} features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply temporal feature engineering\n",
    "enriched_data = create_temporal_features(enriched_data)\n",
    "\n",
    "# Visualize some temporal patterns\n",
    "print(\"\\nüìä Temporal Pattern Analysis:\")\n",
    "\n",
    "# Create visualization of temporal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Monthly sales pattern\n",
    "monthly_sales = enriched_data.groupby('month')['total_amount'].sum()\n",
    "axes[0, 0].bar(monthly_sales.index, monthly_sales.values)\n",
    "axes[0, 0].set_title('Monthly Sales Distribution')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Total Sales ($)')\n",
    "\n",
    "# Day of week pattern\n",
    "dow_sales = enriched_data.groupby('day_of_week')['total_amount'].mean()\n",
    "dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0, 1].bar(range(7), dow_sales.values)\n",
    "axes[0, 1].set_title('Average Sales by Day of Week')\n",
    "axes[0, 1].set_xlabel('Day of Week')\n",
    "axes[0, 1].set_ylabel('Average Sales ($)')\n",
    "axes[0, 1].set_xticks(range(7))\n",
    "axes[0, 1].set_xticklabels(dow_labels)\n",
    "\n",
    "# Quarterly pattern\n",
    "quarterly_sales = enriched_data.groupby('quarter')['total_amount'].sum()\n",
    "axes[1, 0].bar(quarterly_sales.index, quarterly_sales.values)\n",
    "axes[1, 0].set_title('Quarterly Sales Distribution')\n",
    "axes[1, 0].set_xlabel('Quarter')\n",
    "axes[1, 0].set_ylabel('Total Sales ($)')\n",
    "\n",
    "# Holiday vs non-holiday sales\n",
    "holiday_comparison = enriched_data.groupby('is_holiday_season')['total_amount'].mean()\n",
    "axes[1, 1].bar(['Regular Season', 'Holiday Season'], holiday_comparison.values)\n",
    "axes[1, 1].set_title('Holiday Season Impact on Sales')\n",
    "axes[1, 1].set_ylabel('Average Sales ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Key Temporal Insights:\")\n",
    "print(f\"   Highest sales month: {monthly_sales.idxmax()} (${monthly_sales.max():,.0f})\")\n",
    "print(f\"   Best day of week: {dow_labels[dow_sales.idxmax()]} (${dow_sales.max():.0f} avg)\")\n",
    "print(f\"   Holiday season lift: {((holiday_comparison[1] / holiday_comparison[0] - 1) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 5: Product and Category Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_features(df):\n",
    "    \"\"\"\n",
    "    Create product-specific features including pricing, popularity, and performance metrics\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è Creating product and category features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Limpiar columnas duplicadas si existen\n",
    "    columns_to_clean = [col for col in df.columns if col.endswith('_x') or col.endswith('_y')]\n",
    "    if columns_to_clean:\n",
    "        print(f\"   üßπ Cleaning {len(columns_to_clean)} duplicate columns...\")\n",
    "        df = df.drop(columns=columns_to_clean)\n",
    "    \n",
    "    # Price-related features\n",
    "    print(\"   üí∞ Creating price features...\")\n",
    "    \n",
    "    # Price difference between unit price and catalog price\n",
    "    df['price_difference'] = df['unit_price'] - df['price']\n",
    "    df['price_ratio'] = df['unit_price'] / (df['price'] + 0.01)  # Avoid division by zero\n",
    "    \n",
    "    # Discount indicator\n",
    "    df['is_discounted'] = (df['price_difference'] < -0.01).astype(int)\n",
    "    df['discount_percentage'] = np.where(\n",
    "        df['is_discounted'],\n",
    "        ((df['price'] - df['unit_price']) / df['price'] * 100).clip(0, 100),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Price categories by quartiles within each category\n",
    "    df['price_quartile'] = df.groupby('category')['unit_price'].transform(\n",
    "        lambda x: pd.qcut(x, q=4, labels=['Low', 'Medium', 'High', 'Premium'], duplicates='drop')\n",
    "    )\n",
    "    \n",
    "    # Category-level aggregations - SOLO SI NO EXISTEN\n",
    "    print(\"   üì¶ Creating category features...\")\n",
    "    \n",
    "    category_cols_needed = ['category_avg_price', 'category_price_std', 'category_median_price',\n",
    "                           'category_avg_quantity', 'category_quantity_std', 'category_avg_amount', \n",
    "                           'category_total_amount', 'category_transaction_count']\n",
    "    \n",
    "    missing_category_cols = [col for col in category_cols_needed if col not in df.columns]\n",
    "    \n",
    "    if missing_category_cols:\n",
    "        print(f\"   Creating missing category columns: {len(missing_category_cols)}\")\n",
    "        \n",
    "        category_stats = df.groupby('category').agg({\n",
    "            'unit_price': ['mean', 'std', 'median'],\n",
    "            'quantity': ['mean', 'std'],\n",
    "            'total_amount': ['mean', 'sum', 'count']\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        category_stats.columns = category_cols_needed\n",
    "        category_stats = category_stats.reset_index()\n",
    "        \n",
    "        # Merge category statistics\n",
    "        df = df.merge(category_stats, on='category', how='left', suffixes=('', '_new'))\n",
    "    else:\n",
    "        print(\"   ‚úÖ Category features already exist\")\n",
    "    \n",
    "    # Product performance features - CREAR PRIMERO\n",
    "    print(\"   üìä Creating product performance features...\")\n",
    "    \n",
    "    # Product-level aggregations (historical performance)\n",
    "    product_stats = df.groupby('product_id').agg({\n",
    "        'quantity': ['sum', 'mean', 'count'],\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    product_stats.columns = [\n",
    "        'product_total_qty', 'product_avg_qty', 'product_transaction_count',\n",
    "        'product_total_revenue', 'product_avg_revenue',\n",
    "        'product_unique_customers'\n",
    "    ]\n",
    "    \n",
    "    product_stats = product_stats.reset_index()\n",
    "    \n",
    "    # Calculate product popularity score\n",
    "    product_stats['product_popularity_score'] = (\n",
    "        product_stats['product_transaction_count'] * 0.4 +\n",
    "        product_stats['product_unique_customers'] * 0.6\n",
    "    ).round(2)\n",
    "    \n",
    "    # Merge product statistics - CON VERIFICACI√ìN\n",
    "    original_cols = set(df.columns)\n",
    "    df = df.merge(product_stats, on='product_id', how='left', suffixes=('', '_prod'))\n",
    "    new_cols = set(df.columns) - original_cols\n",
    "    print(f\"   ‚úÖ Added product columns: {len(new_cols)}\")\n",
    "    \n",
    "    # Verificar que product_popularity_score existe\n",
    "    if 'product_popularity_score' not in df.columns:\n",
    "        print(\"   ‚ùå product_popularity_score not found, creating fallback...\")\n",
    "        df['product_popularity_score'] = 1.0\n",
    "    \n",
    "    # Relative performance features - AHORA S√ç SE PUEDEN CREAR\n",
    "    print(\"   üìà Creating relative performance features...\")\n",
    "    \n",
    "    # Price relative to category average\n",
    "    if 'category_avg_price' in df.columns:\n",
    "        df['price_vs_category_avg'] = df['unit_price'] / (df['category_avg_price'] + 0.01)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Warning: category_avg_price not found, creating fallback...\")\n",
    "        df['price_vs_category_avg'] = 1.0\n",
    "    \n",
    "    # Product performance relative to category - AHORA CON VERIFICACI√ìN\n",
    "    if 'product_popularity_score' in df.columns:\n",
    "        category_pop_mean = df.groupby('category')['product_popularity_score'].transform('mean')\n",
    "        df['product_performance_vs_category'] = df['product_popularity_score'] / (category_pop_mean + 0.01)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Warning: product_popularity_score not found for relative performance\")\n",
    "        df['product_performance_vs_category'] = 1.0\n",
    "    \n",
    "    # Brand features\n",
    "    print(\"   üè∑Ô∏è Creating brand features...\")\n",
    "    \n",
    "    # Brand-level aggregations\n",
    "    brand_stats = df.groupby('brand').agg({\n",
    "        'total_amount': ['sum', 'mean', 'count'],\n",
    "        'rating': 'mean',\n",
    "        'review_count': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    brand_stats.columns = [\n",
    "        'brand_total_revenue', 'brand_avg_revenue', 'brand_transaction_count',\n",
    "        'brand_avg_rating', 'brand_avg_reviews'\n",
    "    ]\n",
    "    \n",
    "    brand_stats = brand_stats.reset_index()\n",
    "    \n",
    "    # Merge brand statistics\n",
    "    df = df.merge(brand_stats, on='brand', how='left', suffixes=('', '_brand'))\n",
    "    \n",
    "    # Calculate brand market share within category\n",
    "    try:\n",
    "        category_totals = df.groupby('category')['total_amount'].sum().reset_index()\n",
    "        category_totals.columns = ['category', 'category_total_revenue_calc']\n",
    "        \n",
    "        brand_category_revenue = df.groupby(['brand', 'category'])['total_amount'].sum().reset_index()\n",
    "        brand_category_revenue = brand_category_revenue.merge(category_totals, on='category')\n",
    "        brand_category_revenue['brand_market_share'] = (\n",
    "            brand_category_revenue['total_amount'] / brand_category_revenue['category_total_revenue_calc'] * 100\n",
    "        ).round(2)\n",
    "        \n",
    "        # Merge brand market share\n",
    "        df = df.merge(\n",
    "            brand_category_revenue[['brand', 'category', 'brand_market_share']], \n",
    "            on=['brand', 'category'], \n",
    "            how='left',\n",
    "            suffixes=('', '_market')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Warning: Could not calculate brand market share: {e}\")\n",
    "        df['brand_market_share'] = 0.0\n",
    "    \n",
    "    # Product quality indicators\n",
    "    print(\"   ‚≠ê Creating quality indicators...\")\n",
    "    \n",
    "    # Rating-based features\n",
    "    df['is_high_rated'] = (df['rating'] >= 4.0).astype(int)\n",
    "    df['is_well_reviewed'] = (df['review_count'] >= 50).astype(int)\n",
    "    \n",
    "    # Review density (reviews per rating point)\n",
    "    df['review_density'] = df['review_count'] / (df['rating'] + 0.1)\n",
    "    \n",
    "    # Limpiar columnas duplicadas finales\n",
    "    final_columns_to_clean = [col for col in df.columns if col.endswith('_x') or col.endswith('_y') or col.endswith('_prod') or col.endswith('_brand') or col.endswith('_market')]\n",
    "    if final_columns_to_clean:\n",
    "        print(f\"   üßπ Final cleanup: removing {len(final_columns_to_clean)} duplicate columns\")\n",
    "        df = df.drop(columns=final_columns_to_clean)\n",
    "    \n",
    "    product_feature_count = len([col for col in df.columns if any(x in col.lower() for x in ['price', 'brand', 'category', 'product', 'rating', 'review'])])\n",
    "    print(f\"‚úÖ Product features created: {product_feature_count} features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply product feature engineering\n",
    "enriched_data = create_product_features(enriched_data)\n",
    "\n",
    "# Analyze product feature distributions\n",
    "print(\"\\nüìä Product Feature Analysis:\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price distribution by category\n",
    "sns.boxplot(data=enriched_data, x='category', y='unit_price', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Price Distribution by Category')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Discount percentage distribution\n",
    "discount_data = enriched_data[enriched_data['is_discounted'] == 1]['discount_percentage']\n",
    "axes[0, 1].hist(discount_data, bins=20, alpha=0.7)\n",
    "axes[0, 1].set_title('Discount Percentage Distribution')\n",
    "axes[0, 1].set_xlabel('Discount %')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Rating vs Sales correlation\n",
    "rating_sales = enriched_data.groupby('rating')['total_amount'].mean()\n",
    "axes[1, 0].scatter(rating_sales.index, rating_sales.values, alpha=0.6)\n",
    "axes[1, 0].set_title('Rating vs Average Sales')\n",
    "axes[1, 0].set_xlabel('Product Rating')\n",
    "axes[1, 0].set_ylabel('Average Sales ($)')\n",
    "\n",
    "# Brand performance\n",
    "top_brands = enriched_data.groupby('brand')['brand_total_revenue'].first().nlargest(10)\n",
    "axes[1, 1].barh(range(len(top_brands)), top_brands.values)\n",
    "axes[1, 1].set_title('Top 10 Brands by Revenue')\n",
    "axes[1, 1].set_xlabel('Total Revenue ($)')\n",
    "axes[1, 1].set_yticks(range(len(top_brands)))\n",
    "axes[1, 1].set_yticklabels(top_brands.index)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nüìà Key Product Insights:\")\n",
    "discount_rate = (enriched_data['is_discounted'].sum() / len(enriched_data)) * 100\n",
    "avg_discount = enriched_data[enriched_data['is_discounted'] == 1]['discount_percentage'].mean()\n",
    "high_rated_performance = enriched_data.groupby('is_high_rated')['total_amount'].mean()\n",
    "\n",
    "print(f\"   Discount rate: {discount_rate:.1f}% of transactions\")\n",
    "print(f\"   Average discount: {avg_discount:.1f}%\")\n",
    "print(f\"   High-rated products sales lift: {((high_rated_performance[1] / high_rated_performance[0] - 1) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë• Step 6: Customer and Behavioral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_features(df):\n",
    "    \"\"\"\n",
    "    Create customer-specific features including segmentation, behavior, and purchase patterns\n",
    "    \"\"\"\n",
    "    print(\"üë• Creating customer and behavioral features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Customer transaction history features\n",
    "    print(\"   üìä Creating customer transaction features...\")\n",
    "    \n",
    "    # Customer-level aggregations\n",
    "    customer_stats = df.groupby('customer_id').agg({\n",
    "        'total_amount': ['sum', 'mean', 'count'],\n",
    "        'quantity': ['sum', 'mean'],\n",
    "        'product_id': 'nunique',\n",
    "        'category': 'nunique',\n",
    "        'brand': 'nunique',\n",
    "        'date': ['min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    customer_stats.columns = [\n",
    "        'customer_total_spent', 'customer_avg_order_value', 'customer_transaction_count',\n",
    "        'customer_total_quantity', 'customer_avg_quantity',\n",
    "        'customer_unique_products', 'customer_unique_categories', 'customer_unique_brands',\n",
    "        'customer_first_purchase', 'customer_last_purchase'\n",
    "    ]\n",
    "    \n",
    "    customer_stats = customer_stats.reset_index()\n",
    "    \n",
    "    # Calculate customer lifetime value and recency\n",
    "    customer_stats['customer_lifetime_days'] = (\n",
    "        pd.to_datetime(customer_stats['customer_last_purchase']) - \n",
    "        pd.to_datetime(customer_stats['customer_first_purchase'])\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    # Days since last purchase (recency)\n",
    "    reference_date = df['date'].max()\n",
    "    customer_stats['customer_recency_days'] = (\n",
    "        reference_date - pd.to_datetime(customer_stats['customer_last_purchase'])\n",
    "    ).dt.days\n",
    "    \n",
    "    # Customer value per day\n",
    "    customer_stats['customer_value_per_day'] = (\n",
    "        customer_stats['customer_total_spent'] / customer_stats['customer_lifetime_days']\n",
    "    ).round(2)\n",
    "    \n",
    "    # Customer diversity score (based on categories and brands purchased)\n",
    "    customer_stats['customer_diversity_score'] = (\n",
    "        customer_stats['customer_unique_categories'] * 0.6 +\n",
    "        customer_stats['customer_unique_brands'] * 0.4\n",
    "    ).round(2)\n",
    "    \n",
    "    # Merge customer statistics\n",
    "    df = df.merge(customer_stats, on='customer_id', how='left')\n",
    "    \n",
    "    # Customer segmentation based on RFM analysis\n",
    "    print(\"   üéØ Creating customer segmentation features...\")\n",
    "    \n",
    "    # RFM quintiles\n",
    "    df['recency_quintile'] = pd.qcut(df['customer_recency_days'], q=5, labels=[5,4,3,2,1], duplicates='drop')\n",
    "    df['frequency_quintile'] = pd.qcut(df['customer_transaction_count'], q=5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "    df['monetary_quintile'] = pd.qcut(df['customer_total_spent'], q=5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "    \n",
    "    # Convert to numeric\n",
    "    df['recency_quintile'] = pd.to_numeric(df['recency_quintile'], errors='coerce')\n",
    "    df['frequency_quintile'] = pd.to_numeric(df['frequency_quintile'], errors='coerce')\n",
    "    df['monetary_quintile'] = pd.to_numeric(df['monetary_quintile'], errors='coerce')\n",
    "    \n",
    "    # Fill NaN values with median\n",
    "    df['recency_quintile'] = df['recency_quintile'].fillna(df['recency_quintile'].median())\n",
    "    df['frequency_quintile'] = df['frequency_quintile'].fillna(df['frequency_quintile'].median())\n",
    "    df['monetary_quintile'] = df['monetary_quintile'].fillna(df['monetary_quintile'].median())\n",
    "    \n",
    "    # RFM Score\n",
    "    df['rfm_score'] = (\n",
    "        df['recency_quintile'].astype(int) * 100 +\n",
    "        df['frequency_quintile'].astype(int) * 10 +\n",
    "        df['monetary_quintile'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Customer lifecycle stage\n",
    "    def assign_customer_lifecycle(row):\n",
    "        if row['customer_transaction_count'] == 1:\n",
    "            return 'New'\n",
    "        elif row['customer_recency_days'] <= 30:\n",
    "            if row['customer_transaction_count'] >= 5:\n",
    "                return 'Loyal'\n",
    "            else:\n",
    "                return 'Active'\n",
    "        elif row['customer_recency_days'] <= 90:\n",
    "            return 'Regular'\n",
    "        elif row['customer_recency_days'] <= 180:\n",
    "            return 'At_Risk'\n",
    "        else:\n",
    "            return 'Dormant'\n",
    "    \n",
    "    df['customer_lifecycle_stage'] = df.apply(assign_customer_lifecycle, axis=1)\n",
    "    \n",
    "    # Channel and regional preferences\n",
    "    print(\"   üõí Creating channel and regional features...\")\n",
    "    \n",
    "    # Customer's preferred channel\n",
    "    customer_channel_prefs = df.groupby('customer_id')['channel'].agg(\n",
    "        lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "    ).reset_index()\n",
    "    customer_channel_prefs.columns = ['customer_id', 'customer_preferred_channel']\n",
    "    \n",
    "    df = df.merge(customer_channel_prefs, on='customer_id', how='left')\n",
    "    \n",
    "    # Channel consistency (how often customer uses their preferred channel)\n",
    "    df['uses_preferred_channel'] = (df['channel'] == df['customer_preferred_channel']).astype(int)\n",
    "    \n",
    "    customer_channel_consistency = df.groupby('customer_id')['uses_preferred_channel'].mean().reset_index()\n",
    "    customer_channel_consistency.columns = ['customer_id', 'customer_channel_consistency']\n",
    "    \n",
    "    df = df.merge(customer_channel_consistency, on='customer_id', how='left')\n",
    "    \n",
    "    # Regional features\n",
    "    region_stats = df.groupby('region').agg({\n",
    "        'total_amount': ['mean', 'std'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    region_stats.columns = ['region_avg_order_value', 'region_order_std', 'region_unique_customers']\n",
    "    region_stats = region_stats.reset_index()\n",
    "    \n",
    "    df = df.merge(region_stats, on='region', how='left')\n",
    "    \n",
    "    # Customer vs regional performance\n",
    "    df['customer_vs_region_performance'] = df['customer_avg_order_value'] / df['region_avg_order_value']\n",
    "    \n",
    "    # Behavioral features from customer behavior data\n",
    "    print(\"   üé≠ Enhancing with behavioral features...\")\n",
    "    \n",
    "    # Digital engagement score\n",
    "    df['digital_engagement_score'] = (\n",
    "        df['avg_session_duration'] * 0.3 +\n",
    "        df['avg_pages_viewed'] * 0.4 +\n",
    "        (df['total_interactions'] / df['customer_transaction_count']) * 0.3\n",
    "    ).round(2)\n",
    "    \n",
    "    # Customer segment encoding\n",
    "    segment_values = {\n",
    "        'Premium': 4,\n",
    "        'Regular': 3,\n",
    "        'Occasional': 2,\n",
    "        'New': 1,\n",
    "        'Unknown': 0\n",
    "    }\n",
    "    \n",
    "    df['customer_segment_encoded'] = df['customer_segment'].map(segment_values)\n",
    "    \n",
    "    print(f\"‚úÖ Customer features created: {len([col for col in df.columns if any(x in col.lower() for x in ['customer', 'rfm', 'lifecycle', 'channel', 'region', 'segment'])])} features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply customer feature engineering\n",
    "enriched_data = create_customer_features(enriched_data)\n",
    "\n",
    "# Analyze customer features\n",
    "print(\"\\nüìä Customer Feature Analysis:\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Customer lifecycle distribution\n",
    "lifecycle_counts = enriched_data['customer_lifecycle_stage'].value_counts()\n",
    "axes[0, 0].pie(lifecycle_counts.values, labels=lifecycle_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 0].set_title('Customer Lifecycle Distribution')\n",
    "\n",
    "# RFM Score distribution\n",
    "axes[0, 1].hist(enriched_data['rfm_score'], bins=20, alpha=0.7)\n",
    "axes[0, 1].set_title('RFM Score Distribution')\n",
    "axes[0, 1].set_xlabel('RFM Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Customer segment performance\n",
    "segment_performance = enriched_data.groupby('customer_segment')['customer_avg_order_value'].mean().sort_values(ascending=False)\n",
    "axes[1, 0].bar(segment_performance.index, segment_performance.values)\n",
    "axes[1, 0].set_title('Average Order Value by Customer Segment')\n",
    "axes[1, 0].set_ylabel('Average Order Value ($)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Channel preferences\n",
    "channel_counts = enriched_data['customer_preferred_channel'].value_counts()\n",
    "axes[1, 1].bar(channel_counts.index, channel_counts.values)\n",
    "axes[1, 1].set_title('Customer Channel Preferences')\n",
    "axes[1, 1].set_ylabel('Number of Customers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nüìà Key Customer Insights:\")\n",
    "avg_clv = enriched_data['customer_total_spent'].mean()\n",
    "loyal_customers = (enriched_data['customer_lifecycle_stage'] == 'Loyal').sum()\n",
    "digital_engagement = enriched_data['digital_engagement_score'].mean()\n",
    "\n",
    "print(f\"   Average Customer Lifetime Value: ${avg_clv:.2f}\")\n",
    "print(f\"   Loyal customers: {loyal_customers:,} ({loyal_customers/len(enriched_data)*100:.1f}%)\")\n",
    "print(f\"   Average digital engagement score: {digital_engagement:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enriched data with basic features for Part 2\n",
    "print(\"üíæ Saving intermediate results...\")\n",
    "\n",
    "# Save enriched dataset\n",
    "enriched_data.to_csv(processed_dir / \"enriched_data_part1.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Intermediate data saved: {processed_dir / 'enriched_data_part1.csv'}\")\n",
    "print(f\"üìä Current dataset shape: {enriched_data.shape}\")\n",
    "print(f\"üìà Features created so far: {len(enriched_data.columns)}\")\n",
    "\n",
    "# Summary of Part 1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PART 1 FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Features Created in Part 1:\")\n",
    "temporal_features = len([col for col in enriched_data.columns if any(x in col.lower() for x in ['sin', 'cos', 'is_', 'day', 'week', 'month', 'year', 'quarter'])])\n",
    "product_features = len([col for col in enriched_data.columns if any(x in col.lower() for x in ['price', 'brand', 'category', 'product', 'rating', 'review'])])\n",
    "customer_features = len([col for col in enriched_data.columns if any(x in col.lower() for x in ['customer', 'rfm', 'lifecycle', 'channel', 'region', 'segment'])])\n",
    "\n",
    "print(f\"   üïí Temporal features: {temporal_features}\")\n",
    "print(f\"   üè∑Ô∏è Product features: {product_features}\")\n",
    "print(f\"   üë• Customer features: {customer_features}\")\n",
    "print(f\"   üìã Total features: {len(enriched_data.columns)}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   üìÇ Continue to: 02_feature_engineering_part2.ipynb\")\n",
    "print(f\"   üéØ Part 2 will add: Lag features, interactions, and preprocessing\")\n",
    "print(f\"   üìÑ Then proceed to: 03_train_model.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Part 1 Summary\n",
    "\n",
    "This notebook has successfully created the fundamental features for sales forecasting:\n",
    "\n",
    "‚úÖ **Data Integration** - Combined sales, product, and customer behavior data  \n",
    "‚úÖ **Temporal Features** - Seasonal patterns, cyclical encoding, business calendars  \n",
    "‚úÖ **Product Features** - Pricing analysis, brand performance, quality metrics  \n",
    "‚úÖ **Customer Features** - RFM segmentation, behavioral scoring, lifecycle stages  \n",
    "\n",
    "**Ready for Part 2:** Advanced feature engineering including lag features, interaction terms, and final preprocessing.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
