{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Environment Verification\n",
    "## üîç Validating Workshop Setup\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose:** Verify that all components are properly installed and configured for the OpenShift AI workshop.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed dataset download (`download_datasets.ipynb`)\n",
    "- Completed requirements installation (`install_requirements.ipynb`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## üîß System Resources Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üñ•Ô∏è  SYSTEM INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available system resources\n",
    "print(\"üíæ RESOURCE AVAILABILITY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Memory information\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {memory.total / 1024**3:.2f} GB\")\n",
    "print(f\"Available RAM: {memory.available / 1024**3:.2f} GB\")\n",
    "print(f\"Used RAM: {memory.used / 1024**3:.2f} GB ({memory.percent:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# CPU information\n",
    "print(f\"CPU Cores (physical): {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"CPU Cores (logical): {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"CPU Usage: {psutil.cpu_percent(interval=1):.1f}%\")\n",
    "print()\n",
    "\n",
    "# Disk space\n",
    "disk = psutil.disk_usage('/')\n",
    "print(f\"Disk Total: {disk.total / 1024**3:.2f} GB\")\n",
    "print(f\"Disk Free: {disk.free / 1024**3:.2f} GB\")\n",
    "print(f\"Disk Used: {disk.used / 1024**3:.2f} GB ({(disk.used/disk.total)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Resource adequacy check\n",
    "print(\"‚úÖ ADEQUACY CHECK\")\n",
    "ram_ok = memory.available / 1024**3 >= 2.0  # At least 2GB available\n",
    "cpu_ok = psutil.cpu_count(logical=True) >= 2  # At least 2 cores\n",
    "disk_ok = disk.free / 1024**3 >= 5.0  # At least 5GB free\n",
    "\n",
    "print(f\"RAM (‚â•2GB available): {'‚úÖ' if ram_ok else '‚ùå'}\")\n",
    "print(f\"CPU (‚â•2 cores): {'‚úÖ' if cpu_ok else '‚ùå'}\")\n",
    "print(f\"Disk (‚â•5GB free): {'‚úÖ' if disk_ok else '‚ùå'}\")\n",
    "\n",
    "if all([ram_ok, cpu_ok, disk_ok]):\n",
    "    print(\"\\nüéâ System resources are adequate for the workshop!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  System resources may be insufficient. Consider requesting larger instance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## üìö Library Import Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML and Data Libraries\n",
    "print(\"üî¨ CORE ML & DATA LIBRARIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "libraries = [\n",
    "    ('pandas', 'pd'),\n",
    "    ('numpy', 'np'),\n",
    "    ('sklearn', None),\n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('seaborn', 'sns')\n",
    "]\n",
    "\n",
    "for lib_name, alias in libraries:\n",
    "    try:\n",
    "        if alias:\n",
    "            exec(f\"import {lib_name} as {alias}\")\n",
    "        else:\n",
    "            exec(f\"import {lib_name}\")\n",
    "        \n",
    "        # Get version if available\n",
    "        try:\n",
    "            version = eval(f\"{alias or lib_name.split('.')[0]}.__version__\")\n",
    "            print(f\"‚úÖ {lib_name:<20} v{version}\")\n",
    "        except:\n",
    "            print(f\"‚úÖ {lib_name:<20} (version not available)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {lib_name:<20} - Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX and Model Export Libraries\n",
    "print(\"\\nüîÑ ONNX & MODEL EXPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "onnx_libraries = [\n",
    "    'onnx',\n",
    "    'skl2onnx',\n",
    "    'onnxruntime'\n",
    "]\n",
    "\n",
    "for lib in onnx_libraries:\n",
    "    try:\n",
    "        exec(f\"import {lib}\")\n",
    "        try:\n",
    "            version = eval(f\"{lib}.__version__\")\n",
    "            print(f\"‚úÖ {lib:<20} v{version}\")\n",
    "        except:\n",
    "            print(f\"‚úÖ {lib:<20} (version not available)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {lib:<20} - Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain and LLM Libraries\n",
    "print(\"\\nü¶ú LANGCHAIN & LLM FRAMEWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "langchain_libs = [\n",
    "    'langchain',\n",
    "    'langchain_core',\n",
    "    'langchain_community'\n",
    "]\n",
    "\n",
    "for lib in langchain_libs:\n",
    "    try:\n",
    "        exec(f\"import {lib}\")\n",
    "        try:\n",
    "            version = eval(f\"{lib}.__version__\")\n",
    "            print(f\"‚úÖ {lib:<20} v{version}\")\n",
    "        except:\n",
    "            print(f\"‚úÖ {lib:<20} (imported successfully)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {lib:<20} - Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web and Dashboard Libraries\n",
    "print(\"\\nüåê WEB & DASHBOARD LIBRARIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "web_libs = [\n",
    "    'gradio',\n",
    "    'requests',\n",
    "    'plotly'\n",
    "]\n",
    "\n",
    "for lib in web_libs:\n",
    "    try:\n",
    "        exec(f\"import {lib}\")\n",
    "        try:\n",
    "            version = eval(f\"{lib}.__version__\")\n",
    "            print(f\"‚úÖ {lib:<20} v{version}\")\n",
    "        except:\n",
    "            print(f\"‚úÖ {lib:<20} (imported successfully)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {lib:<20} - Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and Utilities\n",
    "print(\"\\nüìä MONITORING & UTILITIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "utility_libs = [\n",
    "    'prometheus_client',\n",
    "    'yaml',\n",
    "    'json',\n",
    "    'pickle',\n",
    "    'joblib'\n",
    "]\n",
    "\n",
    "for lib in utility_libs:\n",
    "    try:\n",
    "        exec(f\"import {lib}\")\n",
    "        print(f\"‚úÖ {lib:<20} (available)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {lib:<20} - Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## üìä Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"üìÇ DATASET AVAILABILITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Expected datasets\n",
    "datasets = {\n",
    "    'sales_historical_data.csv': {\n",
    "        'description': 'Sales transactions with temporal patterns',\n",
    "        'expected_columns': ['date', 'product_id', 'quantity', 'revenue', 'customer_id'],\n",
    "        'min_rows': 1000\n",
    "    },\n",
    "    'product_catalog.csv': {\n",
    "        'description': 'Product metadata and categories',\n",
    "        'expected_columns': ['product_id', 'product_name', 'category', 'price'],\n",
    "        'min_rows': 100\n",
    "    },\n",
    "    'customer_behavior.csv': {\n",
    "        'description': 'User interaction and behavioral data',\n",
    "        'expected_columns': ['customer_id', 'session_id', 'action', 'timestamp'],\n",
    "        'min_rows': 500\n",
    "    }\n",
    "}\n",
    "\n",
    "datasets_path = 'datasets'\n",
    "all_datasets_ok = True\n",
    "\n",
    "for filename, info in datasets.items():\n",
    "    filepath = os.path.join(datasets_path, filename)\n",
    "    print(f\"\\nüìÑ {filename}\")\n",
    "    print(f\"   {info['description']}\")\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            file_size = os.path.getsize(filepath) / 1024 / 1024  # MB\n",
    "            \n",
    "            print(f\"   ‚úÖ File exists ({file_size:.2f} MB)\")\n",
    "            print(f\"   ‚úÖ Loaded successfully ({len(df):,} rows, {len(df.columns)} columns)\")\n",
    "            \n",
    "            # Check minimum rows\n",
    "            if len(df) >= info['min_rows']:\n",
    "                print(f\"   ‚úÖ Row count adequate (‚â•{info['min_rows']:,})\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Row count below expected (‚â•{info['min_rows']:,})\")\n",
    "                all_datasets_ok = False\n",
    "            \n",
    "            # Check for expected columns (at least some should be present)\n",
    "            available_cols = set(df.columns.str.lower())\n",
    "            expected_cols = set([col.lower() for col in info['expected_columns']])\n",
    "            matching_cols = available_cols.intersection(expected_cols)\n",
    "            \n",
    "            if len(matching_cols) >= len(expected_cols) * 0.6:  # At least 60% match\n",
    "                print(f\"   ‚úÖ Column structure looks good ({len(matching_cols)}/{len(expected_cols)} expected columns found)\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Column structure may need review ({len(matching_cols)}/{len(expected_cols)} expected columns found)\")\n",
    "            \n",
    "            # Show basic info\n",
    "            print(f\"   üìä Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading file: {e}\")\n",
    "            all_datasets_ok = False\n",
    "    else:\n",
    "        print(f\"   ‚ùå File not found at {filepath}\")\n",
    "        all_datasets_ok = False\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "if all_datasets_ok:\n",
    "    print(\"üéâ All datasets are available and properly formatted!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some datasets may need attention. Check the download_datasets.ipynb notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## üåê Environment Variables Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß ENVIRONMENT VARIABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check important environment variables\n",
    "env_vars = {\n",
    "    'PYTHONPATH': 'Python module search path',\n",
    "    'MODEL_ENDPOINT_URL': 'Model serving endpoint (will be set later)',\n",
    "    'WORKSHOP_PATH': 'Workshop materials path',\n",
    "    'HOME': 'User home directory',\n",
    "    'USER': 'Current user',\n",
    "    'PWD': 'Current working directory'\n",
    "}\n",
    "\n",
    "for var, description in env_vars.items():\n",
    "    value = os.environ.get(var, 'Not set')\n",
    "    status = '‚úÖ' if value != 'Not set' else '‚ö†Ô∏è '\n",
    "    print(f\"{status} {var:<20} = {value}\")\n",
    "    print(f\"   {description}\")\n",
    "    print()\n",
    "\n",
    "# Check current working directory\n",
    "print(f\"üìÅ Current working directory: {os.getcwd()}\")\n",
    "print(f\"üìÇ Files in current directory: {len(os.listdir('.'))} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## üß™ Functional Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic ML functionality\n",
    "print(\"ü§ñ MACHINE LEARNING FUNCTIONALITY TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create sample data\n",
    "    X = np.random.rand(1000, 10)\n",
    "    y = np.sum(X, axis=1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    # Split and train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"‚úÖ RandomForest training successful\")\n",
    "    print(f\"‚úÖ Model prediction working (MSE: {mse:.4f})\")\n",
    "    print(f\"‚úÖ Basic ML pipeline functional\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ML functionality test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ONNX export capability\n",
    "print(\"\\nüîÑ ONNX EXPORT FUNCTIONALITY TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    from skl2onnx import convert_sklearn\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Use the model from previous test\n",
    "    initial_type = [('float_input', FloatTensorType([None, 10]))]\n",
    "    onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "    \n",
    "    # Verify ONNX model\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    \n",
    "    # Test ONNX Runtime\n",
    "    sess = ort.InferenceSession(onnx_model.SerializeToString())\n",
    "    test_input = X_test[:5].astype(np.float32)\n",
    "    onnx_pred = sess.run(None, {'float_input': test_input})[0]\n",
    "    \n",
    "    print(f\"‚úÖ ONNX model conversion successful\")\n",
    "    print(f\"‚úÖ ONNX model validation passed\")\n",
    "    print(f\"‚úÖ ONNX Runtime inference working\")\n",
    "    print(f\"‚úÖ ONNX predictions shape: {onnx_pred.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ONNX functionality test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data visualization\n",
    "print(\"\\nüìä DATA VISUALIZATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Test matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot([1, 2, 3, 4], [1, 4, 2, 3])\n",
    "    ax.set_title('Test Plot')\n",
    "    plt.close(fig)  # Close to avoid display\n",
    "    print(\"‚úÖ Matplotlib plotting functional\")\n",
    "    \n",
    "    # Test seaborn\n",
    "    test_data = pd.DataFrame({\n",
    "        'x': np.random.randn(100),\n",
    "        'y': np.random.randn(100)\n",
    "    })\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.scatterplot(data=test_data, x='x', y='y', ax=ax)\n",
    "    plt.close(fig)\n",
    "    print(\"‚úÖ Seaborn plotting functional\")\n",
    "    \n",
    "    # Test plotly\n",
    "    fig = go.Figure(data=go.Bar(x=['A', 'B', 'C'], y=[1, 3, 2]))\n",
    "    print(\"‚úÖ Plotly plotting functional\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Visualization test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## üîó Network Connectivity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê NETWORK CONNECTIVITY TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import requests\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Test external connectivity\n",
    "test_urls = [\n",
    "    'https://httpbin.org/status/200',  # Simple HTTP test\n",
    "    'https://api.github.com',          # GitHub API\n",
    "    'https://huggingface.co'           # Hugging Face (for model downloads)\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ {url} - Accessible\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {url} - HTTP {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå {url} - Connection failed: {type(e).__name__}\")\n",
    "\n",
    "# Test DNS resolution\n",
    "print(\"\\nüîç DNS RESOLUTION TEST\")\n",
    "test_domains = ['google.com', 'github.com', 'redhat.com']\n",
    "\n",
    "for domain in test_domains:\n",
    "    try:\n",
    "        ip = socket.gethostbyname(domain)\n",
    "        print(f\"‚úÖ {domain} -> {ip}\")\n",
    "    except socket.gaierror:\n",
    "        print(f\"‚ùå {domain} - DNS resolution failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## üìã Environment Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã ENVIRONMENT VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Collect verification results\n",
    "results = {\n",
    "    'System Resources': {\n",
    "        'RAM Available': f\"{psutil.virtual_memory().available / 1024**3:.2f} GB\",\n",
    "        'CPU Cores': f\"{psutil.cpu_count(logical=True)}\",\n",
    "        'Disk Free': f\"{psutil.disk_usage('/').free / 1024**3:.2f} GB\"\n",
    "    },\n",
    "    'Core Libraries': {\n",
    "        'pandas': '‚úÖ' if 'pandas' in sys.modules else '‚ùå',\n",
    "        'sklearn': '‚úÖ' if 'sklearn' in sys.modules else '‚ùå',\n",
    "        'numpy': '‚úÖ' if 'numpy' in sys.modules else '‚ùå'\n",
    "    },\n",
    "    'ONNX Stack': {\n",
    "        'onnx': '‚úÖ' if 'onnx' in sys.modules else '‚ùå',\n",
    "        'skl2onnx': '‚úÖ' if 'skl2onnx' in sys.modules else '‚ùå',\n",
    "        'onnxruntime': '‚úÖ' if 'onnxruntime' in sys.modules else '‚ùå'\n",
    "    },\n",
    "    'LangChain': {\n",
    "        'langchain': '‚úÖ' if 'langchain' in sys.modules else '‚ùå'\n",
    "    },\n",
    "    'Dashboard Tools': {\n",
    "        'gradio': '‚úÖ' if 'gradio' in sys.modules else '‚ùå',\n",
    "        'plotly': '‚úÖ' if 'plotly' in sys.modules else '‚ùå'\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, items in results.items():\n",
    "    print(f\"üìÇ {category}\")\n",
    "    for item, status in items.items():\n",
    "        print(f\"   {item:<20} {status}\")\n",
    "    print()\n",
    "\n",
    "# Dataset status\n",
    "print(\"üìä Dataset Status\")\n",
    "for filename in datasets.keys():\n",
    "    filepath = os.path.join('datasets', filename)\n",
    "    status = '‚úÖ' if os.path.exists(filepath) else '‚ùå'\n",
    "    print(f\"   {filename:<30} {status}\")\n",
    "print()\n",
    "\n",
    "# Overall readiness assessment\n",
    "critical_modules = ['pandas', 'sklearn', 'numpy', 'onnx', 'langchain']\n",
    "modules_ok = sum(1 for mod in critical_modules if mod in sys.modules)\n",
    "datasets_ok = sum(1 for f in datasets.keys() if os.path.exists(os.path.join('datasets', f)))\n",
    "\n",
    "print(\"üéØ READINESS ASSESSMENT\")\n",
    "print(f\"   Critical libraries: {modules_ok}/{len(critical_modules)} available\")\n",
    "print(f\"   Required datasets: {datasets_ok}/{len(datasets)} available\")\n",
    "print(f\"   System resources: {'Adequate' if all([ram_ok, cpu_ok, disk_ok]) else 'May need attention'}\")\n",
    "print()\n",
    "\n",
    "if modules_ok == len(critical_modules) and datasets_ok == len(datasets):\n",
    "    print(\"üéâ ENVIRONMENT READY FOR WORKSHOP!\")\n",
    "    print(\"\\n‚úÖ You can proceed to Module 2: Predictive Model Development\")\n",
    "    print(\"   Next notebook: 2-predictive-model/notebooks/01_data_exploration.ipynb\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ENVIRONMENT NEEDS ATTENTION\")\n",
    "    print(\"\\nüîß Recommended actions:\")\n",
    "    if modules_ok < len(critical_modules):\n",
    "        print(\"   - Re-run install_requirements.ipynb\")\n",
    "    if datasets_ok < len(datasets):\n",
    "        print(\"   - Re-run download_datasets.ipynb\")\n",
    "    print(\"   - Check troubleshooting section in Module 1 documentation\")\n",
    "\n",
    "print(f\"\\nüìÖ Verification completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## üÜò Troubleshooting Guide\n",
    "\n",
    "If you encounter issues, here are common solutions:\n",
    "\n",
    "### ‚ùå Library Import Failures\n",
    "```python\n",
    "# Reinstall specific package\n",
    "!pip install --force-reinstall package_name\n",
    "\n",
    "# Clear pip cache\n",
    "!pip cache purge\n",
    "\n",
    "# Install from conda-forge\n",
    "!conda install -c conda-forge package_name\n",
    "```\n",
    "\n",
    "### üìÇ Dataset Issues\n",
    "- Re-run `download_datasets.ipynb`\n",
    "- Check internet connectivity\n",
    "- Verify storage space availability\n",
    "\n",
    "### üíæ Resource Constraints\n",
    "- Request larger workbench instance\n",
    "- Close other notebooks\n",
    "- Clear notebook outputs to save memory\n",
    "\n",
    "### üåê Network Issues\n",
    "- Check OpenShift cluster network policies\n",
    "- Verify external connectivity permissions\n",
    "- Contact cluster administrator if needed\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Environment verification complete!**\n",
    "\n",
    "**Next Step:** Module 2 - Predictive Model Development  \n",
    "**File:** `2-predictive-model/notebooks/01_data_exploration.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}