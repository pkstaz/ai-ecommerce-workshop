{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download and Validation\n",
    "## Module 1: Environment Setup - Dataset Preparation\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Download and validate all required datasets for the AI E-commerce Workshop\n",
    "\n",
    "**Datasets to be downloaded:**\n",
    "- `sales_historical_data.csv` - Sales transactions with temporal patterns (~50MB, 10,000+ records)\n",
    "- `product_catalog.csv` - Product metadata and categories (~5MB, 1,000+ records)\n",
    "- `customer_behavior.csv` - User interaction and behavioral data (~20MB, 5,000+ records)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import hashlib\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ“… Execution time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 2: Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets directory if it doesn't exist\n",
    "datasets_dir = Path(\"datasets\")\n",
    "datasets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subdirectories for organization\n",
    "raw_data_dir = datasets_dir / \"raw\"\n",
    "processed_data_dir = datasets_dir / \"processed\"\n",
    "raw_data_dir.mkdir(exist_ok=True)\n",
    "processed_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Directory structure created:\")\n",
    "print(f\"   ğŸ“‚ {datasets_dir.absolute()}\")\n",
    "print(f\"   ğŸ“‚ {raw_data_dir.absolute()}\")\n",
    "print(f\"   ğŸ“‚ {processed_data_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ Step 3: Generate Synthetic Sales Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sales_data(num_records=12000):\n",
    "    \"\"\"\n",
    "    Generate synthetic sales historical data with realistic patterns\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ Generating {num_records:,} sales records...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Date range: 2 years of historical data\n",
    "    start_date = datetime.now() - timedelta(days=730)\n",
    "    end_date = datetime.now() - timedelta(days=1)\n",
    "    \n",
    "    # Product categories and their characteristics\n",
    "    categories = {\n",
    "        'Electronics': {'base_price': 299, 'variance': 500, 'seasonality': 1.2},\n",
    "        'Clothing': {'base_price': 49, 'variance': 100, 'seasonality': 1.5},\n",
    "        'Books': {'base_price': 19, 'variance': 25, 'seasonality': 0.8},\n",
    "        'Home & Garden': {'base_price': 79, 'variance': 150, 'seasonality': 1.1},\n",
    "        'Sports': {'base_price': 89, 'variance': 200, 'seasonality': 1.3},\n",
    "        'Beauty': {'base_price': 29, 'variance': 60, 'seasonality': 1.0}\n",
    "    }\n",
    "    \n",
    "    sales_data = []\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Random date within range\n",
    "        random_days = random.randint(0, (end_date - start_date).days)\n",
    "        sale_date = start_date + timedelta(days=random_days)\n",
    "        \n",
    "        # Category selection with weights\n",
    "        category = np.random.choice(list(categories.keys()), \n",
    "                                  p=[0.25, 0.20, 0.15, 0.15, 0.15, 0.10])\n",
    "        \n",
    "        # Price calculation with seasonality\n",
    "        base_price = categories[category]['base_price']\n",
    "        variance = categories[category]['variance']\n",
    "        seasonality = categories[category]['seasonality']\n",
    "        \n",
    "        # Seasonal adjustment (higher sales in Q4)\n",
    "        seasonal_multiplier = 1.0\n",
    "        if sale_date.month in [11, 12]:  # November, December\n",
    "            seasonal_multiplier = seasonality\n",
    "        elif sale_date.month in [6, 7]:  # Summer sales\n",
    "            seasonal_multiplier = 1.1\n",
    "        \n",
    "        price = max(base_price + np.random.normal(0, variance/3), base_price * 0.3)\n",
    "        quantity = max(1, int(np.random.exponential(1.5)))\n",
    "        total_amount = price * quantity * seasonal_multiplier\n",
    "        \n",
    "        # Customer demographics\n",
    "        customer_id = f\"CUST_{random.randint(1000, 9999)}\"\n",
    "        product_id = f\"PROD_{category[:3].upper()}_{random.randint(100, 999)}\"\n",
    "        \n",
    "        # Sales channel\n",
    "        channel = np.random.choice(['Online', 'Store', 'Mobile'], p=[0.6, 0.25, 0.15])\n",
    "        \n",
    "        # Geographic region\n",
    "        region = np.random.choice(['North', 'South', 'East', 'West', 'Central'], \n",
    "                                p=[0.25, 0.20, 0.20, 0.20, 0.15])\n",
    "        \n",
    "        sales_data.append({\n",
    "            'transaction_id': f\"TXN_{i+1:06d}\",\n",
    "            'date': sale_date.strftime('%Y-%m-%d'),\n",
    "            'customer_id': customer_id,\n",
    "            'product_id': product_id,\n",
    "            'category': category,\n",
    "            'quantity': quantity,\n",
    "            'unit_price': round(price, 2),\n",
    "            'total_amount': round(total_amount, 2),\n",
    "            'channel': channel,\n",
    "            'region': region,\n",
    "            'day_of_week': sale_date.strftime('%A'),\n",
    "            'month': sale_date.month,\n",
    "            'quarter': (sale_date.month - 1) // 3 + 1,\n",
    "            'year': sale_date.year\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"   Generated {i+1:,} records...\")\n",
    "    \n",
    "    df = pd.DataFrame(sales_data)\n",
    "    print(f\"âœ… Sales data generated: {len(df):,} records\")\n",
    "    return df\n",
    "\n",
    "# Generate and save sales data\n",
    "sales_df = generate_sales_data()\n",
    "sales_file = datasets_dir / \"sales_historical_data.csv\"\n",
    "sales_df.to_csv(sales_file, index=False)\n",
    "print(f\"ğŸ’¾ Saved: {sales_file}\")\n",
    "print(f\"ğŸ“Š File size: {sales_file.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Step 4: Generate Product Catalog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_catalog(num_products=1200):\n",
    "    \"\"\"\n",
    "    Generate synthetic product catalog with metadata\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ Generating {num_products:,} product records...\")\n",
    "    \n",
    "    # Product name templates by category\n",
    "    product_templates = {\n",
    "        'Electronics': [\n",
    "            'Smart Phone Pro', 'Wireless Headphones', 'Laptop Computer', \n",
    "            'Tablet Device', 'Smart Watch', 'Gaming Console', 'Digital Camera',\n",
    "            'Bluetooth Speaker', 'Smart TV', 'Fitness Tracker'\n",
    "        ],\n",
    "        'Clothing': [\n",
    "            'Cotton T-Shirt', 'Denim Jeans', 'Summer Dress', 'Winter Jacket',\n",
    "            'Running Shoes', 'Casual Sneakers', 'Business Shirt', 'Yoga Pants',\n",
    "            'Wool Sweater', 'Baseball Cap'\n",
    "        ],\n",
    "        'Books': [\n",
    "            'Mystery Novel', 'Science Fiction', 'Biography', 'Cookbook',\n",
    "            'History Book', 'Self-Help Guide', 'Technical Manual', 'Art Book',\n",
    "            'Children Story', 'Poetry Collection'\n",
    "        ],\n",
    "        'Home & Garden': [\n",
    "            'Garden Tools Set', 'Kitchen Appliance', 'Furniture Item', 'Decorative Lamp',\n",
    "            'Storage Container', 'Cleaning Supplies', 'Bedding Set', 'Wall Art',\n",
    "            'Plant Pot', 'Outdoor Furniture'\n",
    "        ],\n",
    "        'Sports': [\n",
    "            'Exercise Equipment', 'Team Jersey', 'Sports Shoes', 'Fitness Gear',\n",
    "            'Outdoor Equipment', 'Training Accessories', 'Sports Ball', 'Protective Gear',\n",
    "            'Water Bottle', 'Gym Bag'\n",
    "        ],\n",
    "        'Beauty': [\n",
    "            'Skincare Cream', 'Makeup Kit', 'Hair Care Product', 'Fragrance',\n",
    "            'Beauty Tool', 'Nail Care Set', 'Face Mask', 'Body Lotion',\n",
    "            'Lip Balm', 'Sunscreen'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    brands = {\n",
    "        'Electronics': ['TechCorp', 'InnovateTech', 'DigitalPro', 'SmartDevices', 'FutureTech'],\n",
    "        'Clothing': ['FashionForward', 'StyleCo', 'TrendyWear', 'ComfortClothing', 'UrbanStyle'],\n",
    "        'Books': ['BookPress', 'LiteraryHouse', 'KnowledgeBooks', 'WisdomPublishing', 'ReadMore'],\n",
    "        'Home & Garden': ['HomeComfort', 'GardenPro', 'LivingSpace', 'CozyHome', 'GreenThumb'],\n",
    "        'Sports': ['SportsPro', 'ActiveLife', 'FitGear', 'ChampionSports', 'HealthyActive'],\n",
    "        'Beauty': ['BeautyFirst', 'GlamourCo', 'NaturalBeauty', 'SkinCare+', 'PureBeauty']\n",
    "    }\n",
    "    \n",
    "    products_data = []\n",
    "    product_counter = {category: 0 for category in product_templates.keys()}\n",
    "    \n",
    "    for i in range(num_products):\n",
    "        # Select category with distribution\n",
    "        category = np.random.choice(list(product_templates.keys()), \n",
    "                                  p=[0.25, 0.20, 0.15, 0.15, 0.15, 0.10])\n",
    "        \n",
    "        product_counter[category] += 1\n",
    "        \n",
    "        # Generate product details\n",
    "        base_name = random.choice(product_templates[category])\n",
    "        brand = random.choice(brands[category])\n",
    "        \n",
    "        # Price based on category\n",
    "        price_ranges = {\n",
    "            'Electronics': (50, 1500),\n",
    "            'Clothing': (15, 200),\n",
    "            'Books': (10, 50),\n",
    "            'Home & Garden': (20, 300),\n",
    "            'Sports': (25, 400),\n",
    "            'Beauty': (10, 150)\n",
    "        }\n",
    "        \n",
    "        min_price, max_price = price_ranges[category]\n",
    "        price = round(np.random.uniform(min_price, max_price), 2)\n",
    "        \n",
    "        # Stock levels\n",
    "        stock_level = random.randint(0, 500)\n",
    "        \n",
    "        # Ratings and reviews\n",
    "        rating = round(np.random.normal(4.2, 0.8), 1)\n",
    "        rating = max(1.0, min(5.0, rating))  # Clamp between 1-5\n",
    "        review_count = max(0, int(np.random.exponential(50)))\n",
    "        \n",
    "        # Product features\n",
    "        features = {\n",
    "            'Electronics': ['Wireless', 'Waterproof', 'Fast Charging', 'HD Display', 'Voice Control'],\n",
    "            'Clothing': ['Machine Washable', 'Wrinkle Free', 'Breathable', 'Stretch Fabric', 'UV Protection'],\n",
    "            'Books': ['Hardcover', 'Illustrated', 'Large Print', 'Award Winning', 'Bestseller'],\n",
    "            'Home & Garden': ['Eco-Friendly', 'Durable', 'Easy Assembly', 'Weather Resistant', 'Space Saving'],\n",
    "            'Sports': ['Professional Grade', 'Lightweight', 'Adjustable', 'Non-Slip', 'Quick Dry'],\n",
    "            'Beauty': ['Natural Ingredients', 'Dermatologist Tested', 'Paraben Free', 'Long Lasting', 'Hypoallergenic']\n",
    "        }\n",
    "        \n",
    "        product_features = random.sample(features[category], k=random.randint(1, 3))\n",
    "        \n",
    "        products_data.append({\n",
    "            'product_id': f\"PROD_{category[:3].upper()}_{product_counter[category]:03d}\",\n",
    "            'product_name': f\"{brand} {base_name}\",\n",
    "            'category': category,\n",
    "            'brand': brand,\n",
    "            'price': price,\n",
    "            'stock_level': stock_level,\n",
    "            'rating': rating,\n",
    "            'review_count': review_count,\n",
    "            'features': ', '.join(product_features),\n",
    "            'created_date': (datetime.now() - timedelta(days=random.randint(30, 1095))).strftime('%Y-%m-%d'),\n",
    "            'is_active': random.choice([True, True, True, False]),  # 75% active\n",
    "            'supplier_id': f\"SUP_{random.randint(100, 999)}\",\n",
    "            'weight_kg': round(np.random.uniform(0.1, 50.0), 2),\n",
    "            'dimensions': f\"{random.randint(5, 50)}x{random.randint(5, 50)}x{random.randint(2, 30)}\"\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"   Generated {i+1:,} products...\")\n",
    "    \n",
    "    df = pd.DataFrame(products_data)\n",
    "    print(f\"âœ… Product catalog generated: {len(df):,} products\")\n",
    "    print(f\"ğŸ“Š Category distribution:\")\n",
    "    for category, count in product_counter.items():\n",
    "        print(f\"   {category}: {count} products\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate and save product catalog\n",
    "products_df = generate_product_catalog()\n",
    "products_file = datasets_dir / \"product_catalog.csv\"\n",
    "products_df.to_csv(products_file, index=False)\n",
    "print(f\"ğŸ’¾ Saved: {products_file}\")\n",
    "print(f\"ğŸ“Š File size: {products_file.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘¥ Step 5: Generate Customer Behavior Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_customer_behavior(num_interactions=8000):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer behavior and interaction data\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ Generating {num_interactions:,} customer behavior records...\")\n",
    "    \n",
    "    # Customer segments\n",
    "    customer_segments = {\n",
    "        'Premium': {'conversion_rate': 0.15, 'avg_session_time': 12, 'pages_per_session': 8},\n",
    "        'Regular': {'conversion_rate': 0.08, 'avg_session_time': 8, 'pages_per_session': 5},\n",
    "        'Occasional': {'conversion_rate': 0.04, 'avg_session_time': 5, 'pages_per_session': 3},\n",
    "        'New': {'conversion_rate': 0.02, 'avg_session_time': 3, 'pages_per_session': 2}\n",
    "    }\n",
    "    \n",
    "    interaction_types = [\n",
    "        'page_view', 'product_view', 'add_to_cart', 'remove_from_cart',\n",
    "        'add_to_wishlist', 'search', 'filter_apply', 'sort_apply',\n",
    "        'review_read', 'review_write', 'checkout_start', 'purchase_complete'\n",
    "    ]\n",
    "    \n",
    "    devices = ['Desktop', 'Mobile', 'Tablet']\n",
    "    browsers = ['Chrome', 'Firefox', 'Safari', 'Edge']\n",
    "    traffic_sources = ['Organic Search', 'Paid Search', 'Social Media', 'Direct', 'Email', 'Referral']\n",
    "    \n",
    "    behavior_data = []\n",
    "    \n",
    "    for i in range(num_interactions):\n",
    "        # Customer segment selection\n",
    "        segment = np.random.choice(list(customer_segments.keys()), \n",
    "                                 p=[0.15, 0.35, 0.35, 0.15])\n",
    "        \n",
    "        customer_id = f\"CUST_{random.randint(1000, 9999)}\"\n",
    "        session_id = f\"SESS_{random.randint(100000, 999999)}\"\n",
    "        \n",
    "        # Interaction timestamp\n",
    "        interaction_date = datetime.now() - timedelta(\n",
    "            days=random.randint(1, 90),\n",
    "            hours=random.randint(0, 23),\n",
    "            minutes=random.randint(0, 59)\n",
    "        )\n",
    "        \n",
    "        # Interaction type based on funnel probability\n",
    "        funnel_weights = {\n",
    "            'page_view': 0.25,\n",
    "            'product_view': 0.20,\n",
    "            'search': 0.15,\n",
    "            'add_to_cart': 0.10,\n",
    "            'filter_apply': 0.08,\n",
    "            'add_to_wishlist': 0.06,\n",
    "            'sort_apply': 0.05,\n",
    "            'review_read': 0.04,\n",
    "            'checkout_start': 0.03,\n",
    "            'remove_from_cart': 0.02,\n",
    "            'purchase_complete': 0.015,\n",
    "            'review_write': 0.005\n",
    "        }\n",
    "        \n",
    "        interaction_type = np.random.choice(\n",
    "            list(funnel_weights.keys()),\n",
    "            p=list(funnel_weights.values())\n",
    "        )\n",
    "        \n",
    "        # Session metrics based on segment\n",
    "        segment_info = customer_segments[segment]\n",
    "        session_duration = max(1, int(np.random.normal(\n",
    "            segment_info['avg_session_time'], \n",
    "            segment_info['avg_session_time'] * 0.3\n",
    "        )))\n",
    "        \n",
    "        pages_viewed = max(1, int(np.random.normal(\n",
    "            segment_info['pages_per_session'],\n",
    "            segment_info['pages_per_session'] * 0.2\n",
    "        )))\n",
    "        \n",
    "        # Product interaction\n",
    "        product_id = None\n",
    "        if interaction_type in ['product_view', 'add_to_cart', 'remove_from_cart', 'add_to_wishlist']:\n",
    "            # Use product IDs from the catalog we generated\n",
    "            category = random.choice(['ELE', 'CLO', 'BOO', 'HOM', 'SPO', 'BEA'])\n",
    "            product_id = f\"PROD_{category}_{random.randint(1, 200):03d}\"\n",
    "        \n",
    "        # Device and browser\n",
    "        device = np.random.choice(devices, p=[0.45, 0.40, 0.15])\n",
    "        browser = np.random.choice(browsers, p=[0.60, 0.15, 0.15, 0.10])\n",
    "        \n",
    "        # Geographic and demographic info\n",
    "        region = np.random.choice(['North', 'South', 'East', 'West', 'Central'])\n",
    "        age_group = np.random.choice(['18-24', '25-34', '35-44', '45-54', '55+'], \n",
    "                                   p=[0.15, 0.30, 0.25, 0.20, 0.10])\n",
    "        \n",
    "        # Traffic source\n",
    "        traffic_source = np.random.choice(traffic_sources, \n",
    "                                        p=[0.35, 0.20, 0.15, 0.15, 0.10, 0.05])\n",
    "        \n",
    "        behavior_data.append({\n",
    "            'interaction_id': f\"INT_{i+1:06d}\",\n",
    "            'timestamp': interaction_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'customer_id': customer_id,\n",
    "            'session_id': session_id,\n",
    "            'interaction_type': interaction_type,\n",
    "            'product_id': product_id,\n",
    "            'customer_segment': segment,\n",
    "            'device_type': device,\n",
    "            'browser': browser,\n",
    "            'traffic_source': traffic_source,\n",
    "            'region': region,\n",
    "            'age_group': age_group,\n",
    "            'session_duration_minutes': session_duration,\n",
    "            'pages_viewed': pages_viewed,\n",
    "            'time_on_page_seconds': random.randint(5, 300),\n",
    "            'is_mobile': device == 'Mobile',\n",
    "            'is_new_customer': segment == 'New',\n",
    "            'hour_of_day': interaction_date.hour,\n",
    "            'day_of_week': interaction_date.strftime('%A'),\n",
    "            'month': interaction_date.month\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"   Generated {i+1:,} interactions...\")\n",
    "    \n",
    "    df = pd.DataFrame(behavior_data)\n",
    "    print(f\"âœ… Customer behavior data generated: {len(df):,} interactions\")\n",
    "    \n",
    "    # Show segment distribution\n",
    "    segment_dist = df['customer_segment'].value_counts()\n",
    "    print(f\"ğŸ“Š Customer segment distribution:\")\n",
    "    for segment, count in segment_dist.items():\n",
    "        print(f\"   {segment}: {count:,} interactions ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate and save customer behavior data\n",
    "behavior_df = generate_customer_behavior()\n",
    "behavior_file = datasets_dir / \"customer_behavior.csv\"\n",
    "behavior_df.to_csv(behavior_file, index=False)\n",
    "print(f\"ğŸ’¾ Saved: {behavior_file}\")\n",
    "print(f\"ğŸ“Š File size: {behavior_file.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Step 6: Dataset Validation and Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_datasets():\n",
    "    \"\"\"\n",
    "    Comprehensive validation of all generated datasets\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Performing comprehensive dataset validation...\\n\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # Dataset file definitions\n",
    "    datasets_info = {\n",
    "        'sales_historical_data.csv': {\n",
    "            'min_records': 10000,\n",
    "            'required_columns': ['transaction_id', 'date', 'customer_id', 'product_id', \n",
    "                               'category', 'total_amount', 'channel', 'region'],\n",
    "            'numeric_columns': ['quantity', 'unit_price', 'total_amount', 'month', 'quarter', 'year']\n",
    "        },\n",
    "        'product_catalog.csv': {\n",
    "            'min_records': 1000,\n",
    "            'required_columns': ['product_id', 'product_name', 'category', 'brand', \n",
    "                               'price', 'stock_level', 'rating'],\n",
    "            'numeric_columns': ['price', 'stock_level', 'rating', 'review_count', 'weight_kg']\n",
    "        },\n",
    "        'customer_behavior.csv': {\n",
    "            'min_records': 5000,\n",
    "            'required_columns': ['interaction_id', 'timestamp', 'customer_id', 'session_id',\n",
    "                               'interaction_type', 'customer_segment', 'device_type'],\n",
    "            'numeric_columns': ['session_duration_minutes', 'pages_viewed', 'time_on_page_seconds', \n",
    "                              'hour_of_day', 'month']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for filename, requirements in datasets_info.items():\n",
    "        filepath = datasets_dir / filename\n",
    "        print(f\"ğŸ“‹ Validating {filename}...\")\n",
    "        \n",
    "        # Check file existence\n",
    "        if not filepath.exists():\n",
    "            print(f\"âŒ File not found: {filepath}\")\n",
    "            validation_results[filename] = False\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = pd.read_csv(filepath)\n",
    "            file_size_mb = filepath.stat().st_size / 1024 / 1024\n",
    "            \n",
    "            print(f\"   ğŸ“Š Records: {len(df):,}\")\n",
    "            print(f\"   ğŸ“Š Columns: {len(df.columns)}\")\n",
    "            print(f\"   ğŸ“Š File size: {file_size_mb:.2f} MB\")\n",
    "            \n",
    "            # Validation checks\n",
    "            checks_passed = 0\n",
    "            total_checks = 6\n",
    "            \n",
    "            # 1. Record count check\n",
    "            if len(df) >= requirements['min_records']:\n",
    "                print(f\"   âœ… Record count: {len(df):,} >= {requirements['min_records']:,}\")\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                print(f\"   âŒ Record count: {len(df):,} < {requirements['min_records']:,}\")\n",
    "            \n",
    "            # 2. Required columns check\n",
    "            missing_columns = set(requirements['required_columns']) - set(df.columns)\n",
    "            if not missing_columns:\n",
    "                print(f\"   âœ… All required columns present\")\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                print(f\"   âŒ Missing columns: {missing_columns}\")\n",
    "            \n",
    "            # 3. Data types check\n",
    "            numeric_cols_valid = True\n",
    "            for col in requirements['numeric_columns']:\n",
    "                if col in df.columns:\n",
    "                    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                        print(f\"   âš ï¸  Column '{col}' should be numeric\")\n",
    "                        numeric_cols_valid = False\n",
    "            \n",
    "            if numeric_cols_valid:\n",
    "                print(f\"   âœ… Numeric columns have correct data types\")\n",
    "                checks_passed += 1\n",
    "            \n",
    "            # 4. Missing values check\n",
    "            missing_values = df.isnull().sum().sum()\n",
    "            missing_percentage = (missing_values / (len(df) * len(df.columns))) * 100\n",
    "            \n",
    "            if missing_percentage < 5:  # Less than 5% missing values\n",
    "                print(f\"   âœ… Missing values: {missing_percentage:.2f}% (acceptable)\")\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                print(f\"   âš ï¸  Missing values: {missing_percentage:.2f}% (high)\")\n",
    "            \n",
    "            # 5. Duplicate records check\n",
    "            duplicates = df.duplicated().sum()\n",
    "            duplicate_percentage = (duplicates / len(df)) * 100\n",
    "            \n",
    "            if duplicate_percentage < 1:  # Less than 1% duplicates\n",
    "                print(f\"   âœ… Duplicate records: {duplicate_percentage:.2f}% (acceptable)\")\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                print(f\"   âš ï¸  Duplicate records: {duplicate_percentage:.2f}% (high)\")\n",
    "            \n",
    "            # 6. Data distribution check\n",
    "            has_good_distribution = True\n",
    "            \n",
    "            # Check for categorical columns with reasonable distribution\n",
    "            categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols[:3]:  # Check first 3 categorical columns\n",
    "                unique_values = df[col].nunique()\n",
    "                if unique_values == 1:\n",
    "                    print(f\"   âš ï¸  Column '{col}' has only one unique value\")\n",
    "                    has_good_distribution = False\n",
    "            \n",
    "            if has_good_distribution:\n",
    "                print(f\"   âœ… Data distribution looks healthy\")\n",
    "                checks_passed += 1\n",
    "            \n",
    "            # Overall validation result\n",
    "            success_rate = (checks_passed / total_checks) * 100\n",
    "            if success_rate >= 80:\n",
    "                print(f\"   ğŸ‰ Validation PASSED: {success_rate:.0f}% ({checks_passed}/{total_checks} checks)\")\n",
    "                validation_results[filename] = True\n",
    "            else:\n",
    "                print(f\"   âŒ Validation FAILED: {success_rate:.0f}% ({checks_passed}/{total_checks} checks)\")\n",
    "                validation_results[filename] = False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error reading file: {str(e)}\")\n",
    "            validation_results[filename] = False\n",
    "        \n",
    "        print()  # Empty line for readability\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Perform validation\n",
    "validation_results = validate_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 7: Generate Dataset Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary report of all datasets\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ˆ DATASET SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_size = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for filename in ['sales_historical_data.csv', 'product_catalog.csv', 'customer_behavior.csv']:\n",
    "        filepath = datasets_dir / filename\n",
    "        \n",
    "        if filepath.exists():\n",
    "            df = pd.read_csv(filepath)\n",
    "            file_size = filepath.stat().st_size\n",
    "            \n",
    "            print(f\"\\nğŸ“ {filename}\")\n",
    "            print(f\"   Records: {len(df):,}\")\n",
    "            print(f\"   Columns: {len(df.columns)}\")\n",
    "            print(f\"   Size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "            print(f\"   Validation: {'âœ… PASSED' if validation_results.get(filename, False) else 'âŒ FAILED'}\")\n",
    "            \n",
    "            # Show sample data structure\n",
    "            print(f\"   Sample columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "            \n",
    "            total_size += file_size\n",
    "            total_records += len(df)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š OVERALL SUMMARY\")\n",
    "    print(f\"   Total records: {total_records:,}\")\n",
    "    print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Files created: {len([f for f in validation_results.values() if f])} / {len(validation_results)}\")\n",
    "    \n",
    "    # Overall success status\n",
    "    success_count = sum(validation_results.values())\n",
    "    total_files = len(validation_results)\n",
    "    \n",
    "    if success_count == total_files:\n",
    "        print(f\"\\nğŸ‰ ALL DATASETS SUCCESSFULLY CREATED AND VALIDATED!\")\n",
    "        print(f\"âœ… Ready to proceed to Module 2: Predictive Model Development\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  {total_files - success_count} datasets failed validation\")\n",
    "        print(f\"â— Please review the errors above and re-run failed sections\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generate final report\n",
    "generate_summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 8: Next Steps and Workshop Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final preparation check\n",
    "print(\"ğŸš€ WORKSHOP PREPARATION CHECKLIST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "checklist_items = [\n",
    "    (\"âœ…\", \"OpenShift AI environment verified\"),\n",
    "    (\"âœ…\", \"Data Science Project created\"),\n",
    "    (\"âœ…\", \"Jupyter workbench configured\"),\n",
    "    (\"âœ…\" if validation_results.get('sales_historical_data.csv', False) else \"âŒ\", \"Sales historical data ready\"),\n",
    "    (\"âœ…\" if validation_results.get('product_catalog.csv', False) else \"âŒ\", \"Product catalog prepared\"),\n",
    "    (\"âœ…\" if validation_results.get('customer_behavior.csv', False) else \"âŒ\", \"Customer behavior data generated\"),\n",
    "    (\"âœ…\", \"Dataset validation completed\")\n",
    "]\n",
    "\n",
    "for status, item in checklist_items:\n",
    "    print(f\"   {status} {item}\")\n",
    "\n",
    "all_passed = all(validation_results.values())\n",
    "\n",
    "if all_passed:\n",
    "    print(f\"\\nğŸ‰ ENVIRONMENT SETUP COMPLETE!\")\n",
    "    print(f\"\\nğŸ“š You're ready to proceed to:\")\n",
    "    print(f\"   ğŸ“‚ Module 2: Predictive Model Development\")\n",
    "    print(f\"   ğŸ“„ File: 02-predictive-model.md\")\n",
    "    print(f\"\\nğŸ’¡ What's next:\")\n",
    "    print(f\"   â€¢ Explore sales data patterns\")\n",
    "    print(f\"   â€¢ Engineer features for ML\")\n",
    "    print(f\"   â€¢ Train Random Forest model\")\n",
    "    print(f\"   â€¢ Export to ONNX format\")\n",
    "    print(f\"   â€¢ Deploy with OpenVINO\")\nelse:\n",
    "    print(f\"\\nâš ï¸  Please resolve validation issues before proceeding\")\n",
    "    print(f\"   Re-run the failed sections above\")\n",
    "    print(f\"   Check the troubleshooting guide if needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"ğŸ“§ Need help? Contact: cestay@redhat.com\")\n",
    "print(f\"ğŸ™ Workshop repo: https://github.com/pkstaz/ai-ecommerce-workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "âœ… **Created comprehensive datasets** for the AI e-commerce workshop  \n",
    "âœ… **Generated realistic sales historical data** with seasonal patterns and business logic  \n",
    "âœ… **Built a detailed product catalog** with categories, brands, and features  \n",
    "âœ… **Simulated customer behavior data** with user segments and interaction patterns  \n",
    "âœ… **Validated all datasets** for quality and completeness  \n",
    "âœ… **Prepared the environment** for AI model development  \n",
    "\n",
    "**Dataset Overview:**\n",
    "- **Sales Data:** 12,000+ transactions with temporal patterns\n",
    "- **Product Catalog:** 1,200+ products across 6 categories\n",
    "- **Customer Behavior:** 8,000+ interactions with behavioral insights\n",
    "\n",
    "**Ready for Module 2:** Predictive Model Development\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}